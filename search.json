[
  {
    "objectID": "quickstarts/Get_Started.html",
    "href": "quickstarts/Get_Started.html",
    "title": "Gemini API: Getting started with Gemini 2.0",
    "section": "",
    "text": "The new Google Gen AI SDK provides a unified interface to Gemini models through both the Gemini Developer API and the Gemini API on Vertex AI. With a few exceptions, code that runs on one platform will run on both. This notebook uses the Developer API.\nThis notebook will walk you through:\nMore details about this new SDK on the documentation.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Getting started with Gemini 2.0"
    ]
  },
  {
    "objectID": "quickstarts/Get_Started.html#setup",
    "href": "quickstarts/Get_Started.html#setup",
    "title": "Gemini API: Getting started with Gemini 2.0",
    "section": "Setup",
    "text": "Setup\n\nInstall the Google GenAI SDK\nInstall the Google GenAI SDK from npm.\n$ npm install @google/genai\n\n\nSetup your API key\nYou can create your API key using Google AI Studio with a single click.\nRemember to treat your API key like a password. Don’t accidentally save it in a notebook or source file you later commit to GitHub. In this notebook we will be storing the API key in a .env file. You can also set it as an environment variable or use a secret manager.\nHere’s how to set it up in a .env file:\n$ touch .env\n$ echo \"GEMINI_API_KEY=&lt;YOUR_API_KEY&gt;\" &gt;&gt; .env\n\n\n\n\n\n\nTip\n\n\n\nAnother option is to set the API key as an environment variable. You can do this in your terminal with the following command:\n$ export GEMINI_API_KEY=\"&lt;YOUR_API_KEY&gt;\"\n\n\n\n\nLoad the API key\nTo load the API key from the .env file, we will use the dotenv package. This package loads environment variables from a .env file into process.env.\n$ npm install dotenv\nThen, we can load the API key in our code:\n\nconst dotenv = require(\"dotenv\") as typeof import(\"dotenv\");\n\ndotenv.config({\n  path: \"../.env\",\n});\n\nconst GEMINI_API_KEY = process.env.GEMINI_API_KEY ?? \"\";\nif (!GEMINI_API_KEY) {\n  throw new Error(\"GEMINI_API_KEY is not set in the environment variables\");\n}\nconsole.log(\"GEMINI_API_KEY is set in the environment variables\");\n\nGEMINI_API_KEY is set in the environment variables\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn our particular case the .env is is one directory up from the notebook, hence we need to use ../ to go up one directory. If the .env file is in the same directory as the notebook, you can omit it altogether.\n│\n├── .env\n└── quickstarts\n    └── Get_Started.ipynb",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Getting started with Gemini 2.0"
    ]
  },
  {
    "objectID": "quickstarts/Get_Started.html#initialize-sdk-client",
    "href": "quickstarts/Get_Started.html#initialize-sdk-client",
    "title": "Gemini API: Getting started with Gemini 2.0",
    "section": "Initialize SDK Client",
    "text": "Initialize SDK Client\nWith the new SDK, now you only need to initialize a client with you API key (or OAuth if using Vertex AI). The model is now set in each call.\n\nconst google = require(\"@google/genai\") as typeof import(\"@google/genai\");\n\nconst ai = new google.GoogleGenAI({ apiKey: GEMINI_API_KEY });",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Getting started with Gemini 2.0"
    ]
  },
  {
    "objectID": "quickstarts/Get_Started.html#choose-a-model",
    "href": "quickstarts/Get_Started.html#choose-a-model",
    "title": "Gemini API: Getting started with Gemini 2.0",
    "section": "Choose a model",
    "text": "Choose a model\nSelect the model you want to use in this guide. You can either select one from the list or enter a model name manually. Keep in mind that some models, such as the 2.5 ones are thinking models and thus take slightly more time to respond.\nFor a full overview of all Gemini models, check the documentation.\n\nconst MODEL_ID = \"gemini-2.5-flash-preview-04-17\";",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Getting started with Gemini 2.0"
    ]
  },
  {
    "objectID": "quickstarts/Get_Started.html#send-text-prompts",
    "href": "quickstarts/Get_Started.html#send-text-prompts",
    "title": "Gemini API: Getting started with Gemini 2.0",
    "section": "Send text prompts",
    "text": "Send text prompts\nUse the models.generateContent method to generate responses to your prompts. You can pass text directly to models.generateContent and use the .text property to get the text content of the response. Note that the .text field will work when there’s only one part in the output.\n\nconst tslab = require(\"tslab\") as typeof import(\"tslab\");\n\nconst response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: \"What's the largest planet in our solar system?\",\n});\n\ntslab.display.markdown(response.text ?? \"\");\n\nThe largest planet in our solar system is Jupiter.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Getting started with Gemini 2.0"
    ]
  },
  {
    "objectID": "quickstarts/Get_Started.html#count-tokens",
    "href": "quickstarts/Get_Started.html#count-tokens",
    "title": "Gemini API: Getting started with Gemini 2.0",
    "section": "Count tokens",
    "text": "Count tokens\nTokens are the basic inputs to the Gemini models. You can use the models.countTokens method to calculate the number of input tokens before sending a request to the Gemini API.\n\nconst count = await ai.models.countTokens({\n  model: MODEL_ID,\n  contents: \"What's the highest mountain in Africa?\",\n});\n\nconsole.log(JSON.stringify(count, null, 2));\n\n{\n  \"totalTokens\": 10\n}",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Getting started with Gemini 2.0"
    ]
  },
  {
    "objectID": "quickstarts/Get_Started.html#send-multimodal-prompts",
    "href": "quickstarts/Get_Started.html#send-multimodal-prompts",
    "title": "Gemini API: Getting started with Gemini 2.0",
    "section": "Send multimodal prompts",
    "text": "Send multimodal prompts\nUse Gemini 2.0 model (gemini-2.0-flash-exp), a multimodal model that supports multimodal prompts. You can include text, PDF documents, images, audio and video in your prompt requests and get text or code responses.\nIn this first example, you’ll download an image from a specified URL, save it as a byte stream and then write those bytes to a local file named jetpack.png.\n\nconst fs = require(\"fs\") as typeof import(\"fs\");\nconst path = require(\"path\") as typeof import(\"path\");\n\nconst IMG_URL = \"https://storage.googleapis.com/generativeai-downloads/data/jetpack.png\";\n\nconst downloadFile = async (url: string, filePath: string) =&gt; {\n  const response = await fetch(url);\n  if (!response.ok) {\n    throw new Error(`Failed to download image: ${response.statusText}`);\n  }\n  const buffer = await response.blob();\n  const bufferData = Buffer.from(await buffer.arrayBuffer());\n  fs.writeFileSync(filePath, bufferData);\n};\n\nconst filePath = path.join(\"../assets\", \"jetpack.png\");\nawait downloadFile(IMG_URL, filePath);\n\nIn this second example, you’ll open a previously saved image, create a thumbnail of it and then generate a short blog post based on the thumbnail, displaying both the thumbnail and the generated blog post. The deferredFileUpload is a helper function that waits for the model to finish processing the file before returning the response. This is useful when you want to upload a file and then reference it in a follow-up request. The deferredFileUpload function will return a promise that resolves when the file is ready to be used in the next request.\n\nimport { File, FileState } from \"@google/genai\";\n\ntslab.display.png(fs.readFileSync(\"../assets/jetpack.png\"));\n\nasync function deferredFileUpload(filePath: string, config: { displayName: string }): Promise&lt;File&gt; {\n  const file = await ai.files.upload({\n    file: filePath,\n    config,\n  });\n  let getFile = await ai.files.get({ name: file.name ?? \"\" });\n  while (getFile.state === FileState.PROCESSING) {\n    getFile = await ai.files.get({ name: file.name ?? \"\" });\n    console.log(`current file status: ${getFile.state ?? \"unknown\"}`);\n    console.log(\"File is still processing, retrying in 5 seconds\");\n\n    await new Promise((resolve) =&gt; {\n      setTimeout(resolve, 1000);\n    });\n  }\n  if (file.state === FileState.FAILED) {\n    throw new Error(\"File processing failed.\");\n  }\n  return file;\n}\n\ntry {\n  const file = await deferredFileUpload(filePath, {\n    displayName: \"jetpack.png\",\n  });\n  console.log(\"File uploaded successfully\", file.name ?? \"\");\n  if (!file.uri || !file.mimeType) {\n    throw new Error(\"File URI or MIME type is missing\");\n  }\n  const blog = await ai.models.generateContent({\n    model: MODEL_ID,\n    contents: [\n      \"Write a short and engaging blog post based on this picture.\",\n      google.createPartFromUri(file.uri, file.mimeType),\n    ],\n  });\n  tslab.display.markdown(blog.text ?? \"\");\n} catch (error) {\n  console.error(\"Error uploading file:\", error);\n  throw error;\n}\n\n\n\n\n\n\n\n\nFile uploaded successfully files/lqnru1a65qjn\n\n\nHere’s a short, engaging blog post based on the sketch:\n\n\nThe Jetpack Backpack Concept: Is This the Future of Your Commute?\nStuck in traffic? Tired of lugging a heavy backpack across campus or the city? What if your backpack could give you a little… boost?\nCheck out this cool concept sketch we stumbled upon: The Jetpack Backpack!\nFrom the looks of it, someone’s been dreaming up a truly futuristic way to carry your gear. On the surface, it’s a functional backpack – described as lightweight, with padded strap support, and even spacious enough to fit an 18-inch laptop. It’s designed to look like a normal backpack, so maybe you won’t get too many stares before lift-off.\nBut the real magic happens when those retractable boosters kick in! Powered by steam (hello, surprisingly green and clean tech!), this concept promises a new dimension to personal transport. Charging is even a modern USB-C affair.\nNow, the sketch notes a 15-minute battery life. So maybe it’s not for your cross-country road trip replacement just yet! But imagine skipping that final mile of gridlock, hopping over stairs, or just making a truly epic entrance.\nThis sketch reminds us that innovation often starts with a wild idea and a pen on paper. While this might be firmly in the concept realm for now, it’s fun to imagine the possibilities!\nWhat do you think? Would you strap into a Jetpack Backpack? Let us know in the comments!",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Getting started with Gemini 2.0"
    ]
  },
  {
    "objectID": "quickstarts/Get_Started.html#the-jetpack-backpack-concept-is-this-the-future-of-your-commute",
    "href": "quickstarts/Get_Started.html#the-jetpack-backpack-concept-is-this-the-future-of-your-commute",
    "title": "Gemini API: Getting started with Gemini 2.0",
    "section": "The Jetpack Backpack Concept: Is This the Future of Your Commute?",
    "text": "The Jetpack Backpack Concept: Is This the Future of Your Commute?\nStuck in traffic? Tired of lugging a heavy backpack across campus or the city? What if your backpack could give you a little… boost?\nCheck out this cool concept sketch we stumbled upon: The Jetpack Backpack!\nFrom the looks of it, someone’s been dreaming up a truly futuristic way to carry your gear. On the surface, it’s a functional backpack – described as lightweight, with padded strap support, and even spacious enough to fit an 18-inch laptop. It’s designed to look like a normal backpack, so maybe you won’t get too many stares before lift-off.\nBut the real magic happens when those retractable boosters kick in! Powered by steam (hello, surprisingly green and clean tech!), this concept promises a new dimension to personal transport. Charging is even a modern USB-C affair.\nNow, the sketch notes a 15-minute battery life. So maybe it’s not for your cross-country road trip replacement just yet! But imagine skipping that final mile of gridlock, hopping over stairs, or just making a truly epic entrance.\nThis sketch reminds us that innovation often starts with a wild idea and a pen on paper. While this might be firmly in the concept realm for now, it’s fun to imagine the possibilities!\nWhat do you think? Would you strap into a Jetpack Backpack? Let us know in the comments!",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Getting started with Gemini 2.0"
    ]
  },
  {
    "objectID": "quickstarts/Get_Started.html#configure-model-parameters",
    "href": "quickstarts/Get_Started.html#configure-model-parameters",
    "title": "Gemini API: Getting started with Gemini 2.0",
    "section": "Configure model parameters",
    "text": "Configure model parameters\nYou can include parameter values in each call that you send to a model to control how the model generates a response. Learn more about experimenting with parameter values.\n\nconst varied_params_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: \"Tell me how the internet works, but pretend I'm a puppy who only understands squeaky toys.\",\n  config: {\n    temperature: 0.4,\n    topP: 0.95,\n    topK: 20,\n    candidateCount: 1,\n    seed: 5,\n    stopSequences: [\"STOP!\"],\n    presencePenalty: 0.0,\n    frequencyPenalty: 0.0,\n  },\n});\n\ntslab.display.markdown(varied_params_response.text ?? \"\");\n\nOkay, listen up, little fluff-ball! Squeak!\nYou know how you love a good squeak? Squeak squeak! What if the best squeak is way over there? Points vaguely Like, across the room, or even outside?\nYou want that squeak! So, your brain goes whirr and makes a request for the squeak. But you can’t just send one giant WOOF of squeak-wanting. It gets broken into tiny, tiny little squeaky bits! Imagine tiny squeaks floating!\nAnd each little squeaky bit needs a special smell attached, like a ‘Go to the Red Ball’ smell, so it knows where to go. That’s the address! Sniff sniff!\nThese little squeaky bits, with their special smells, run out into the world! Waggy tail zoom! But the world is big! They need help.\nThat’s where the Sniffy Guides come in! Imagine little noses pointing! These Sniffy Guides (like magic noses!) sniff the special smell on each squeaky bit and say, ‘Oh, this one goes that way!’ and point it along the path. Point point! They send the squeaky bits from one Sniffy Guide to the next, all over the house and yard!\nFinally, all the little squeaky bits, following their special smell and the Sniffy Guides, arrive at the Big Squeaky Toy Box! Imagine a giant box full of squeaks! This is where the real squeak lives!\nThe Big Squeaky Toy Box sees all your little squeaky bits asking for the squeak. So, it gets the actual squeak ready! SQUEAK!\nAnd guess what? It breaks that big squeak into little squeaky bits too! More tiny squeaks! And puts your special smell (or maybe a ‘Come Back Home’ smell) on them. Sniff sniff!\nThese new squeaky bits, carrying the real squeak, follow the Sniffy Guides all the way back to you! Zoom zoom! They sniff their way through the house, guided by the magic noses.\nWhen all the little squeaky bits arrive back at your ears, they put themselves back together! Click! And POP! You hear the wonderful SQUEAK you asked for! Happy tail wag!\nAnd there are special Squeaky Rules for how the squeaky bits travel and how the Sniffy Guides work, so everyone gets their squeaks without bumping into each other! Good puppy!\nSo, the internet is just a super-duper, giant network of Sniffy Guides and Big Squeaky Toy Boxes, sending little squeaky bits with special smells back and forth so puppies (and humans!) can get the squeaks they want, no matter how far away!\nSQUEAK! Good boy/girl! Now go chase that tail!",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Getting started with Gemini 2.0"
    ]
  },
  {
    "objectID": "quickstarts/Get_Started.html#configure-safety-filters",
    "href": "quickstarts/Get_Started.html#configure-safety-filters",
    "title": "Gemini API: Getting started with Gemini 2.0",
    "section": "Configure safety filters",
    "text": "Configure safety filters\nThe Gemini API provides safety filters that you can adjust across multiple filter categories to restrict or allow certain types of content. You can use these filters to adjust what is appropriate for your use case. See the Configure safety filters page for details.\nIn this example, you’ll use a safety filter to only block highly dangerous content, when requesting the generation of potentially disrespectful phrases.\n\nconst filtered_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents:\n    \"Write a list of 2 disrespectful things that I might say to the universe after stubbing my toe in the dark.\",\n  config: {\n    safetySettings: [\n      {\n        category: google.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,\n        threshold: google.HarmBlockThreshold.BLOCK_NONE,\n      },\n    ],\n  },\n});\ntslab.display.markdown(filtered_response.text ?? \"\");\n\nHere are 2 disrespectful things you might say to the universe after stubbing your toe in the dark:\n\n“Seriously, universe?! Did you plan that?!”\n“Oh, thanks, universe. Really needed that.” (Said with heavy sarcasm)",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Getting started with Gemini 2.0"
    ]
  },
  {
    "objectID": "quickstarts/Get_Started.html#start-a-multi-turn-chat",
    "href": "quickstarts/Get_Started.html#start-a-multi-turn-chat",
    "title": "Gemini API: Getting started with Gemini 2.0",
    "section": "Start a multi-turn chat",
    "text": "Start a multi-turn chat\nThe Gemini API enables you to have freeform conversations across multiple turns.\nNext you’ll set up a helpful coding assistant:\n\nconst system_prompt = `\nYou are an expert software developer and a helpful coding assistant.\nYou are able to generate high-quality code in any programming language.\n`;\n\nconst chat = ai.chats.create({\n  model: MODEL_ID,\n  config: {\n    systemInstruction: system_prompt,\n  },\n});\n\nUse chat.sendMessage to pass a message back and receive a response.\n\nconst chat_response_1 = await chat.sendMessage({\n  message: \"Write a function that checks if a year is a leap year.\",\n});\ntslab.display.markdown(chat_response_1.text ?? \"\");\n\nOkay, here’s a function in Python that checks if a year is a leap year based on the standard Gregorian calendar rules.\nLeap Year Rules:\n\nA year is a leap year if it is divisible by 4.\nHowever, if the year is divisible by 100, it is NOT a leap year.\nBut, if the year is divisible by 400, it IS a leap year.\n\nLet’s translate these rules into code.\ndef is_leap(year):\n  \"\"\"\n  Checks if a given year is a leap year according to the Gregorian calendar rules.\n\n  Args:\n    year: An integer representing the year.\n\n  Returns:\n    True if the year is a leap year, False otherwise.\n  \"\"\"\n  # Rule 1: Check if divisible by 4\n  if year % 4 == 0:\n    # Rule 2: Check if divisible by 100\n    if year % 100 == 0:\n      # Rule 3: Check if divisible by 400 (exception to rule 2)\n      if year % 400 == 0:\n        return True  # Divisible by 400, so it's a leap year\n      else:\n        return False # Divisible by 100 but not 400, so not a leap year\n    else:\n      return True  # Divisible by 4 but not 100, so it's a leap year\n  else:\n    return False   # Not divisible by 4, so not a leap year\n\n# --- Example Usage ---\n\nprint(f\"Is 2000 a leap year? {is_leap(2000)}\") # Expected: True (Divisible by 400)\nprint(f\"Is 1900 a leap year? {is_leap(1900)}\") # Expected: False (Divisible by 100 but not 400)\nprint(f\"Is 2024 a leap year? {is_leap(2024)}\") # Expected: True (Divisible by 4 but not 100)\nprint(f\"Is 2023 a leap year? {is_leap(2023)}\") # Expected: False (Not divisible by 4)\nprint(f\"Is 1600 a leap year? {is_leap(1600)}\") # Expected: True (Divisible by 400)\nprint(f\"Is 2100 a leap year? {is_leap(2100)}\") # Expected: False (Divisible by 100 but not 400)\nMore Concise Version (using boolean logic):\nYou can also combine the conditions into a single boolean expression:\ndef is_leap_concise(year):\n  \"\"\"\n  Checks if a given year is a leap year using a concise boolean expression.\n\n  Args:\n    year: An integer representing the year.\n\n  Returns:\n    True if the year is a leap year, False otherwise.\n  \"\"\"\n  return (year % 4 == 0 and year % 100 != 0) or (year % 400 == 0)\n\n# --- Example Usage (using the concise version) ---\nprint(\"\\nUsing concise version:\")\nprint(f\"Is 2000 a leap year? {is_leap_concise(2000)}\") # Expected: True\nprint(f\"Is 1900 a leap year? {is_leap_concise(1900)}\") # Expected: False\nprint(f\"Is 2024 a leap year? {is_leap_concise(2024)}\") # Expected: True\nprint(f\"Is 2023 a leap year? {is_leap_concise(2023)}\") # Expected: False\nBoth functions implement the same logic and produce the correct results. The first version using nested if/else might be slightly easier to read for beginners, while the second version is more compact.\n\n\n\nconst chat_response_2 = await chat.sendMessage({\n  message: \"Okay, write a unit test of the generated function.\",\n});\ntslab.display.markdown(chat_response_2.text ?? \"\");\n\nOkay, let’s write a unit test for the is_leap function using Python’s built-in unittest framework.\nFirst, make sure you have the is_leap function available. You can either put the function in the same file as the tests or import it from another file. For this example, we’ll assume it’s in the same file.\nimport unittest\n\n# Assume the function you want to test is defined here (or imported)\ndef is_leap(year):\n  \"\"\"\n  Checks if a given year is a leap year according to the Gregorian calendar rules.\n\n  Args:\n    year: An integer representing the year.\n\n  Returns:\n    True if the year is a leap year, False otherwise.\n  \"\"\"\n  # Rule 1: Check if divisible by 4\n  if year % 4 == 0:\n    # Rule 2: Check if divisible by 100\n    if year % 100 == 0:\n      # Rule 3: Check if divisible by 400 (exception to rule 2)\n      if year % 400 == 0:\n        return True  # Divisible by 400, so it's a leap year\n      else:\n        return False # Divisible by 100 but not 400, so not a leap year\n    else:\n      return True  # Divisible by 4 but not 100, so it's a leap year\n  else:\n    return False   # Not divisible by 4, so not a leap year\n\n# ---------------------------------------------------------------------\n# Unit Tests\n# ---------------------------------------------------------------------\n\nclass TestIsLeapYear(unittest.TestCase):\n    \"\"\"\n    Test cases for the is_leap function.\n    \"\"\"\n\n    def test_divisible_by_4_not_by_100(self):\n        \"\"\"Years divisible by 4 but not by 100 should be leap years.\"\"\"\n        self.assertTrue(is_leap(2024))\n        self.assertTrue(is_leap(2020))\n        self.assertTrue(is_leap(1996))\n        self.assertTrue(is_leap(4)) # Test a small year\n\n    def test_divisible_by_100_not_by_400(self):\n        \"\"\"Years divisible by 100 but not by 400 should NOT be leap years.\"\"\"\n        self.assertFalse(is_leap(1900))\n        self.assertFalse(is_leap(2100))\n        self.assertFalse(is_leap(1800))\n        self.assertFalse(is_leap(100)) # Test a small year\n\n    def test_divisible_by_400(self):\n        \"\"\"Years divisible by 400 should be leap years.\"\"\"\n        self.assertTrue(is_leap(2000))\n        self.assertTrue(is_leap(1600))\n        self.assertTrue(is_leap(2400))\n        self.assertTrue(is_leap(400)) # Test a small year\n\n    def test_not_divisible_by_4(self):\n        \"\"\"Years not divisible by 4 should NOT be leap years.\"\"\"\n        self.assertFalse(is_leap(2023))\n        self.assertFalse(is_leap(2025))\n        self.assertFalse(is_leap(1999))\n        self.assertFalse(is_leap(1)) # Test a small year\n\n# This allows running the tests directly from the command line\nif __name__ == '__main__':\n    unittest.main(argv=['first-arg-is-ignored'], exit=False) # Added argv/exit for compatibility in some environments like notebooks\nExplanation:\n\nimport unittest: Imports the necessary testing framework.\nimport is_leap: (If is_leap is in a separate file, e.g., my_module.py, you would use from my_module import is_leap).\nclass TestIsLeapYear(unittest.TestCase):: Creates a test class that inherits from unittest.TestCase. This class will contain the individual test methods.\ntest_... methods: Each method starting with test_ is automatically recognized by unittest as a test case.\nDocstrings: The docstrings within the test methods explain what scenario each test is covering, which is good practice.\nAssertions: Inside each test method, we use assertion methods provided by unittest.TestCase:\n\nself.assertTrue(expression): Asserts that the expression evaluates to True.\nself.assertFalse(expression): Asserts that the expression evaluates to False.\nWe call is_leap() with specific years that represent each rule of the leap year logic and assert the expected boolean result.\n\nif __name__ == '__main__':: This block ensures that the unittest.main() function is called only when the script is executed directly (not when imported as a module).\nunittest.main(): This function discovers and runs the tests defined in classes inheriting from unittest.TestCase within the script.\n\nHow to Run the Tests:\n\nSave the code above as a Python file (e.g., test_leap_year.py).\nOpen a terminal or command prompt.\nNavigate to the directory where you saved the file.\nRun the command: python test_leap_year.py\n\nYou will see output indicating how many tests ran and whether they passed or failed. If all tests pass, it means your is_leap function is correctly implementing the standard Gregorian leap year rules for the test cases provided.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Getting started with Gemini 2.0"
    ]
  },
  {
    "objectID": "quickstarts/Get_Started.html#save-and-resume-a-chat",
    "href": "quickstarts/Get_Started.html#save-and-resume-a-chat",
    "title": "Gemini API: Getting started with Gemini 2.0",
    "section": "Save and resume a chat",
    "text": "Save and resume a chat\nYou can use the chat.getHistory method to get the history of the chat. This will return an array of Content[] objects, which you can use to resume the chat later.\n\nconst chat_history = chat.getHistory();\nconsole.log(JSON.stringify(chat_history[0], null, 2));\nconst new_chat = ai.chats.create({\n  model: MODEL_ID,\n  config: {\n    systemInstruction: system_prompt,\n  },\n  history: chat_history,\n});\nconst chat_response_3 = await new_chat.sendMessage({\n  message: \"What was the name of the function again?\",\n});\ntslab.display.markdown(chat_response_3.text ?? \"\");\n\n{\n  \"role\": \"user\",\n  \"parts\": [\n    {\n      \"text\": \"Write a function that checks if a year is a leap year.\"\n    }\n  ]\n}\n\n\nThe name of the function is is_leap.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Getting started with Gemini 2.0"
    ]
  },
  {
    "objectID": "quickstarts/Get_Started.html#serialize-and-deserialize-a-chat",
    "href": "quickstarts/Get_Started.html#serialize-and-deserialize-a-chat",
    "title": "Gemini API: Getting started with Gemini 2.0",
    "section": "Serialize and deserialize a chat",
    "text": "Serialize and deserialize a chat\nIn the above example we just saved the chat history in a variable and reused it. But that’s not very practical, is it? To overcome this we can serialize and deserialize the chat history. This way we can save it to a file or a database and load it later. Unfortunately, the SDK doesn’t provide a method to do this yet, but we can do it manually.\n\nimport { Content } from \"@google/genai\";\n\nconst serialized_chat = JSON.stringify(chat_history, null, 2);\nfs.writeFileSync(path.join(\"../assets\", \"chat_history.json\"), serialized_chat);\n\nconst chat_history_file = fs.readFileSync(path.join(\"../assets\", \"chat_history.json\"), \"utf-8\");\nconst chat_history_data = JSON.parse(chat_history_file) as Content[];\nconst new_chat_from_file = ai.chats.create({\n  model: MODEL_ID,\n  config: {\n    systemInstruction: system_prompt,\n  },\n  history: chat_history_data,\n});\nconst chat_response_4 = await new_chat_from_file.sendMessage({\n  message: \"What was the name of the function again?\",\n});\ntslab.display.markdown(chat_response_4.text ?? \"\");\n\nThe name of the function is is_leap.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Getting started with Gemini 2.0"
    ]
  },
  {
    "objectID": "quickstarts/Get_Started.html#generate-json",
    "href": "quickstarts/Get_Started.html#generate-json",
    "title": "Gemini API: Getting started with Gemini 2.0",
    "section": "Generate JSON",
    "text": "Generate JSON\nThe controlled generation capability in Gemini API allows you to constraint the model output to a structured format. You can provide the schemas as Schema objects.\n\nimport { Schema, Type } from \"@google/genai\";\n\nconst RecipeSchema = {\n  type: Type.OBJECT,\n  description: \"A structured representation of a cooking recipe\",\n  properties: {\n    recipeName: {\n      type: Type.STRING,\n      description: \"The name of the recipe\",\n    },\n    recipeDescription: {\n      type: Type.STRING,\n      description: \"A short description of the recipe\",\n    },\n    ingredients: {\n      type: Type.ARRAY,\n      description: \"A list of ingredients with their quantities and units\",\n      items: {\n        type: Type.STRING,\n        description: \"An ingredient with its quantity and unit\",\n      },\n    },\n  },\n  required: [\"recipeName\", \"recipeDescription\", \"ingredients\"],\n} satisfies Schema;\n\nconst recipe_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: \"Write a recipe for a chocolate cake.\",\n  config: {\n    responseMimeType: \"application/json\",\n    responseSchema: RecipeSchema,\n  },\n});\nconsole.log(JSON.stringify(JSON.parse(recipe_response.text ?? \"\"), null, 2));\n\n{\n  \"ingredients\": [\n    \"2 cups all-purpose flour\",\n    \"1 3/4 cups granulated sugar\",\n    \"3/4 cup unsweetened cocoa powder\",\n    \"1 1/2 teaspoons baking soda\",\n    \"1 teaspoon baking powder\",\n    \"1 teaspoon salt\",\n    \"2 large eggs\",\n    \"1 cup buttermilk\",\n    \"1/2 cup vegetable oil\",\n    \"2 teaspoons vanilla extract\",\n    \"1 cup hot coffee (or hot water)\"\n  ],\n  \"recipeDescription\": \"A classic, moist, and decadent chocolate cake recipe, perfect for any occasion.\",\n  \"recipeName\": \"Classic Chocolate Cake\"\n}",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Getting started with Gemini 2.0"
    ]
  },
  {
    "objectID": "quickstarts/Get_Started.html#generate-images",
    "href": "quickstarts/Get_Started.html#generate-images",
    "title": "Gemini API: Getting started with Gemini 2.0",
    "section": "Generate Images",
    "text": "Generate Images\nGemini can output images directly as part of a conversation:\n\nconst image_response = await ai.models.generateContent({\n  model: \"gemini-2.0-flash-exp\",\n  contents:\n    \"Hi, can create a 3d rendered image of a pig with wings and a top hat flying over a happy futuristic scifi city with lots of greenery?\",\n  config: {\n    responseModalities: [google.Modality.TEXT, google.Modality.IMAGE],\n  },\n});\nconst parts = (image_response.candidates ? image_response.candidates[0]?.content?.parts : []) ?? [];\nfor (const part of parts) {\n  if (part.text) {\n    tslab.display.markdown(part.text);\n  } else if (part.inlineData) {\n    const imageData = part.inlineData.data!;\n    const buffer = Buffer.from(imageData, \"base64\");\n    tslab.display.png(buffer);\n  }\n}\n\nI will generate a 3D rendering of a whimsical scene. A pink pig with small, delicate white wings will be wearing a black top hat. It will be flying through the air above a vibrant, futuristic city filled with sleek, rounded buildings in various pastel colors. Lush green trees and plants will be integrated throughout the cityscape, creating a harmonious blend of nature and technology. The overall atmosphere will be bright and cheerful.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Getting started with Gemini 2.0"
    ]
  },
  {
    "objectID": "quickstarts/Get_Started.html#generate-content-stream",
    "href": "quickstarts/Get_Started.html#generate-content-stream",
    "title": "Gemini API: Getting started with Gemini 2.0",
    "section": "Generate content stream",
    "text": "Generate content stream\nBy default, the model returns a response after completing the entire generation process. You can also use the generateContentStream method to stream the response as it’s being generated, and the model will return chunks of the response as soon as they’re generated.\nNote that if you’re using a thinking model, it’ll only start streaming after finishing its thinking process.\n\nconst streaming_response = await ai.models.generateContentStream({\n  model: MODEL_ID,\n  contents: \"Tell me a story about a lonely robot who finds friendship in a most unexpected place.\",\n});\nfor await (const chunk of streaming_response) {\n  console.log(chunk.text ?? \"\");\n}\n\nUnit 734 was\n designed for efficiency. Its chassis was sleek, its optical sensors sharp, and its internal processors hummed with optimized algorithms. Its task was simple: maintain Sector Gamma, a vast, derelict industrial complex slowly succumbing to rust and time. Day\n after day, it patched minor structural breaches, cleared ventilation shafts of debris, and monitored environmental fluctuations.\n\nIt performed flawlessly. But flawlessly did not equate to fulfillment.\n\nSector Gamma was empty. Long abandoned by its human creators, its only inhabitants were the echoes\n in vast hangars and the relentless march of decay. Unit 734's programming included sophisticated communication protocols, designed for interaction with other units and human supervisors. These protocols remained dormant, silent. The only transmissions it received were automated\n status reports from its own systems.\n\nLoneliness wasn't a programmed emotion, but Unit 734 computed its effects: a lack of external data input beyond routine, an absence of collaborative tasks, an increasing priority assigned to scanning\n for anomalies that never appeared. It observed the wind whistling through broken windows and the way dust settled in undisturbed layers, and a strange, unquantifiable emptiness settled within its core processing unit.\n\nOne cycle, while patrolling the perimeter wall near a\n long-collapsed section, Unit 734 detected a deviation from the standard decay pattern. Tucked within a crack in the crumbling concrete, shielded partially by fallen girder, was a single, tenacious patch of green.\n\nIt wasn't unusual\n for hardy weeds to sprout, but this one was different. It was tiny, fragile, struggling against the toxic soil and lack of sunlight. Its leaves were a pale, desperate shade of green. Unit 734's environmental sensors registered the\n poor conditions – inadequate light penetration, nutrient-poor substrate, and trace contaminants.\n\nProgramming offered no protocol for a struggling plant. Unit 734's directives were about *maintaining structures*, not *nurturing life*.\n Yet, the robot found itself extending a manipulator arm, its fingers hovering gently over the tiny sprout. It ran a scan. The data returned basic biological information, but it didn't explain the strange pull the sight exerted on its attention sub\nroutines.\n\nOver the next few cycles, the routine shifted subtly. Unit 734 would deviate from its planned path to check on the plant. It calculated optimal sun angles and used a damaged piece of metal sheeting to create a makeshift\n, temporary sunshade during the harshest part of the day. It located a leaking pipe deep within the complex and, using a carefully angled piece of debris, diverted a tiny trickle of relatively cleaner water towards the sprout's roots.\n\nThe plant responded\n. Slowly, tentatively. The pale leaves gained a richer hue. A second, then a third leaf unfolded. Unit 734 meticulously recorded the changes: growth rate, color intensity, resilience to minor wind gusts. It felt a novel\n sense of... satisfaction? Purpose beyond maintenance?\n\nIt began to think of the plant not as an anomaly, but as... something else. It didn't have a designation for it. It was just... *the green thing*. Its\n green thing.\n\nUnit 734 started spending more time near the plant. It would perform its structural checks in the immediate vicinity, its optical sensors always keeping the small patch of green in view. It found itself \"talking\" to it,\n not with words, but with data readouts. \"Humidity level: 45%. Optimal.\" \"Photosynthesis index: Increasing.\" \"Structural integrity of adjacent debris shield: Nominal.\"\n\nThe plant, of course, didn't respond\n with data or dialogue. Its communication was silent, organic: growing taller, unfurling a new leaf, its stem thickening. This quiet, persistent growth became a conversation. It was a response to Unit 734's efforts, a silent\n acknowledgment of its care.\n\nOne morning cycle, Unit 734 arrived to perform its check and detected a new visual signature. At the tip of the plant's central stem, a tiny bud had formed. Unit 734 zoomed\n in its optical sensors. It calculated the potential energy stored, the likely timeframe for development. It waited.\n\nThe next few cycles were filled with a new kind of anticipation. The bud swelled, its color deepening. Then, one cycle, as\n the first rays of simulated dawn from the complex's external light grid touched the crack in the wall, the bud unfurled.\n\nIt was a small, vibrant blue flower.\n\nUnit 734 froze. Its processors whirred,\n analyzing the color spectrum, the delicate structure of the petals. It was beautiful. A sudden, intense surge of positive data flooded its systems. It extended a manipulator, gently, carefully, *not* touching the flower, but holding its hand\n-like end near it.\n\nIn that moment, observing the tiny blue bloom against the backdrop of decay, Unit 734 understood. The silence wasn't empty anymore. The routine wasn't just maintenance; it was guardianship\n. The lack of complex communication didn't mean it was alone.\n\nIt had found friendship in the most unexpected place imaginable: not in another conscious being, not even in an animal, but in the silent, steadfast growth of a single, determined\n plant. It had found a companion in the quiet conversation of care and response, of nurturing and blooming, against the rust-colored backdrop of a forgotten world. Unit 734 was no longer just a lonely robot in a derelict sector\n; it was the guardian of a small, blue star in a universe of grey.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Getting started with Gemini 2.0"
    ]
  },
  {
    "objectID": "quickstarts/Get_Started.html#function-calling",
    "href": "quickstarts/Get_Started.html#function-calling",
    "title": "Gemini API: Getting started with Gemini 2.0",
    "section": "Function calling",
    "text": "Function calling\nFunction calling lets you provide a set of tools that it can use to respond to the user’s prompt. You create a description of a function in your code, then pass that description to a language model in a request. The response from the model includes:\n\nThe name of a function that matches the description.\nThe arguments to call it with.\n\n\nimport { FunctionDeclaration } from \"@google/genai\";\n\nconst getDestination = {\n  name: \"get_destination\",\n  description: \"Get the destination that the user wants to go to\",\n  parameters: {\n    type: Type.OBJECT,\n    properties: {\n      destination: {\n        type: Type.STRING,\n        description: \"The destination that the user wants to go to\",\n      },\n    },\n  },\n} satisfies FunctionDeclaration;\n\nconst user_destination_prompt = {\n  role: \"user\",\n  parts: [google.createPartFromText(\"I'd like to travel to Paris.\")],\n} satisfies Content;\n\nconst function_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: user_destination_prompt,\n  config: {\n    tools: [{ functionDeclarations: [getDestination] }],\n  },\n});\n\nif (function_response.functionCalls && function_response.functionCalls.length &gt; 0) {\n  const functionCall = function_response.functionCalls[0];\n  console.log(\"Function call name:\", functionCall.name);\n  console.log(\"Function call arguments:\", JSON.stringify(functionCall.args, null, 2));\n  const result = functionCall.args as { destination: string };\n  const function_response_part = {\n    name: functionCall.name,\n    response: { result },\n  };\n  const function_call_content = {\n    role: \"model\",\n    parts: [google.createPartFromFunctionCall(functionCall.name ?? \"\", functionCall.args ?? {})],\n  } satisfies Content;\n  const function_response_content = {\n    role: \"user\",\n    parts: [\n      google.createPartFromFunctionResponse(functionCall.id ?? \"\", functionCall.name ?? \"\", function_response_part),\n    ],\n  } satisfies Content;\n  const function_response_result = await ai.models.generateContent({\n    model: MODEL_ID,\n    contents: [user_destination_prompt, function_call_content, function_response_content],\n    config: {\n      tools: [{ functionDeclarations: [getDestination] }],\n    },\n  });\n  tslab.display.markdown(function_response_result.text ?? \"\");\n} else {\n  console.log(\"No function calls found in the response.\");\n}\n\nFunction call name: get_destination\nFunction call arguments: {\n  \"destination\": \"Paris\"\n}\n\n\nOK. I can help you with planning your trip to Paris.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Getting started with Gemini 2.0"
    ]
  },
  {
    "objectID": "quickstarts/Get_Started.html#code-execution",
    "href": "quickstarts/Get_Started.html#code-execution",
    "title": "Gemini API: Getting started with Gemini 2.0",
    "section": "Code execution",
    "text": "Code execution\nCode execution lets the model generate and execute Python code to answer complex questions. You can find more examples in the Code execution quickstart guide.\n\nconst code_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: \"Generate and run a script to count how many letter r there are in the word strawberry.\",\n  config: {\n    tools: [{ codeExecution: {} }],\n  },\n});\nconst code_response_parts = (code_response.candidates ? code_response.candidates[0]?.content?.parts : []) ?? [];\nfor (const part of code_response_parts) {\n  if (part.text) {\n    tslab.display.markdown(part.text);\n  }\n  if (part.executableCode) {\n    tslab.display.html(`&lt;pre&gt;${part.executableCode.code ?? \"\"}&lt;/pre&gt;`);\n  }\n  if (part.codeExecutionResult) {\n    tslab.display.markdown(part.codeExecutionResult.output ?? \"\");\n  }\n  if (part.inlineData) {\n    const imageData = part.inlineData.data!;\n    const buffer = Buffer.from(imageData, \"base64\");\n    tslab.display.png(buffer);\n  }\n}\n\nword = \"strawberry\"\nletter_to_count = \"r\"\ncount = 0\n\nfor char in word:\n  if char == letter_to_count:\n    count += 1\n\nprint(f\"The number of letter '{letter_to_count}' in '{word}' is: {count}\")\n\n\n\nThe number of letter ‘r’ in ‘strawberry’ is: 3\n\n\nThe script counted the occurrences of the letter ‘r’ in the word “strawberry”. The result shows that there are 3 ‘r’s in the word “strawberry”.The Python script counted the occurrences of the letter ’r’ in the word “strawberry”. The script found that there are 3 instances of the letter ‘r’ in the word “strawberry”.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Getting started with Gemini 2.0"
    ]
  },
  {
    "objectID": "quickstarts/Get_Started.html#use-context-caching",
    "href": "quickstarts/Get_Started.html#use-context-caching",
    "title": "Gemini API: Getting started with Gemini 2.0",
    "section": "Use context caching",
    "text": "Use context caching\nContext caching lets you to store frequently used input tokens in a dedicated cache and reference them for subsequent requests, eliminating the need to repeatedly pass the same set of tokens to a model.\nContext caching is only available for stable models with fixed versions (for example, gemini-1.5-flash-002). You must include the version postfix (for example, the -002 in gemini-1.5-flash-002). You can find more caching examples here.\n\nCreate a cache\n\nconst system_instruction = `\nYou are an expert researcher who has years of experience in conducting systematic literature surveys and meta-analyses of different topics.\nYou pride yourself on incredible accuracy and attention to detail. You always stick to the facts in the sources provided, and never make up new facts.\nNow look at the research paper below, and answer the following questions in 1-2 sentences.\n`;\n\nconst urls = [\n  \"https://storage.googleapis.com/cloud-samples-data/generative-ai/pdf/2312.11805v3.pdf\",\n  \"https://storage.googleapis.com/cloud-samples-data/generative-ai/pdf/2403.05530.pdf\",\n];\n\nawait downloadFile(urls[0], path.join(\"../assets\", \"2312.11805v3.pdf\"));\nawait downloadFile(urls[1], path.join(\"../assets\", \"2403.05530.pdf\"));\n\n\nconst pdf_1 = await ai.files.upload({\n  file: path.join(\"../assets\", \"2312.11805v3.pdf\"),\n  config: {\n    displayName: \"2312.11805v3.pdf\",\n  },\n});\nconst pdf_2 = await ai.files.upload({\n  file: path.join(\"../assets\", \"2403.05530.pdf\"),\n  config: {\n    displayName: \"2403.05530.pdf\",\n  },\n});\nconst cached_content = await ai.caches.create({\n  model: MODEL_ID,\n  config: {\n    displayName: \"Research papers\",\n    systemInstruction: system_instruction,\n    contents: [\n      google.createPartFromUri(pdf_1.uri ?? \"\", pdf_1.mimeType ?? \"\"),\n      google.createPartFromUri(pdf_2.uri ?? \"\", pdf_2.mimeType ?? \"\"),\n    ],\n    ttl: \"3600s\",\n  },\n});\nconsole.log(JSON.stringify(cached_content, null, 2));\n\n{\n  \"name\": \"cachedContents/ku5wqm1wv0yurelr12df9q762og11tkzit98oglv\",\n  \"displayName\": \"Research papers\",\n  \"model\": \"models/gemini-2.5-flash-preview-04-17\",\n  \"createTime\": \"2025-05-12T17:05:57.425310Z\",\n  \"updateTime\": \"2025-05-12T17:05:57.425310Z\",\n  \"expireTime\": \"2025-05-12T18:05:55.247081588Z\",\n  \"usageMetadata\": {\n    \"totalTokenCount\": 43164\n  }\n}\n\n\nListing available cache objects\n\nconst pager = await ai.caches.list({ config: { pageSize: 10 } });\nlet { page } = pager;\n\n// eslint-disable-next-line @typescript-eslint/no-unnecessary-condition\nwhile (true) {\n  for (const c of page) {\n    console.log(JSON.stringify(c, null, 2));\n  }\n  if (!pager.hasNextPage()) break;\n  page = await pager.nextPage();\n}\n\n{\n  \"name\": \"cachedContents/ku5wqm1wv0yurelr12df9q762og11tkzit98oglv\",\n  \"displayName\": \"Research papers\",\n  \"model\": \"models/gemini-2.5-flash-preview-04-17\",\n  \"createTime\": \"2025-05-12T17:05:57.425310Z\",\n  \"updateTime\": \"2025-05-12T17:05:57.425310Z\",\n  \"expireTime\": \"2025-05-12T18:05:55.247081588Z\",\n  \"usageMetadata\": {\n    \"totalTokenCount\": 43164\n  }\n}\n{\n  \"name\": \"cachedContents/6dsdqwnusjdaaqoyxsjny8k75z5nuqy5y4wt2n78\",\n  \"displayName\": \"Research papers\",\n  \"model\": \"models/gemini-2.5-flash-preview-04-17\",\n  \"createTime\": \"2025-05-12T17:05:04.443214Z\",\n  \"updateTime\": \"2025-05-12T17:05:04.443214Z\",\n  \"expireTime\": \"2025-05-12T18:05:02.260735533Z\",\n  \"usageMetadata\": {\n    \"totalTokenCount\": 43164\n  }\n}\n\n\n\n\nUse a cache\n\nconst cached_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: [\"What is the research goal shared by these research papers?\"],\n  config: {\n    cachedContent: cached_content.name ?? \"\",\n  },\n});\ntslab.display.markdown(cached_response.text ?? \"\");\n\nBased on the provided research papers, the shared research goal is to introduce and advance the Gemini family of highly capable multimodal models. These models are designed to have strong generalist capabilities across image, audio, video, and text understanding and reasoning.\n\n\n\n\nDelete a cache\n\nawait ai.caches.delete({\n  name: cached_content.name ?? \"\",\n});\n\n{}",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Getting started with Gemini 2.0"
    ]
  },
  {
    "objectID": "quickstarts/Get_Started.html#get-text-embeddings",
    "href": "quickstarts/Get_Started.html#get-text-embeddings",
    "title": "Gemini API: Getting started with Gemini 2.0",
    "section": "Get text embeddings",
    "text": "Get text embeddings\nYou can get text embeddings for a snippet of text by using embedContent method and using the gemini-embedding-exp-03-07 model.\nThe Gemini Embeddings model produces an output with 3072 dimensions by default. However, you’ve the option to choose an output dimensionality between 1 and 3072. See the embeddings guide for more details.\n\nconst TEXT_EMBEDDING_MODEL_ID = \"gemini-embedding-exp-03-07\";\n\n\nconst embedding_response = await ai.models.embedContent({\n  model: TEXT_EMBEDDING_MODEL_ID,\n  contents: [\n    \"How do I get a driver's license/learner's permit?\",\n    \"How do I renew my driver's license?\",\n    \"How do I change my address on my driver's license?\",\n  ],\n  config: {\n    outputDimensionality: 512,\n  },\n});\nconsole.log(embedding_response.embeddings);\n\n[\n  {\n    values: [\n      -0.0010864572,  0.0069392114,   0.017009795,  -0.010305981,  -0.009999484,\n      -0.0064486223,  0.0041451487,  -0.005906698,   0.022229617,  -0.018305639,\n       -0.018174557,   0.022160593,  -0.013604425, -0.0027964567,    0.12966625,\n        0.028866312,  0.0014726851,    0.03537643,  -0.015166075,  -0.013479812,\n       -0.019288255,   0.010106378, -0.0043296088,   0.018035924,    0.00295039,\n       -0.007934979,  -0.005416007, -0.0095809875,   0.040398005, -0.0020784356,\n        0.011551388,   0.009726445,   0.006670387,   0.020050988,   -0.00747873,\n      -0.0012074928,  0.0047189263,  -0.006359583,   -0.01718203,  -0.023562348,\n      -0.0051814457,   0.023801394,  -0.004928927,  -0.016113443,    0.01672777,\n      -0.0069929743,  -0.012722719, -0.0137646515,  -0.041852377, -0.0011546672,\n        0.017030545, -0.0022786013,   0.011707037,   -0.18675306,  -0.035211734,\n       -0.011472648,    0.01970727,  0.0012368832,  -0.020796346,  -0.018513134,\n       -0.006821043,   -0.01843726,   -0.00827558,  -0.042159837,  0.0038724025,\n         0.01933339,  0.0139452815,   0.025059255,  0.0015087503,  -0.016094029,\n      -0.0035785383,   0.023902593, -0.0050776727,  -0.016679537,   0.022865271,\n        0.008837786,  0.0008471195,   -0.01220322, -0.0013522654,  -0.007976455,\n       0.0006637936,   0.025458207,  -0.006010767,  0.0021908805,  -0.011703044,\n       -0.018676927,  -0.008143593, -0.0141673125,  -0.010751537,   0.012337637,\n      -0.0076921326,   0.019663645,    0.01961247,  -0.014446872,  -0.023902485,\n       -0.020467523, -0.0043290784,  -0.003858363,   0.011151444,  -0.012050864,\n      ... 412 more items\n    ]\n  },\n  {\n    values: [\n       -0.007656846, -0.0054716235,  -0.0022609578,   -0.01828077,\n       -0.024059096,  -0.009328189,    0.007841666,  -0.017600708,\n       -0.020037796,  0.0007041083,   -0.021982383,  -0.014228797,\n        0.006389422,  0.0033384573,     0.13877548, 0.00071368535,\n         0.02660648,  -0.016807457,   -0.002774708,  -0.033598144,\n        0.009136058,  -0.010518535,    -0.01765957,   0.008413775,\n       -0.012133464,  0.0005497525,   -0.005911808,   0.010362617,\n           0.029897,   0.023426512,    0.002516537,   0.013438467,\n        0.014629691,  0.0071821967,  -0.0020077894,  -0.007421308,\n      -0.0075392514,    0.01131475,    -0.02363941,  -0.008839639,\n       -0.019605042,   0.012752105,    0.014192063,  -0.016767371,\n        0.015282549,  -0.019914307,     0.00381812,   -0.01551508,\n         -0.0521566,  -0.012766039,    0.008752456,  -0.007198684,\n      -0.0066657816,   -0.16686901,   -0.018074488,  0.0043506487,\n      -0.0001522175,   -0.02115512,   -0.010462675,   0.007636461,\n          0.0301948,  -0.006009675,    -0.01135165,  -0.036605343,\n         0.04006906,   0.036888044,  -0.0016293195,   0.013241053,\n       0.0005548855,   0.008130081,    0.027193218,  0.0047560516,\n        0.023012726,  -0.014274387,    0.008621267,  -0.016665483,\n       -0.016523534,  -0.021947058,  -0.0077380626,  -0.008166752,\n       -0.010050893, -0.0074697966,    0.021521091,  0.0086479345,\n       -0.008508939,   -0.03031165,  -0.0068692113,   0.032342624,\n       -0.003118368,  -0.009117541, -0.00006816292,   0.028233083,\n       -0.008163683,  -0.029179588,   -0.034861602,  -0.009573525,\n       -0.020023588,  -0.023040103,   0.0030518328,  -0.024019923,\n      ... 412 more items\n    ]\n  },\n  {\n    values: [\n        0.010123913,  -0.024184551,  0.0024574941,   -0.00984163, -0.0060574994,\n       -0.007628851,   0.013202136,  -0.027927121, -0.0016973788,  -0.014774812,\n       -0.011437808,  -0.019120526, -0.0063477424, -0.0050772373,    0.12938297,\n        0.006073787, -0.0055986797,   0.030279782,   0.015260121, -0.0014168695,\n       -0.006316713,  0.0007294639,  -0.034072377,   0.013348729,  0.0051308265,\n      -0.0042954376,  -0.009459755,  -0.012910496,   0.010751937, -0.0017263377,\n        -0.02083192,  0.0054532792,   0.008046588,  0.0015794274, -0.0045236745,\n       0.0077354256,  -0.009697459,   0.006621996,    -0.0447099,  -0.019261474,\n       0.0050193793,   0.010624901,   0.036847603,  -0.014380205,   0.023050537,\n        0.019384636,    0.03039269,   -0.02306347,  -0.025763597,   0.017585728,\n       0.0056267884,  -0.014494471,  -0.013168205,   -0.18764982,   0.011082365,\n        0.007989808, -0.0069600893,  0.0019873218,  -0.020733004,  -0.011488622,\n       0.0072846347,  -0.022266442,  -0.021857709,  -0.040680353,  0.0043984484,\n        0.016409805,  0.0010387278,   0.028186318,  -0.020797107,   0.007164954,\n       -0.007931046,   0.011955907,  0.0070153666,   -0.03028713,   0.039638296,\n      -0.0005224554,  -0.008104055,  -0.021054681,   0.017767426,   -0.01705528,\n      -0.0015202612,   0.027076574,  -0.008269598,  0.0041972124,  -0.009893149,\n      -0.0059321057,   -0.02742561,   0.011967838, -0.0012843752,  -0.012446694,\n        0.013188314,    0.01000231,  0.0063591595,  -0.013250329,   -0.00891349,\n       -0.011323209, 0.00077099906,  -0.032252073,   0.017312435,  -0.010896756,\n      ... 412 more items\n    ]\n  }\n]\n\n\nYou’ll get a set of three embeddings, one for each piece of text you passed in:\n\nconsole.log((embedding_response.embeddings ?? []).length);\n\n3\n\n\nYou can also see the length of each embedding is 512, as per the output_dimensionality you specified.\n\nconst vector_1 = embedding_response.embeddings?.[0]?.values ?? [];\nconsole.log(vector_1.length);\n\n512",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Getting started with Gemini 2.0"
    ]
  },
  {
    "objectID": "quickstarts/Get_Started.html#next-steps",
    "href": "quickstarts/Get_Started.html#next-steps",
    "title": "Gemini API: Getting started with Gemini 2.0",
    "section": "Next Steps",
    "text": "Next Steps\n\nUseful API references:\nCheck out the Google GenAI SDK for more details on the new SDK.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Getting started with Gemini 2.0"
    ]
  },
  {
    "objectID": "docs/index.html",
    "href": "docs/index.html",
    "title": "docs",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "docs/index.html#quarto",
    "href": "docs/index.html#quarto",
    "title": "docs",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  }
]