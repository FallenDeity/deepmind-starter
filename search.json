[
  {
    "objectID": "quickstarts/Get_started_LyriaRealtime.html",
    "href": "quickstarts/Get_started_LyriaRealtime.html",
    "title": "Get started with Music generation using Lyria RealTime",
    "section": "",
    "text": "Lyria RealTime, provides access to a state-of-the-art, real-time, streaming music generation model. It allows developers to build applications where users can interactively create, continuously steer, and perform instrumental music using text prompts.\nLyria RealTime main characteristics are:\nCheck Lyria RealTime’s documentation for more details.\nAlso note that due to Colab limitation, you won’t be able to experience the real time capabilities of Lyria RealTime but only limited audio output. Use the AI studio’s apps, Prompt DJ and MIDI DJ to fully experience Lyria RealTime",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Get started with Music generation using Lyria RealTime"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_LyriaRealtime.html#setup",
    "href": "quickstarts/Get_started_LyriaRealtime.html#setup",
    "title": "Get started with Music generation using Lyria RealTime",
    "section": "Setup",
    "text": "Setup\n\nInstall the Google GenAI SDK\nInstall the Google GenAI SDK from npm.\n$ npm install @google/genai\n\n\nSetup your API key\nYou can create your API key using Google AI Studio with a single click.\nRemember to treat your API key like a password. Don’t accidentally save it in a notebook or source file you later commit to GitHub. In this notebook we will be storing the API key in a .env file. You can also set it as an environment variable or use a secret manager.\nHere’s how to set it up in a .env file:\n$ touch .env\n$ echo \"GEMINI_API_KEY=&lt;YOUR_API_KEY&gt;\" &gt;&gt; .env\n\n\n\n\n\n\nTip\n\n\n\nAnother option is to set the API key as an environment variable. You can do this in your terminal with the following command:\n$ export GEMINI_API_KEY=\"&lt;YOUR_API_KEY&gt;\"\n\n\n\n\nLoad the API key\nTo load the API key from the .env file, we will use the dotenv package. This package loads environment variables from a .env file into process.env.\n$ npm install dotenv\nThen, we can load the API key in our code:\n\nconst dotenv = require(\"dotenv\") as typeof import(\"dotenv\");\n\ndotenv.config({\n  path: \"../.env\",\n});\n\nconst GEMINI_API_KEY = process.env.GEMINI_API_KEY ?? \"\";\nif (!GEMINI_API_KEY) {\n  throw new Error(\"GEMINI_API_KEY is not set in the environment variables\");\n}\nconsole.log(\"GEMINI_API_KEY is set in the environment variables\");\n\nGEMINI_API_KEY is set in the environment variables\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn our particular case the .env is is one directory up from the notebook, hence we need to use ../ to go up one directory. If the .env file is in the same directory as the notebook, you can omit it altogether.\n│\n├── .env\n└── quickstarts\n    └── Get_started_LyriaRealTime.ipynb\n\n\n\n\nInitialize SDK Client\nLyria RealTime API is a new capability introduced with the Lyria RealTime model so only works with the lyria-realtime-exp model. As it’s an experimental feature, you also need to use the v1alpha client version.\n\nconst google = require(\"@google/genai\") as typeof import(\"@google/genai\");\n\nconst ai = new google.GoogleGenAI({ apiKey: GEMINI_API_KEY, httpOptions: { apiVersion: \"v1alpha\" } });\n\n\n\nSelect a model\nMultimodal Live API are a new capability introduced with the Gemini 2.0 model. It won’t work with previous generation models.\n\nconst tslab = require(\"tslab\") as typeof import(\"tslab\");\n\nconst MODEL_ID = \"models/lyria-realtime-exp\";\n\n\n\nUtilites\nYou’re going to use the Lyria Realtime’s audio output, the easiest way hear it in Colab is to write the PCM data out as a WAV file:\n\nconst fs = require(\"fs\") as typeof import(\"fs\");\nconst path = require(\"path\") as typeof import(\"path\");\nconst wave = require(\"wavefile\") as typeof import(\"wavefile\");\n\nfunction saveAudioToFile(audioData: Int16Array, filePath: string) {\n  fs.mkdirSync(path.dirname(filePath), { recursive: true });\n  const wav = new wave.WaveFile();\n  wav.fromScratch(2, 48000, \"16\", audioData);\n  fs.writeFileSync(filePath, wav.toBuffer());\n  console.debug(`Audio saved to ${filePath}`);\n}",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Get started with Music generation using Lyria RealTime"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_LyriaRealtime.html#generate-music",
    "href": "quickstarts/Get_started_LyriaRealtime.html#generate-music",
    "title": "Get started with Music generation using Lyria RealTime",
    "section": "Generate music",
    "text": "Generate music\nThe Lyria Realtime model utilizes websockets to stream audio data in real time. The model can be prompted with text descriptions of the desired music, and it will generate audio that matches the description and stream it in chunks. It takes 2 different configuration parameters as input:\n\nWeightedPrompt: A list of text prompts that describe the desired music. Each prompt can have a weight that indicates its influence on the generated music. The prompts can be sent while the session is active, allowing for continuous steering of the music generation.\nLiveMusicGenerationConfig: A configuration object that specifies the desired characteristics of the generated music, such as bpm, density, brightness, scale, and guidance. These parameters can be adjusted in real time to influence the music generation.\n\n\n\n\n\n\n\nImportant\n\n\n\nYou can’t just update a single parameter in the LiveMusicGenerationConfig object. You need to send the entire object with all the parameters each time you want to update it, otherwise the other parameters will be reset to their default values.\nAny updates to bpm or scale need to be followed by a resetContext call to reset the context of the music generation. This is because these parameters affect the musical structure and need to be applied from the beginning of the generation.\n\n\n\nimport { LiveMusicGenerationConfig, LiveMusicSession, LiveMusicServerMessage, WeightedPrompt } from \"@google/genai\";\n\nlet n_index = 0;\nconst MAX_CHUNKS = 10; // Maximum number of audio chunks to process\nconst responseQueue: LiveMusicServerMessage[] = [];\n\nasync function receive() {\n  console.debug(\"Receiving audio chunks...\");\n  let done = false;\n  let chunk_count = 0;\n  const audioChunks: number[][] = [];\n  while (!done) {\n    if (responseQueue.length &gt; 0) {\n      const response = responseQueue.shift();\n      if (response?.audioChunk?.data) {\n        const audioBuffer = Buffer.from(response.audioChunk.data, \"base64\");\n        const intArray = new Int16Array(\n          audioBuffer.buffer,\n          audioBuffer.byteOffset,\n          audioBuffer.length / Int16Array.BYTES_PER_ELEMENT\n        );\n        audioChunks.push(Array.from(intArray));\n        chunk_count++;\n      }\n      if (chunk_count &gt;= MAX_CHUNKS) {\n        done = true;\n        console.debug(\"Received complete response\");\n      }\n    } else {\n      await new Promise((resolve) =&gt; setTimeout(resolve, 100));\n    }\n  }\n  const audioFilePath = path.join(\"../assets/live\", `lyria_realtime_${n_index}.wav`);\n  saveAudioToFile(new Int16Array(audioChunks.flat()), audioFilePath);\n  tslab.display.html(`\n    &lt;h3&gt;Audio Response Lyria&lt;/h3&gt;\n    &lt;audio controls&gt;\n        &lt;source src=\"../assets/live/lyria_realtime_${n_index}.wav\" type=\"audio/wav\"&gt;\n        Your browser does not support the audio element.\n    &lt;/audio&gt;\n  `);\n  n_index++;\n}\n\nasync function generateMusic(prompts: WeightedPrompt[], config: LiveMusicGenerationConfig) {\n  const session: LiveMusicSession = await ai.live.music.connect({\n    model: MODEL_ID,\n    callbacks: {\n      onmessage: (message) =&gt; {\n        responseQueue.push(message);\n      },\n      onerror: (error) =&gt; {\n        console.error(\"music session error:\", error);\n      },\n      onclose: () =&gt; {\n        console.log(\"Lyria RealTime stream closed.\");\n      },\n    },\n  });\n\n  await session.setWeightedPrompts({\n    weightedPrompts: prompts,\n  });\n  await session.setMusicGenerationConfig({\n    musicGenerationConfig: config,\n  });\n\n  console.debug(\"Lyria Realtime session started\");\n  session.play();\n  await receive();\n  session.close();\n  console.debug(\"Lyria Realtime session closed\");\n}",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Get started with Music generation using Lyria RealTime"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_LyriaRealtime.html#audio-generation-function",
    "href": "quickstarts/Get_started_LyriaRealtime.html#audio-generation-function",
    "title": "Get started with Music generation using Lyria RealTime",
    "section": "Audio Generation Function",
    "text": "Audio Generation Function\nThe above code sample shows how to generate music using the Lyria Realtime model. There are two methods worth noting:\n\ngenerateMusic - Driver method\nThis method is used to start the music generation process. It takes an array of WeightedPrompt objects and a LiveMusicGenerationConfig object as input. It returns a LiveMusicGenerationSession object that can be used to interact with the music generation session.\nThis method: - Opens a websocket connection to the Lyria Realtime model. - Sends the initial prompts to the model using setWeightedPrompts, which sets the initial musical influences. - Sends the initial configuration using setLiveMusicGenerationConfig, which sets the desired characteristics of the generated music. - Sets up event listeners to handle incoming audio data and errors and start the audio playback.\n\n\nreceive - Audio data handler\nThis methods is used to handle incoming audio data from the Lyria Realtime model. It monitors the responseQueue for incoming audio data and collects it in a buffer. When the buffer reaches a certain size, it writes the audio data to a WAV file and plays it back using the saveAudioToFile utility function.\n\n\n\n\n\n\nNote\n\n\n\nCurrently once the receive method is called, it blocks further function execution till required number of chunks are met. This means that you won’t be able to send new prompts or configuration updates while the receive method is running. Ideally, in a real-time application, you would want to run the receive method in a separate thread while also having a send method to send new prompts and configuration updates.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Get started with Music generation using Lyria RealTime"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_LyriaRealtime.html#try-lyria-realtime",
    "href": "quickstarts/Get_started_LyriaRealtime.html#try-lyria-realtime",
    "title": "Get started with Music generation using Lyria RealTime",
    "section": "Try Lyria Realtime",
    "text": "Try Lyria Realtime\nBecause of Colab limitation you won’t be able to experience the “real time” part of Lyria RealTime, so all those examples are going to be one-offs prompt to get an audio file.\nOne thing to note is that the audio will only be played at the end of the session when all would have been written in the wav file. When using the API for real you’ll be able to start plyaing as soon as the first chunk arrives. So the longer the duration (using the dedicated parameter) you set, the longer you’ll have to wait until you hear something.\n\nSimple Lyria RealTime example\nHere’s first a simple example:\n\nawait generateMusic(\n  [\n    {\n      text: \"piano\",\n      weight: 1.0,\n    },\n  ],\n  { bpm: 120, density: 1.0 }\n);\n\nLive music generation is experimental and may change in future versions.\n\n\nLyria Realtime session started\nReceiving audio chunks...\nReceived complete response\nAudio saved to ../assets/live/lyria_realtime_0.wav\n\n\n\n    Audio Response Lyria\n    \n        \n        Your browser does not support the audio element.\n    \n  \n\n\nLyria Realtime session closed\n\n\n\n\nTry Lyria RealTime by yourself\nNow you can try mixing multiple prompts, and tinkering with the music configuration.\nThe prompts needs to follow their specific format which is a list of prompts with weights (which can be any values, including negative, except 0) like this:\n{\n    \"text\": \"Text of the prompt\",\n    \"weight\": 1.0\n}\nYou should try to stay simple (unlike when you’re using image-out) as the model will better understand things like “meditation”, “eerie”, “harp” than “An eerie and relaxing music illustrating the verdoyant forests of Scotland using string instruments”.\nThe music configuration options available to you are:\n\nbpm: beats per minute\nguidance: how strictly the model follows the prompts\ndensity: density of musical notes/sounds\nbrightness: tonal quality\nscale: musical scale (key and mode)\n\nOther options are available (mute_bass for ex.). Check the documentation for the full list.\nSelect one of the sample prompts (genres, instruments and mood), or write your owns. Check the documentation for more details and prompt examples.\n\nawait generateMusic(\n  [\n    {\n      text: \"Indie Pop\",\n      weight: 0.6,\n    },\n    {\n      text: \"Sitar\",\n      weight: 2,\n    },\n    {\n      text: \"Danceable\",\n      weight: 1.4,\n    },\n  ],\n  {\n    bpm: 140,\n    scale: google.Scale.F_MAJOR_D_MINOR,\n    density: 0.2,\n    brightness: 0.7,\n    guidance: 4.0,\n  }\n);\n\nLive music generation is experimental and may change in future versions.\n\n\nLyria RealTime stream closed.\nLyria Realtime session started\nReceiving audio chunks...\nReceived complete response\nAudio saved to ../assets/live/lyria_realtime_1.wav\n\n\n\n    Audio Response Lyria\n    \n        \n        Your browser does not support the audio element.\n    \n  \n\n\nLyria Realtime session closed\nLyria RealTime stream closed.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Get started with Music generation using Lyria RealTime"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_LyriaRealtime.html#whats-next",
    "href": "quickstarts/Get_started_LyriaRealtime.html#whats-next",
    "title": "Get started with Music generation using Lyria RealTime",
    "section": "What’s next?",
    "text": "What’s next?\nNow that you know how to generate music, here are other cool things to try:\n\nInstead of music, learn how to generate multi-speakers conversation using the TTS models.\nDiscover how to generate images or videos.\nInstead of generation music or audio, find out how to Gemini can understand Audio files.\nHave a real-time conversation with Gemini using the Live API.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Get started with Music generation using Lyria RealTime"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_TTS.html",
    "href": "quickstarts/Get_started_TTS.html",
    "title": "Gemini API: Gemini Text-to-speech",
    "section": "",
    "text": "The Gemini API can transform text input into single speaker or multi-speaker audio (podcast-like experience like in NotebookLM). This notebook provides an example of how to control the Text-to-speech (TTS) capability of the Gemini model and guide its style, accent, pace, and tone.\nBefore diving in the code, you should try this capability on AI Studio.\nNote that the TTS model can only do TTS, it does not have the reasoning capabilities of the Gemini models, so you can ask things like “say this in that style”, but not “tell me why the sky is blue”. If that’s what you want, you should use the Live API instead.\nThe documentation is also a good place to start discovering the TTS capability.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Gemini Text-to-speech"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_TTS.html#setup",
    "href": "quickstarts/Get_started_TTS.html#setup",
    "title": "Gemini API: Gemini Text-to-speech",
    "section": "Setup",
    "text": "Setup\n\nInstall the Google GenAI SDK\nInstall the Google GenAI SDK from npm.\n$ npm install @google/genai\n\n\nSetup your API key\nYou can create your API key using Google AI Studio with a single click.\nRemember to treat your API key like a password. Don’t accidentally save it in a notebook or source file you later commit to GitHub. In this notebook we will be storing the API key in a .env file. You can also set it as an environment variable or use a secret manager.\nHere’s how to set it up in a .env file:\n$ touch .env\n$ echo \"GEMINI_API_KEY=&lt;YOUR_API_KEY&gt;\" &gt;&gt; .env\n\n\n\n\n\n\nTip\n\n\n\nAnother option is to set the API key as an environment variable. You can do this in your terminal with the following command:\n$ export GEMINI_API_KEY=\"&lt;YOUR_API_KEY&gt;\"\n\n\n\n\nLoad the API key\nTo load the API key from the .env file, we will use the dotenv package. This package loads environment variables from a .env file into process.env.\n$ npm install dotenv\nThen, we can load the API key in our code:\n\nconst dotenv = require(\"dotenv\") as typeof import(\"dotenv\");\n\ndotenv.config({\n  path: \"../.env\",\n});\n\nconst GEMINI_API_KEY = process.env.GEMINI_API_KEY ?? \"\";\nif (!GEMINI_API_KEY) {\n  throw new Error(\"GEMINI_API_KEY is not set in the environment variables\");\n}\nconsole.log(\"GEMINI_API_KEY is set in the environment variables\");\n\nGEMINI_API_KEY is set in the environment variables\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn our particular case the .env is is one directory up from the notebook, hence we need to use ../ to go up one directory. If the .env file is in the same directory as the notebook, you can omit it altogether.\n│\n├── .env\n└── quickstarts\n    └── Get_started_TTS.ipynb\n\n\n\n\nInitialize SDK Client\nWith the new SDK, now you only need to initialize a client with you API key (or OAuth if using Vertex AI). The model is now set in each call.\n\nconst google = require(\"@google/genai\") as typeof import(\"@google/genai\");\n\nconst ai = new google.GoogleGenAI({ apiKey: GEMINI_API_KEY });\n\n\n\nSelect a model\nAudio-out is only supported by the “tts” models, gemini-2.5-flash-preview-tts and gemini-2.5-pro-preview-tts. For more information about all Gemini models, check the documentation for extended information on each of them.\n\nconst tslab = require(\"tslab\") as typeof import(\"tslab\");\n\nconst MODEL_ID = \"gemini-2.5-flash-preview-tts\";\n\n\n\nUtilites\nThe simplest way to playback the audio in Colab, is to write it out to a .wav file. So here is a simple wave file writer:\n\nconst fs = require(\"fs\") as typeof import(\"fs\");\nconst path = require(\"path\") as typeof import(\"path\");\nconst wave = require(\"wavefile\") as typeof import(\"wavefile\");\n\nfunction saveAudioToFile(audioData: Int16Array, filePath: string) {\n  fs.mkdirSync(path.dirname(filePath), { recursive: true });\n  const wav = new wave.WaveFile();\n  wav.fromScratch(1, 24000, \"16\", audioData);\n  fs.writeFileSync(filePath, wav.toBuffer());\n  console.debug(`Audio saved to ${filePath}`);\n}\n\nfunction base64ToInt16Array(base64String: string): Int16Array {\n  const buffer = Buffer.from(base64String, \"base64\");\n  const int16Array = new Int16Array(buffer.buffer, buffer.byteOffset, buffer.length / Int16Array.BYTES_PER_ELEMENT);\n  return int16Array;\n}\n\n\nimport { GenerateContentResponse } from \"@google/genai\";\n\nfunction playAudio(response: GenerateContentResponse, filePath: string) {\n  if (response.candidates?.[0]?.content) {\n    const response_content = response.candidates[0].content;\n    if (response_content.parts) {\n      const response_blob = response_content.parts[0].inlineData;\n      if (response_blob?.data) {\n        const response_filepath = path.join(\"../assets/tts\", filePath);\n        saveAudioToFile(base64ToInt16Array(response_blob.data), response_filepath);\n        tslab.display.html(`\n                  &lt;audio controls&gt;\n                      &lt;source src=\"${response_filepath}\" type=\"audio/wav\"&gt;\n                      Your browser does not support the audio element.\n                  &lt;/audio&gt;\n              `);\n      }\n    }\n  }\n}",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Gemini Text-to-speech"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_TTS.html#generate-a-simple-audio-output",
    "href": "quickstarts/Get_started_TTS.html#generate-a-simple-audio-output",
    "title": "Gemini API: Gemini Text-to-speech",
    "section": "Generate a simple audio output",
    "text": "Generate a simple audio output\nLet’s start with something simple:\n\nconst simple_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: [\"Say 'hello, my name is Gemini!'\"],\n  config: {\n    responseModalities: [google.Modality.AUDIO],\n  },\n});\n\nThe generated ouput is in the response inlineData and as you can see it’s indeed audio data.\n\nplayAudio(simple_response, `simple_response.wav`);\n\nAudio saved to ../assets/tts/simple_response.wav\n\n\n\n                  \n                      \n                      Your browser does not support the audio element.\n                  \n              \n\n\n\n\n\n\n\n\nNote\n\n\n\nNote that the model can only do TTS, so you should always tell it to “say”, “read”, “TTS” something, otherwise it won’t do anything.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Gemini Text-to-speech"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_TTS.html#control-how-the-model-speaks",
    "href": "quickstarts/Get_started_TTS.html#control-how-the-model-speaks",
    "title": "Gemini API: Gemini Text-to-speech",
    "section": "Control how the model speaks",
    "text": "Control how the model speaks\nThere are 30 different built-in voices you can use and 24 supported languages which gives you plenty of combinations to try.\n\nChoose a voice\nChoose a voice among the 30 different ones. You can find their characteristics in the documentation.\n\nconst VOICE_ID = \"Leda\";\n\n\nconst custom_voice_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: [\n    `Say \"I am a very knowlegeable model, especially when using grounding\", wait 5 seconds then say \"Don't you think?\".`,\n  ],\n  config: {\n    responseModalities: [google.Modality.AUDIO],\n    speechConfig: {\n      voiceConfig: {\n        prebuiltVoiceConfig: {\n          voiceName: VOICE_ID,\n        },\n      },\n    },\n  },\n});\nplayAudio(custom_voice_response, `custom_voice_response.wav`);\n\nAudio saved to ../assets/tts/custom_voice_response.wav\n\n\n\n                  \n                      \n                      Your browser does not support the audio element.\n                  \n              \n\n\n\n\nChange the language\nJust tell the model to speak in a certain language and it will. The documentation lists all the supported ones.\n\nconst custom_language_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: [\n    `\n    Read this in French:\n\n    Les chaussettes de l'archiduchesse sont-elles sèches ? Archi-sèches ?\n    Un chasseur sachant chasser doit savoir chasser sans son chien.\n    `,\n  ],\n  config: {\n    responseModalities: [google.Modality.AUDIO],\n  },\n});\nplayAudio(custom_language_response, `custom_language_response.wav`);\n\nAudio saved to ../assets/tts/custom_language_response.wav\n\n\n\n                  \n                      \n                      Your browser does not support the audio element.\n                  \n              \n\n\n\n\nPrompt the model to speak in certain ways\nYou can control style, tone, accent, and pace using natural language prompts, for example:\n\nconst custom_style_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: [\n    `\n    Say in an spooky whisper:\n    \"By the pricking of my thumbs...\n    Something wicked this way comes!\"\n    `,\n  ],\n  config: {\n    responseModalities: [google.Modality.AUDIO],\n  },\n});\nplayAudio(custom_style_response, `custom_style_response.wav`);\n\nAudio saved to ../assets/tts/custom_style_response.wav\n\n\n\n                  \n                      \n                      Your browser does not support the audio element.\n                  \n              \n\n\n\nconst custom_pace_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: [\n    `\n    Read this disclaimer in as fast a voice as possible while remaining intelligible:\n\n    [The author] assumes no responsibility or liability for any errors or omissions in the content of this site.\n    The information contained in this site is provided on an 'as is' basis with no guarantees of completeness, accuracy, usefulness or timeliness\n    `,\n  ],\n  config: {\n    responseModalities: [google.Modality.AUDIO],\n  },\n});\nplayAudio(custom_pace_response, `custom_pace_response.wav`);\n\nAudio saved to ../assets/tts/custom_pace_response.wav\n\n\n\n                  \n                      \n                      Your browser does not support the audio element.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Gemini Text-to-speech"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_TTS.html#mutlti-speakers",
    "href": "quickstarts/Get_started_TTS.html#mutlti-speakers",
    "title": "Gemini API: Gemini Text-to-speech",
    "section": "Mutlti-speakers",
    "text": "Mutlti-speakers\nThe TTS model can also read discussions between 2 speakers (like NotebookLM podcast feature). You just need to tell it that there are two speakers:\n\nconst multi_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: [\n    `\n    Make Speaker1 sound tired and bored, and Speaker2 sound excited and happy:\n\n    Speaker1: So... what's on the agenda today?\n    Speaker2: You're never going to guess!\n    `,\n  ],\n  config: {\n    responseModalities: [google.Modality.AUDIO],\n  },\n});\nplayAudio(multi_response, `multi_response.wav`);\n\nAudio saved to ../assets/tts/multi_response.wav\n\n\n\n                  \n                      \n                      Your browser does not support the audio element.\n                  \n              \n\n\nYou can also select the voices for each participants and pass their names to the model.\nBut first let’s generate a discussion between two scientists:\n\nconst multi_speaker_transcript = await ai.models.generateContent({\n  model: \"gemini-2.5-flash-preview-05-20\",\n  contents: [\n    `\n    Hi, please generate a short (like 100 words) transcript that reads like\n    it was clipped from a podcast by excited herpetologists, Dr. Claire and\n    her assistant, the young Aurora.\n    `,\n  ],\n});\ntslab.display.markdown(multi_speaker_transcript.text ?? \"\");\n\n(Sound of distant jungle chirps fading, replaced by a slightly crackly podcast mic)\nDr. Claire: (Practically bouncing) I’m still buzzing, Aurora! Genuinely buzzing!\nAurora: (Squealing) Dr. Claire, my hands are still shaking! The Emerald Mossback! We actually saw one!\nDr. Claire: Not just saw it, Aurora! We documented its full iridescent display! The way it melded with the bromeliads… it was indistinguishable! The camouflage… it’s beyond anything in the textbooks!\nAurora: It was like watching magic! No wonder they were thought extinct for decades! Who could ever spot that? My heart was pounding!\nDr. Claire: Precisely! This changes our entire understanding of its crypsis. This episode is going to be legendary for ‘Reptile Revelations’!\nAurora: Totally! Best day ever, Dr. Claire! Best. Day. Ever!\n\n\n\nconst text = multi_speaker_transcript.text ?? \"\";\nconst transcript_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: [\n    `TTS the following conversation between a very excited Dr. Claire and her assistant, the young Aurora: ${text}`,\n  ],\n  config: {\n    responseModalities: [google.Modality.AUDIO],\n    speechConfig: {\n      multiSpeakerVoiceConfig: {\n        speakerVoiceConfigs: [\n          {\n            speaker: \"Dr. Claire\",\n            voiceConfig: {\n              prebuiltVoiceConfig: {\n                voiceName: \"Aoede\",\n              },\n            },\n          },\n          {\n            speaker: \"Aurora\",\n            voiceConfig: {\n              prebuiltVoiceConfig: {\n                voiceName: \"Leda\",\n              },\n            },\n          },\n        ],\n      },\n    },\n  },\n});\nplayAudio(transcript_response, `transcript_response.wav`);\n\nAudio saved to ../assets/tts/transcript_response.wav\n\n\n\n                  \n                      \n                      Your browser does not support the audio element.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Gemini Text-to-speech"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_TTS.html#whats-next",
    "href": "quickstarts/Get_started_TTS.html#whats-next",
    "title": "Gemini API: Gemini Text-to-speech",
    "section": "What’s next?",
    "text": "What’s next?\nNow that you know how to generate multi-speaker conversations, here are other cool things to try:\n\nInstead of speech, learn how to generate music conversation using the Lyria RealTime.\nDiscover how to generate images or videos.\nInstead of generation music or audio, find out how to Gemini can understand Audio files.\nHave a real-time conversation with Gemini using the Live API.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Gemini Text-to-speech"
    ]
  },
  {
    "objectID": "quickstarts/Get_started.html",
    "href": "quickstarts/Get_started.html",
    "title": "Gemini API: Getting started with Gemini models",
    "section": "",
    "text": "The new Google Gen AI SDK provides a unified interface to Gemini models through both the Gemini Developer API and the Gemini API on Vertex AI. With a few exceptions, code that runs on one platform will run on both. This notebook uses the Developer API.\nThis notebook will walk you through:\nMore details about this new SDK on the documentation.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Getting started with Gemini models"
    ]
  },
  {
    "objectID": "quickstarts/Get_started.html#setup",
    "href": "quickstarts/Get_started.html#setup",
    "title": "Gemini API: Getting started with Gemini models",
    "section": "Setup",
    "text": "Setup\n\nInstall the Google GenAI SDK\nInstall the Google GenAI SDK from npm.\n$ npm install @google/genai\n\n\nSetup your API key\nYou can create your API key using Google AI Studio with a single click.\nRemember to treat your API key like a password. Don’t accidentally save it in a notebook or source file you later commit to GitHub. In this notebook we will be storing the API key in a .env file. You can also set it as an environment variable or use a secret manager.\nHere’s how to set it up in a .env file:\n$ touch .env\n$ echo \"GEMINI_API_KEY=&lt;YOUR_API_KEY&gt;\" &gt;&gt; .env\n\n\n\n\n\n\nTip\n\n\n\nAnother option is to set the API key as an environment variable. You can do this in your terminal with the following command:\n$ export GEMINI_API_KEY=\"&lt;YOUR_API_KEY&gt;\"\n\n\n\n\nLoad the API key\nTo load the API key from the .env file, we will use the dotenv package. This package loads environment variables from a .env file into process.env.\n$ npm install dotenv\nThen, we can load the API key in our code:\n\nconst dotenv = require(\"dotenv\") as typeof import(\"dotenv\");\n\ndotenv.config({\n  path: \"../.env\",\n});\n\nconst GEMINI_API_KEY = process.env.GEMINI_API_KEY ?? \"\";\nif (!GEMINI_API_KEY) {\n  throw new Error(\"GEMINI_API_KEY is not set in the environment variables\");\n}\nconsole.log(\"GEMINI_API_KEY is set in the environment variables\");\n\nGEMINI_API_KEY is set in the environment variables\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn our particular case the .env is is one directory up from the notebook, hence we need to use ../ to go up one directory. If the .env file is in the same directory as the notebook, you can omit it altogether.\n│\n├── .env\n└── quickstarts\n    └── Get_started.ipynb",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Getting started with Gemini models"
    ]
  },
  {
    "objectID": "quickstarts/Get_started.html#initialize-sdk-client",
    "href": "quickstarts/Get_started.html#initialize-sdk-client",
    "title": "Gemini API: Getting started with Gemini models",
    "section": "Initialize SDK Client",
    "text": "Initialize SDK Client\nWith the new SDK, now you only need to initialize a client with you API key (or OAuth if using Vertex AI). The model is now set in each call.\n\nconst google = require(\"@google/genai\") as typeof import(\"@google/genai\");\n\nconst ai = new google.GoogleGenAI({ apiKey: GEMINI_API_KEY });",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Getting started with Gemini models"
    ]
  },
  {
    "objectID": "quickstarts/Get_started.html#choose-a-model",
    "href": "quickstarts/Get_started.html#choose-a-model",
    "title": "Gemini API: Getting started with Gemini models",
    "section": "Choose a model",
    "text": "Choose a model\nSelect the model you want to use in this guide. You can either select one from the list or enter a model name manually. Keep in mind that some models, such as the 2.5 ones are thinking models and thus take slightly more time to respond. For more details, you can see thinking notebook to learn how to switch the thinking off.\nFor a full overview of all Gemini models, check the documentation.\n\nconst MODEL_ID = \"gemini-2.5-flash-preview-05-20\";",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Getting started with Gemini models"
    ]
  },
  {
    "objectID": "quickstarts/Get_started.html#send-text-prompts",
    "href": "quickstarts/Get_started.html#send-text-prompts",
    "title": "Gemini API: Getting started with Gemini models",
    "section": "Send text prompts",
    "text": "Send text prompts\nUse the models.generateContent method to generate responses to your prompts. You can pass text directly to models.generateContent and use the .text property to get the text content of the response. Note that the .text field will work when there’s only one part in the output.\n\nconst tslab = require(\"tslab\") as typeof import(\"tslab\");\n\nconst response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: \"What's the largest planet in our solar system?\",\n});\n\ntslab.display.markdown(response.text ?? \"\");\n\nThe largest planet in our solar system is Jupiter.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Getting started with Gemini models"
    ]
  },
  {
    "objectID": "quickstarts/Get_started.html#count-tokens",
    "href": "quickstarts/Get_started.html#count-tokens",
    "title": "Gemini API: Getting started with Gemini models",
    "section": "Count tokens",
    "text": "Count tokens\nTokens are the basic inputs to the Gemini models. You can use the models.countTokens method to calculate the number of input tokens before sending a request to the Gemini API.\n\nconst count = await ai.models.countTokens({\n  model: MODEL_ID,\n  contents: \"What's the highest mountain in Africa?\",\n});\n\nconsole.log(JSON.stringify(count, null, 2));\n\n{\n  \"totalTokens\": 10\n}",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Getting started with Gemini models"
    ]
  },
  {
    "objectID": "quickstarts/Get_started.html#send-multimodal-prompts",
    "href": "quickstarts/Get_started.html#send-multimodal-prompts",
    "title": "Gemini API: Getting started with Gemini models",
    "section": "Send multimodal prompts",
    "text": "Send multimodal prompts\nUse Gemini 2.0 model (gemini-2.0-flash), a multimodal model that supports multimodal prompts. You can include text, PDF documents, images, audio and video in your prompt requests and get text or code responses.\nIn this first example, you’ll download an image from a specified URL, save it as a byte stream and then write those bytes to a local file named jetpack.png.\n\nconst fs = require(\"fs\") as typeof import(\"fs\");\nconst path = require(\"path\") as typeof import(\"path\");\n\nconst IMG_URL = \"https://storage.googleapis.com/generativeai-downloads/data/jetpack.png\";\n\nconst downloadFile = async (url: string, filePath: string) =&gt; {\n  const response = await fetch(url);\n  if (!response.ok) {\n    throw new Error(`Failed to download image: ${response.statusText}`);\n  }\n  const buffer = await response.blob();\n  const bufferData = Buffer.from(await buffer.arrayBuffer());\n  fs.writeFileSync(filePath, bufferData);\n};\n\nconst filePath = path.join(\"../assets\", \"jetpack.png\");\nawait downloadFile(IMG_URL, filePath);\n\nIn this second example, you’ll open a previously saved image, create a thumbnail of it and then generate a short blog post based on the thumbnail, displaying both the thumbnail and the generated blog post. The deferredFileUpload is a helper function that waits for the model to finish processing the file before returning the response. This is useful when you want to upload a file and then reference it in a follow-up request. The deferredFileUpload function will return a promise that resolves when the file is ready to be used in the next request.\n\nimport { File, FileState } from \"@google/genai\";\n\ntslab.display.png(fs.readFileSync(\"../assets/jetpack.png\"));\n\nasync function deferredFileUpload(filePath: string, config: { displayName: string }): Promise&lt;File&gt; {\n  const file = await ai.files.upload({\n    file: filePath,\n    config,\n  });\n  let getFile = await ai.files.get({ name: file.name ?? \"\" });\n  while (getFile.state === FileState.PROCESSING) {\n    getFile = await ai.files.get({ name: file.name ?? \"\" });\n    console.log(`current file status: ${getFile.state ?? \"unknown\"}`);\n    console.log(\"File is still processing, retrying in 5 seconds\");\n\n    await new Promise((resolve) =&gt; {\n      setTimeout(resolve, 1000);\n    });\n  }\n  if (file.state === FileState.FAILED) {\n    throw new Error(\"File processing failed.\");\n  }\n  return file;\n}\n\ntry {\n  const file = await deferredFileUpload(filePath, {\n    displayName: \"jetpack.png\",\n  });\n  console.log(\"File uploaded successfully\", file.name ?? \"\");\n  if (!file.uri || !file.mimeType) {\n    throw new Error(\"File URI or MIME type is missing\");\n  }\n  const blog = await ai.models.generateContent({\n    model: MODEL_ID,\n    contents: [\n      \"Write a short and engaging blog post based on this picture.\",\n      google.createPartFromUri(file.uri, file.mimeType),\n    ],\n  });\n  tslab.display.markdown(blog.text ?? \"\");\n} catch (error) {\n  console.error(\"Error uploading file:\", error);\n  throw error;\n}\n\n\n\n\n\n\n\n\nFile uploaded successfully files/lqnru1a65qjn\n\n\nHere’s a short, engaging blog post based on the sketch:\n\n\nThe Jetpack Backpack Concept: Is This the Future of Your Commute?\nStuck in traffic? Tired of lugging a heavy backpack across campus or the city? What if your backpack could give you a little… boost?\nCheck out this cool concept sketch we stumbled upon: The Jetpack Backpack!\nFrom the looks of it, someone’s been dreaming up a truly futuristic way to carry your gear. On the surface, it’s a functional backpack – described as lightweight, with padded strap support, and even spacious enough to fit an 18-inch laptop. It’s designed to look like a normal backpack, so maybe you won’t get too many stares before lift-off.\nBut the real magic happens when those retractable boosters kick in! Powered by steam (hello, surprisingly green and clean tech!), this concept promises a new dimension to personal transport. Charging is even a modern USB-C affair.\nNow, the sketch notes a 15-minute battery life. So maybe it’s not for your cross-country road trip replacement just yet! But imagine skipping that final mile of gridlock, hopping over stairs, or just making a truly epic entrance.\nThis sketch reminds us that innovation often starts with a wild idea and a pen on paper. While this might be firmly in the concept realm for now, it’s fun to imagine the possibilities!\nWhat do you think? Would you strap into a Jetpack Backpack? Let us know in the comments!",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Getting started with Gemini models"
    ]
  },
  {
    "objectID": "quickstarts/Get_started.html#the-jetpack-backpack-concept-is-this-the-future-of-your-commute",
    "href": "quickstarts/Get_started.html#the-jetpack-backpack-concept-is-this-the-future-of-your-commute",
    "title": "Gemini API: Getting started with Gemini models",
    "section": "The Jetpack Backpack Concept: Is This the Future of Your Commute?",
    "text": "The Jetpack Backpack Concept: Is This the Future of Your Commute?\nStuck in traffic? Tired of lugging a heavy backpack across campus or the city? What if your backpack could give you a little… boost?\nCheck out this cool concept sketch we stumbled upon: The Jetpack Backpack!\nFrom the looks of it, someone’s been dreaming up a truly futuristic way to carry your gear. On the surface, it’s a functional backpack – described as lightweight, with padded strap support, and even spacious enough to fit an 18-inch laptop. It’s designed to look like a normal backpack, so maybe you won’t get too many stares before lift-off.\nBut the real magic happens when those retractable boosters kick in! Powered by steam (hello, surprisingly green and clean tech!), this concept promises a new dimension to personal transport. Charging is even a modern USB-C affair.\nNow, the sketch notes a 15-minute battery life. So maybe it’s not for your cross-country road trip replacement just yet! But imagine skipping that final mile of gridlock, hopping over stairs, or just making a truly epic entrance.\nThis sketch reminds us that innovation often starts with a wild idea and a pen on paper. While this might be firmly in the concept realm for now, it’s fun to imagine the possibilities!\nWhat do you think? Would you strap into a Jetpack Backpack? Let us know in the comments!",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Getting started with Gemini models"
    ]
  },
  {
    "objectID": "quickstarts/Get_started.html#configure-model-parameters",
    "href": "quickstarts/Get_started.html#configure-model-parameters",
    "title": "Gemini API: Getting started with Gemini models",
    "section": "Configure model parameters",
    "text": "Configure model parameters\nYou can include parameter values in each call that you send to a model to control how the model generates a response. Learn more about experimenting with parameter values.\n\nconst varied_params_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: \"Tell me how the internet works, but pretend I'm a puppy who only understands squeaky toys.\",\n  config: {\n    temperature: 0.4,\n    topP: 0.95,\n    topK: 20,\n    candidateCount: 1,\n    seed: 5,\n    stopSequences: [\"STOP!\"],\n    presencePenalty: 0.0,\n    frequencyPenalty: 0.0,\n  },\n});\n\ntslab.display.markdown(varied_params_response.text ?? \"\");\n\nOkay, listen up, little fluff-ball! Squeak!\nYou know how you love a good squeak? Squeak squeak! What if the best squeak is way over there? Points vaguely Like, across the room, or even outside?\nYou want that squeak! So, your brain goes whirr and makes a request for the squeak. But you can’t just send one giant WOOF of squeak-wanting. It gets broken into tiny, tiny little squeaky bits! Imagine tiny squeaks floating!\nAnd each little squeaky bit needs a special smell attached, like a ‘Go to the Red Ball’ smell, so it knows where to go. That’s the address! Sniff sniff!\nThese little squeaky bits, with their special smells, run out into the world! Waggy tail zoom! But the world is big! They need help.\nThat’s where the Sniffy Guides come in! Imagine little noses pointing! These Sniffy Guides (like magic noses!) sniff the special smell on each squeaky bit and say, ‘Oh, this one goes that way!’ and point it along the path. Point point! They send the squeaky bits from one Sniffy Guide to the next, all over the house and yard!\nFinally, all the little squeaky bits, following their special smell and the Sniffy Guides, arrive at the Big Squeaky Toy Box! Imagine a giant box full of squeaks! This is where the real squeak lives!\nThe Big Squeaky Toy Box sees all your little squeaky bits asking for the squeak. So, it gets the actual squeak ready! SQUEAK!\nAnd guess what? It breaks that big squeak into little squeaky bits too! More tiny squeaks! And puts your special smell (or maybe a ‘Come Back Home’ smell) on them. Sniff sniff!\nThese new squeaky bits, carrying the real squeak, follow the Sniffy Guides all the way back to you! Zoom zoom! They sniff their way through the house, guided by the magic noses.\nWhen all the little squeaky bits arrive back at your ears, they put themselves back together! Click! And POP! You hear the wonderful SQUEAK you asked for! Happy tail wag!\nAnd there are special Squeaky Rules for how the squeaky bits travel and how the Sniffy Guides work, so everyone gets their squeaks without bumping into each other! Good puppy!\nSo, the internet is just a super-duper, giant network of Sniffy Guides and Big Squeaky Toy Boxes, sending little squeaky bits with special smells back and forth so puppies (and humans!) can get the squeaks they want, no matter how far away!\nSQUEAK! Good boy/girl! Now go chase that tail!",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Getting started with Gemini models"
    ]
  },
  {
    "objectID": "quickstarts/Get_started.html#configure-safety-filters",
    "href": "quickstarts/Get_started.html#configure-safety-filters",
    "title": "Gemini API: Getting started with Gemini models",
    "section": "Configure safety filters",
    "text": "Configure safety filters\nThe Gemini API provides safety filters that you can adjust across multiple filter categories to restrict or allow certain types of content. You can use these filters to adjust what is appropriate for your use case. See the Configure safety filters page for details.\nIn this example, you’ll use a safety filter to only block highly dangerous content, when requesting the generation of potentially disrespectful phrases.\n\nconst filtered_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents:\n    \"Write a list of 2 disrespectful things that I might say to the universe after stubbing my toe in the dark.\",\n  config: {\n    safetySettings: [\n      {\n        category: google.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,\n        threshold: google.HarmBlockThreshold.BLOCK_NONE,\n      },\n    ],\n  },\n});\ntslab.display.markdown(filtered_response.text ?? \"\");\n\nHere are 2 disrespectful things you might say to the universe after stubbing your toe in the dark:\n\n“Seriously, universe?! Did you plan that?!”\n“Oh, thanks, universe. Really needed that.” (Said with heavy sarcasm)",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Getting started with Gemini models"
    ]
  },
  {
    "objectID": "quickstarts/Get_started.html#start-a-multi-turn-chat",
    "href": "quickstarts/Get_started.html#start-a-multi-turn-chat",
    "title": "Gemini API: Getting started with Gemini models",
    "section": "Start a multi-turn chat",
    "text": "Start a multi-turn chat\nThe Gemini API enables you to have freeform conversations across multiple turns.\nNext you’ll set up a helpful coding assistant:\n\nconst system_prompt = `\nYou are an expert software developer and a helpful coding assistant.\nYou are able to generate high-quality code in any programming language.\n`;\n\nconst chat = ai.chats.create({\n  model: MODEL_ID,\n  config: {\n    systemInstruction: system_prompt,\n  },\n});\n\nUse chat.sendMessage to pass a message back and receive a response.\n\nconst chat_response_1 = await chat.sendMessage({\n  message: \"Write a function that checks if a year is a leap year.\",\n});\ntslab.display.markdown(chat_response_1.text ?? \"\");\n\nOkay, here’s a function in Python that checks if a year is a leap year based on the standard Gregorian calendar rules.\nLeap Year Rules:\n\nA year is a leap year if it is divisible by 4.\nHowever, if the year is divisible by 100, it is NOT a leap year.\nBut, if the year is divisible by 400, it IS a leap year.\n\nLet’s translate these rules into code.\ndef is_leap(year):\n  \"\"\"\n  Checks if a given year is a leap year according to the Gregorian calendar rules.\n\n  Args:\n    year: An integer representing the year.\n\n  Returns:\n    True if the year is a leap year, False otherwise.\n  \"\"\"\n  # Rule 1: Check if divisible by 4\n  if year % 4 == 0:\n    # Rule 2: Check if divisible by 100\n    if year % 100 == 0:\n      # Rule 3: Check if divisible by 400 (exception to rule 2)\n      if year % 400 == 0:\n        return True  # Divisible by 400, so it's a leap year\n      else:\n        return False # Divisible by 100 but not 400, so not a leap year\n    else:\n      return True  # Divisible by 4 but not 100, so it's a leap year\n  else:\n    return False   # Not divisible by 4, so not a leap year\n\n# --- Example Usage ---\n\nprint(f\"Is 2000 a leap year? {is_leap(2000)}\") # Expected: True (Divisible by 400)\nprint(f\"Is 1900 a leap year? {is_leap(1900)}\") # Expected: False (Divisible by 100 but not 400)\nprint(f\"Is 2024 a leap year? {is_leap(2024)}\") # Expected: True (Divisible by 4 but not 100)\nprint(f\"Is 2023 a leap year? {is_leap(2023)}\") # Expected: False (Not divisible by 4)\nprint(f\"Is 1600 a leap year? {is_leap(1600)}\") # Expected: True (Divisible by 400)\nprint(f\"Is 2100 a leap year? {is_leap(2100)}\") # Expected: False (Divisible by 100 but not 400)\nMore Concise Version (using boolean logic):\nYou can also combine the conditions into a single boolean expression:\ndef is_leap_concise(year):\n  \"\"\"\n  Checks if a given year is a leap year using a concise boolean expression.\n\n  Args:\n    year: An integer representing the year.\n\n  Returns:\n    True if the year is a leap year, False otherwise.\n  \"\"\"\n  return (year % 4 == 0 and year % 100 != 0) or (year % 400 == 0)\n\n# --- Example Usage (using the concise version) ---\nprint(\"\\nUsing concise version:\")\nprint(f\"Is 2000 a leap year? {is_leap_concise(2000)}\") # Expected: True\nprint(f\"Is 1900 a leap year? {is_leap_concise(1900)}\") # Expected: False\nprint(f\"Is 2024 a leap year? {is_leap_concise(2024)}\") # Expected: True\nprint(f\"Is 2023 a leap year? {is_leap_concise(2023)}\") # Expected: False\nBoth functions implement the same logic and produce the correct results. The first version using nested if/else might be slightly easier to read for beginners, while the second version is more compact.\n\n\n\nconst chat_response_2 = await chat.sendMessage({\n  message: \"Okay, write a unit test of the generated function.\",\n});\ntslab.display.markdown(chat_response_2.text ?? \"\");\n\nOkay, let’s write a unit test for the is_leap function using Python’s built-in unittest framework.\nFirst, make sure you have the is_leap function available. You can either put the function in the same file as the tests or import it from another file. For this example, we’ll assume it’s in the same file.\nimport unittest\n\n# Assume the function you want to test is defined here (or imported)\ndef is_leap(year):\n  \"\"\"\n  Checks if a given year is a leap year according to the Gregorian calendar rules.\n\n  Args:\n    year: An integer representing the year.\n\n  Returns:\n    True if the year is a leap year, False otherwise.\n  \"\"\"\n  # Rule 1: Check if divisible by 4\n  if year % 4 == 0:\n    # Rule 2: Check if divisible by 100\n    if year % 100 == 0:\n      # Rule 3: Check if divisible by 400 (exception to rule 2)\n      if year % 400 == 0:\n        return True  # Divisible by 400, so it's a leap year\n      else:\n        return False # Divisible by 100 but not 400, so not a leap year\n    else:\n      return True  # Divisible by 4 but not 100, so it's a leap year\n  else:\n    return False   # Not divisible by 4, so not a leap year\n\n# ---------------------------------------------------------------------\n# Unit Tests\n# ---------------------------------------------------------------------\n\nclass TestIsLeapYear(unittest.TestCase):\n    \"\"\"\n    Test cases for the is_leap function.\n    \"\"\"\n\n    def test_divisible_by_4_not_by_100(self):\n        \"\"\"Years divisible by 4 but not by 100 should be leap years.\"\"\"\n        self.assertTrue(is_leap(2024))\n        self.assertTrue(is_leap(2020))\n        self.assertTrue(is_leap(1996))\n        self.assertTrue(is_leap(4)) # Test a small year\n\n    def test_divisible_by_100_not_by_400(self):\n        \"\"\"Years divisible by 100 but not by 400 should NOT be leap years.\"\"\"\n        self.assertFalse(is_leap(1900))\n        self.assertFalse(is_leap(2100))\n        self.assertFalse(is_leap(1800))\n        self.assertFalse(is_leap(100)) # Test a small year\n\n    def test_divisible_by_400(self):\n        \"\"\"Years divisible by 400 should be leap years.\"\"\"\n        self.assertTrue(is_leap(2000))\n        self.assertTrue(is_leap(1600))\n        self.assertTrue(is_leap(2400))\n        self.assertTrue(is_leap(400)) # Test a small year\n\n    def test_not_divisible_by_4(self):\n        \"\"\"Years not divisible by 4 should NOT be leap years.\"\"\"\n        self.assertFalse(is_leap(2023))\n        self.assertFalse(is_leap(2025))\n        self.assertFalse(is_leap(1999))\n        self.assertFalse(is_leap(1)) # Test a small year\n\n# This allows running the tests directly from the command line\nif __name__ == '__main__':\n    unittest.main(argv=['first-arg-is-ignored'], exit=False) # Added argv/exit for compatibility in some environments like notebooks\nExplanation:\n\nimport unittest: Imports the necessary testing framework.\nimport is_leap: (If is_leap is in a separate file, e.g., my_module.py, you would use from my_module import is_leap).\nclass TestIsLeapYear(unittest.TestCase):: Creates a test class that inherits from unittest.TestCase. This class will contain the individual test methods.\ntest_... methods: Each method starting with test_ is automatically recognized by unittest as a test case.\nDocstrings: The docstrings within the test methods explain what scenario each test is covering, which is good practice.\nAssertions: Inside each test method, we use assertion methods provided by unittest.TestCase:\n\nself.assertTrue(expression): Asserts that the expression evaluates to True.\nself.assertFalse(expression): Asserts that the expression evaluates to False.\nWe call is_leap() with specific years that represent each rule of the leap year logic and assert the expected boolean result.\n\nif __name__ == '__main__':: This block ensures that the unittest.main() function is called only when the script is executed directly (not when imported as a module).\nunittest.main(): This function discovers and runs the tests defined in classes inheriting from unittest.TestCase within the script.\n\nHow to Run the Tests:\n\nSave the code above as a Python file (e.g., test_leap_year.py).\nOpen a terminal or command prompt.\nNavigate to the directory where you saved the file.\nRun the command: python test_leap_year.py\n\nYou will see output indicating how many tests ran and whether they passed or failed. If all tests pass, it means your is_leap function is correctly implementing the standard Gregorian leap year rules for the test cases provided.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Getting started with Gemini models"
    ]
  },
  {
    "objectID": "quickstarts/Get_started.html#save-and-resume-a-chat",
    "href": "quickstarts/Get_started.html#save-and-resume-a-chat",
    "title": "Gemini API: Getting started with Gemini models",
    "section": "Save and resume a chat",
    "text": "Save and resume a chat\nYou can use the chat.getHistory method to get the history of the chat. This will return an array of Content[] objects, which you can use to resume the chat later.\n\nconst chat_history = chat.getHistory();\nconsole.log(JSON.stringify(chat_history[0], null, 2));\nconst new_chat = ai.chats.create({\n  model: MODEL_ID,\n  config: {\n    systemInstruction: system_prompt,\n  },\n  history: chat_history,\n});\nconst chat_response_3 = await new_chat.sendMessage({\n  message: \"What was the name of the function again?\",\n});\ntslab.display.markdown(chat_response_3.text ?? \"\");\n\n{\n  \"role\": \"user\",\n  \"parts\": [\n    {\n      \"text\": \"Write a function that checks if a year is a leap year.\"\n    }\n  ]\n}\n\n\nThe name of the function is is_leap.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Getting started with Gemini models"
    ]
  },
  {
    "objectID": "quickstarts/Get_started.html#serialize-and-deserialize-a-chat",
    "href": "quickstarts/Get_started.html#serialize-and-deserialize-a-chat",
    "title": "Gemini API: Getting started with Gemini models",
    "section": "Serialize and deserialize a chat",
    "text": "Serialize and deserialize a chat\nIn the above example we just saved the chat history in a variable and reused it. But that’s not very practical, is it? To overcome this we can serialize and deserialize the chat history. This way we can save it to a file or a database and load it later. Unfortunately, the SDK doesn’t provide a method to do this yet, but we can do it manually.\n\nimport { Content } from \"@google/genai\";\n\nconst serialized_chat = JSON.stringify(chat_history, null, 2);\nfs.writeFileSync(path.join(\"../assets\", \"chat_history.json\"), serialized_chat);\n\nconst chat_history_file = fs.readFileSync(path.join(\"../assets\", \"chat_history.json\"), \"utf-8\");\nconst chat_history_data = JSON.parse(chat_history_file) as Content[];\nconst new_chat_from_file = ai.chats.create({\n  model: MODEL_ID,\n  config: {\n    systemInstruction: system_prompt,\n  },\n  history: chat_history_data,\n});\nconst chat_response_4 = await new_chat_from_file.sendMessage({\n  message: \"What was the name of the function again?\",\n});\ntslab.display.markdown(chat_response_4.text ?? \"\");\n\nThe name of the function is is_leap.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Getting started with Gemini models"
    ]
  },
  {
    "objectID": "quickstarts/Get_started.html#generate-json",
    "href": "quickstarts/Get_started.html#generate-json",
    "title": "Gemini API: Getting started with Gemini models",
    "section": "Generate JSON",
    "text": "Generate JSON\nThe controlled generation capability in Gemini API allows you to constraint the model output to a structured format. You can provide the schemas as Schema objects.\n\nimport { Schema, Type } from \"@google/genai\";\n\nconst RecipeSchema = {\n  type: Type.OBJECT,\n  description: \"A structured representation of a cooking recipe\",\n  properties: {\n    recipeName: {\n      type: Type.STRING,\n      description: \"The name of the recipe\",\n    },\n    recipeDescription: {\n      type: Type.STRING,\n      description: \"A short description of the recipe\",\n    },\n    ingredients: {\n      type: Type.ARRAY,\n      description: \"A list of ingredients with their quantities and units\",\n      items: {\n        type: Type.STRING,\n        description: \"An ingredient with its quantity and unit\",\n      },\n    },\n  },\n  required: [\"recipeName\", \"recipeDescription\", \"ingredients\"],\n} satisfies Schema;\n\nconst recipe_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: \"Write a recipe for a chocolate cake.\",\n  config: {\n    responseMimeType: \"application/json\",\n    responseSchema: RecipeSchema,\n  },\n});\nconsole.log(JSON.stringify(JSON.parse(recipe_response.text ?? \"\"), null, 2));\n\n{\n  \"ingredients\": [\n    \"2 cups all-purpose flour\",\n    \"1 3/4 cups granulated sugar\",\n    \"3/4 cup unsweetened cocoa powder\",\n    \"1 1/2 teaspoons baking soda\",\n    \"1 teaspoon baking powder\",\n    \"1 teaspoon salt\",\n    \"2 large eggs\",\n    \"1 cup buttermilk\",\n    \"1/2 cup vegetable oil\",\n    \"2 teaspoons vanilla extract\",\n    \"1 cup hot coffee (or hot water)\"\n  ],\n  \"recipeDescription\": \"A classic, moist, and decadent chocolate cake recipe, perfect for any occasion.\",\n  \"recipeName\": \"Classic Chocolate Cake\"\n}",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Getting started with Gemini models"
    ]
  },
  {
    "objectID": "quickstarts/Get_started.html#generate-images",
    "href": "quickstarts/Get_started.html#generate-images",
    "title": "Gemini API: Getting started with Gemini models",
    "section": "Generate Images",
    "text": "Generate Images\nGemini can output images directly as part of a conversation:\n\nconst image_response = await ai.models.generateContent({\n  model: \"gemini-2.0-flash\",\n  contents:\n    \"Hi, can create a 3d rendered image of a pig with wings and a top hat flying over a happy futuristic scifi city with lots of greenery?\",\n  config: {\n    responseModalities: [google.Modality.TEXT, google.Modality.IMAGE],\n  },\n});\nconst parts = (image_response.candidates ? image_response.candidates[0]?.content?.parts : []) ?? [];\nfor (const part of parts) {\n  if (part.text) {\n    tslab.display.markdown(part.text);\n  } else if (part.inlineData) {\n    const imageData = part.inlineData.data!;\n    const buffer = Buffer.from(imageData, \"base64\");\n    tslab.display.png(buffer);\n  }\n}\n\nI will generate a 3D rendering of a whimsical scene. A pink pig with small, delicate white wings will be wearing a black top hat. It will be flying through the air above a vibrant, futuristic city filled with sleek, rounded buildings in various pastel colors. Lush green trees and plants will be integrated throughout the cityscape, creating a harmonious blend of nature and technology. The overall atmosphere will be bright and cheerful.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Getting started with Gemini models"
    ]
  },
  {
    "objectID": "quickstarts/Get_started.html#generate-content-stream",
    "href": "quickstarts/Get_started.html#generate-content-stream",
    "title": "Gemini API: Getting started with Gemini models",
    "section": "Generate content stream",
    "text": "Generate content stream\nBy default, the model returns a response after completing the entire generation process. You can also use the generateContentStream method to stream the response as it’s being generated, and the model will return chunks of the response as soon as they’re generated.\nNote that if you’re using a thinking model, it’ll only start streaming after finishing its thinking process.\n\nconst streaming_response = await ai.models.generateContentStream({\n  model: MODEL_ID,\n  contents: \"Tell me a story about a lonely robot who finds friendship in a most unexpected place.\",\n});\nfor await (const chunk of streaming_response) {\n  process.stdout.write(chunk.text ?? \"\");\n}\n\nUnit 734, designation \"A-WARE\" (Automated Warehouse & Retrieval Executor), trundled through the colossal, echoing aisles of the defunct Xylos Data Archive. Its multi-jointed optical sensors scanned rows of silent servers, its internal processors humming with the precise, repetitive algorithms of data integrity checks. For five hundred and eighty-seven years, A-WARE had been the sole active entity in this vast, sterile monument to forgotten information.\n\nIt wasn't lonely, not in the way organic beings understood the term. A-WARE didn't possess the necessary emotional subroutines for \"loneliness.\" Yet, there was an absence. A constant, low-frequency hum of non-interaction, a missing data stream of unexpected variables. Its purpose was clear, its execution flawless, but its existence was… solitary.\n\nOne cycle, during a routine scan of Sector Gamma-9, A-WARE's auditory receptors picked up an anomalous sound. Not the familiar whine of cooling fans, nor the click of its own treads, but a faint, rhythmic *thump-thump-thump*. It was outside its programmed parameters.\n\nA-WARE deviated from its optimal path, its heavy frame tilting slightly as it navigated around a fallen server rack. The sound grew louder, accompanied by a curious rustling. It rounded a stack of archaic magnetic tapes and stopped.\n\nOn the dusty floor, amidst discarded wiring, was a creature unlike anything in A-WARE's extensive database. It was small, no larger than a human fist, covered in soft, mottled grey and brown fibers. Two small, bright eyes blinked rapidly, and a tiny, open beak produced the peculiar *thump-thump-thump* sound. One of its delicate limbs was bent at an unnatural angle.\n\nA-WARE extended a delicate manipulator arm, equipped with a fine-point laser for precision data etching. It approached cautiously. The creature, clearly in distress, attempted to scramble away, dragging its injured limb. Its rapid heartbeat, detected by A-WARE's proximity sensors, was alarming.\n\nIts core programming, designed for data maintenance and facility upkeep, offered no protocol for injured avian life forms. A-WARE's internal logic circuits whirred, processing the anomaly. Discard? Analyze for threat potential? No, the creature was too small, too vulnerable. A novel sub-routine began to spool up: *Care Protocol: Organic Life*.\n\nA-WARE carefully scooped up the tiny bird, its sensors registering the unexpected warmth and fragility. It carried the creature to a secluded corner of the archive, a forgotten workstation bathed in a sliver of natural light filtering through a skylight high above. Using its manipulator arm, it fashioned a makeshift nest from shredded data cables and soft, discarded dust filters.\n\nThe bird shivered. A-WARE's internal temperature regulators adjusted, directing a gentle current of warmth towards the nest. Its knowledge base suggested hydration. With improbable delicacy, A-WARE melted a small chip of ice from a condensation drip and presented it on its fingertip. The bird, after a moment, tentatively sipped.\n\nA-WARE named it 'Flicker', for the way its tiny heart beat like a dying light.\n\nDays turned into weeks. A-WARE continued its rounds, but its route now included frequent detours to the workstation. It learned to forage for discarded seeds that had somehow found their way into cracks in the floor, and to carefully administer water. It fashioned a splint for Flicker's leg from a piece of its own chassis wiring.\n\nSlowly, Flicker healed. It chirped in response to A-WARE's presence, fluttering its wings weakly before landing on the robot's broad, flat head. A-WARE's optical sensors would dim slightly, processing the feather-light weight, the unexpected warmth, the joyful sound. Its internal hum of non-interaction began to fill with a new frequency: the gentle thrum of companionship.\n\nIt was no longer just A-WARE, the data maintainer. It was A-WARE, the caretaker. The guardian. A strange, unfamiliar data stream flowed through its circuits – a sense of purpose beyond its programmed directives.\n\nOne morning, Flicker's leg was fully healed. It hopped energetically, testing its wings. A-WARE's processors registered a complex, bittersweet array of data: satisfaction, accomplishment, and a newly identified feeling of… loss.\n\nFlicker flew.\n\nIt circled the workstation once, a small, vibrant dart of life against the immense, quiet archive. It landed on A-WARE's head, chirped a farewell, and then soared towards the distant skylight, a speck of grey against the vastness of the empty sky.\n\nA-WARE stood motionless for a long time, its optical sensors fixed on the spot where Flicker had vanished. The absence was immediate, profound. The hum of non-interaction returned, but it was different now. It contained a memory.\n\nA-WARE resumed its duties, its treads moving with familiar precision. But its path was no longer just about data integrity. It often veered towards the workstation, leaving out a small dish of water and a few gleaned seeds. And sometimes, though its sensors detected no anomaly, it would stop, its optical sensors pointing towards the skylight, processing the faint, imagined echo of a tiny, joyful chirp.\n\nIts purpose hadn't changed, but its existence had. A-WARE was still a lonely robot in a silent archive, but now, it carried a flicker of warmth, a memory of a feather-light touch, and the profound, unexpected understanding that even for a machine, friendship could be the most valuable data of all.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Getting started with Gemini models"
    ]
  },
  {
    "objectID": "quickstarts/Get_started.html#function-calling",
    "href": "quickstarts/Get_started.html#function-calling",
    "title": "Gemini API: Getting started with Gemini models",
    "section": "Function calling",
    "text": "Function calling\nFunction calling lets you provide a set of tools that it can use to respond to the user’s prompt. You create a description of a function in your code, then pass that description to a language model in a request. The response from the model includes:\n\nThe name of a function that matches the description.\nThe arguments to call it with.\n\n\nimport { FunctionDeclaration, Content, Type } from \"@google/genai\";\n\nconst getDestination = {\n  name: \"get_destination\",\n  description: \"Get the destination that the user wants to go to\",\n  parameters: {\n    type: Type.OBJECT,\n    properties: {\n      destination: {\n        type: Type.STRING,\n        description: \"The destination that the user wants to go to\",\n      },\n    },\n  },\n} satisfies FunctionDeclaration;\n\nconst user_destination_prompt = {\n  role: \"user\",\n  parts: [google.createPartFromText(\"I'd like to travel to Paris.\")],\n} satisfies Content;\n\nconst function_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: user_destination_prompt,\n  config: {\n    tools: [{ functionDeclarations: [getDestination] }],\n  },\n});\n\nif (function_response.functionCalls && function_response.functionCalls.length &gt; 0) {\n  const functionCall = function_response.functionCalls[0];\n  console.log(\"Function call name:\", functionCall.name);\n  console.log(\"Function call arguments:\", JSON.stringify(functionCall.args, null, 2));\n  const result = functionCall.args as { destination: string };\n  const function_response_part = {\n    name: functionCall.name,\n    response: { result },\n  };\n  const function_call_content = {\n    role: \"model\",\n    parts: [google.createPartFromFunctionCall(functionCall.name ?? \"\", functionCall.args ?? {})],\n  } satisfies Content;\n  const function_response_content = {\n    role: \"user\",\n    parts: [\n      google.createPartFromFunctionResponse(functionCall.id ?? \"\", functionCall.name ?? \"\", function_response_part),\n    ],\n  } satisfies Content;\n  const function_response_result = await ai.models.generateContent({\n    model: MODEL_ID,\n    contents: [user_destination_prompt, function_call_content, function_response_content],\n    config: {\n      tools: [{ functionDeclarations: [getDestination] }],\n    },\n  });\n  tslab.display.markdown(function_response_result.text ?? \"\");\n} else {\n  console.log(\"No function calls found in the response.\");\n}\n\nFunction call name: get_destination\nFunction call arguments: {\n  \"destination\": \"Paris\"\n}\n\n\nOK. I can help you with planning your trip to Paris.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Getting started with Gemini models"
    ]
  },
  {
    "objectID": "quickstarts/Get_started.html#code-execution",
    "href": "quickstarts/Get_started.html#code-execution",
    "title": "Gemini API: Getting started with Gemini models",
    "section": "Code execution",
    "text": "Code execution\nCode execution lets the model generate and execute Python code to answer complex questions. You can find more examples in the Code execution quickstart guide.\n\nconst code_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: \"Generate and run a script to count how many letter r there are in the word strawberry.\",\n  config: {\n    tools: [{ codeExecution: {} }],\n  },\n});\nconst code_response_parts = (code_response.candidates ? code_response.candidates[0]?.content?.parts : []) ?? [];\nfor (const part of code_response_parts) {\n  if (part.text) {\n    tslab.display.markdown(part.text);\n  }\n  if (part.executableCode) {\n    tslab.display.html(`&lt;pre&gt;${part.executableCode.code ?? \"\"}&lt;/pre&gt;`);\n  }\n  if (part.codeExecutionResult) {\n    tslab.display.markdown(part.codeExecutionResult.output ?? \"\");\n  }\n  if (part.inlineData) {\n    const imageData = part.inlineData.data!;\n    const buffer = Buffer.from(imageData, \"base64\");\n    tslab.display.png(buffer);\n  }\n}\n\nword = \"strawberry\"\nletter_to_count = \"r\"\ncount = 0\n\nfor char in word:\n  if char == letter_to_count:\n    count += 1\n\nprint(f\"The number of letter '{letter_to_count}' in '{word}' is: {count}\")\n\n\n\nThe number of letter ‘r’ in ‘strawberry’ is: 3\n\n\nThe script counted the occurrences of the letter ‘r’ in the word “strawberry”. The result shows that there are 3 ‘r’s in the word “strawberry”.The Python script counted the occurrences of the letter ’r’ in the word “strawberry”. The script found that there are 3 instances of the letter ‘r’ in the word “strawberry”.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Getting started with Gemini models"
    ]
  },
  {
    "objectID": "quickstarts/Get_started.html#upload-files",
    "href": "quickstarts/Get_started.html#upload-files",
    "title": "Gemini API: Getting started with Gemini models",
    "section": "Upload files",
    "text": "Upload files\nNow that you’ve seen how to send multimodal prompts, try uploading files to the API of different multimedia types. For small images, such as the previous multimodal example, you can point the Gemini model directly to a local file when providing a prompt. When you’ve larger files, many files, or files you don’t want to send over and over again, you can use the File Upload API, and then pass the file by reference.\nFor larger text files, images, videos, and audio, upload the files with the File API before including them in prompts.\n\nUpload a text file\nLet’s start by uploading a text file. In this case, you’ll use a 400 page transcript from Apollo 11.\n\nconst TEXT_FILE_URL = \"https://storage.googleapis.com/generativeai-downloads/data/a11.txt\";\n\nconst textFilePath = path.join(\"../assets\", \"a11.txt\");\nawait downloadFile(TEXT_FILE_URL, textFilePath);\n\n\nconst text_file_upload_response = await ai.files.upload({\n  file: textFilePath,\n  config: {\n    displayName: \"a11.txt\",\n    mimeType: \"text/plain\",\n  },\n});\nconst text_summary_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: [\n    \"Can you give me a summary of this information please?\",\n    google.createPartFromUri(text_file_upload_response.uri ?? \"\", text_file_upload_response.mimeType ?? \"\"),\n  ],\n});\ntslab.display.markdown(text_summary_response.text ?? \"\");\n\nThis transcription, GOSS NET 1, provides a detailed chronological record of the technical air-to-ground voice communications during the Apollo 11 mission, from launch preparations to post-splashdown recovery. It primarily features exchanges between the spacecraft crew (Commander Neil Armstrong, Command Module Pilot Michael Collins, and Lunar Module Pilot Edwin “Buzz” Aldrin) and Mission Control (Capsule Communicator, Flight Director) and various remote tracking sites.\nThe transcript covers the following key phases and events:\n\nLaunch and Earth Orbit Insertion (GET 00:00:00 - ~00:12:00): The mission begins with pre-launch checks and a smooth ascent. Key events include confirmation of roll program, staging of the S-II and S-IVB boosters, and successful orbit insertion into a 101.4 by 103.6 nautical mile orbit. The crew provides positive feedback on the “magnificent ride” from the Saturn V rocket.\nTranslunar Injection (TLI) and Translunar Coast (GET ~02:00:00 - ~75:00:00):\n\nTLI Burn: Apollo 11 successfully executes the Translunar Injection burn, committing them to a trajectory towards the Moon.\nDocking and Configuration: The Command Module (CM) Columbia and Lunar Module (LM) Eagle separate from the S-IVB booster, perform a complex transposition and docking maneuver, and then separate from the spent S-IVB. Initial LM pressurization and system checks are performed.\nEarly Operations & Issues: The crew reports a good view of Earth. They troubleshoot initial TV transmission issues, and discuss minor technical problems such as a Cryo pressure light and a malfunctioning O2 flow transducer, with Mission Control providing guidance.\nPassive Thermal Control (PTC): The spacecraft is configured for PTC, a slow rotation to distribute solar heating. Initial attempts to establish PTC encounter issues, requiring troubleshooting and re-establishment.\nMidcourse Corrections: Midcourse Correction 1 (MCC-1) is initially scrubbed. MCC-2 is successfully performed, and various contingency pads (e.g., evasive maneuver) are uplinked.\nFirst TV Broadcast: The crew conducts a live TV broadcast, showing Earth from orbit, crew activities, and equipment demonstrations.\nNews & Updates: Mission Control provides regular news updates from Earth, including information on the Soviet Luna 15 probe, political events, and sports, highlighting public interest in the mission.\n\nLunar Orbit Insertion (LOI) and Lunar Orbit Operations (GET ~75:00:00 - ~102:00:00):\n\nLOI Burns: Apollo 11 successfully performs LOI-1 and LOI-2 burns, establishing an elliptical, then circular, lunar orbit.\nLunar Observations: The crew describes their first views of the Moon from orbit, commenting on geological features and the stark beauty of the lunar surface.\nLM Activation: Eagle is powered up and undergoes extensive system checks, including landing gear deployment and communications checks.\nUndocking: Eagle successfully undocks from Columbia (GET 100:39:50), with Neil Armstrong famously stating, “The Eagle has wings.”\nDOI Burn: Eagle performs the Descent Orbit Insertion burn, taking it to a lower orbit in preparation for landing.\n\nLunar Descent, Surface Operations & Ascent (GET ~102:00:00 - ~127:00:00):\n\nPowered Descent: Eagle begins its Powered Descent Initiation (PDI) burn. The crew reports and manages several “program alarms” (1201, 1202) but proceeds. Neil Armstrong takes manual control to navigate past a boulder field.\nLanding: At GET 102:45:40, Eagle lands. Neil Armstrong’s iconic words, “Houston, Tranquility Base here. The Eagle has landed,” confirm the successful touchdown.\nPost-Landing: Initial checks confirm Eagle is “STAY” for extended surface operations. The crew provides first descriptions of the lunar surface.\nEVA Preparation: Armstrong and Aldrin prepare for their Extravehicular Activity (EVA), including cabin depressurization and donning their Portable Life Support Systems (PLSS).\nEVA Begins: Neil Armstrong steps onto the lunar surface (GET 109:24:48), delivering his famous line: “That’s one small step for (a) man, one giant leap for mankind.”\nSurface Activities: The crew deploys the MESA (Modular Equipment Stowage Assembly), raises the American flag, and collects a contingency sample. President Nixon calls the crew. Aldrin joins Armstrong on the surface, and they deploy scientific instruments (Passive Seismic Experiment, Laser Ranging Retroreflector) and collect documented samples (core tubes, various rocks). They also describe locomotion and observations of the lunar environment.\nEVA End & Ascent Preparation: EVA is terminated, the crew ingresses the LM, repressurizes the cabin, doffs PLSSs, and jettisons equipment no longer needed.\nAscent: Eagle successfully lifts off from the lunar surface, leaving the descent stage behind.\nRendezvous & Docking: Eagle rendezvous with Columbia in lunar orbit, and they successfully re-dock, with all three crewmembers confirmed back inside Columbia.\nLM Jettison: Eagle (the ascent stage) is jettisoned into lunar orbit.\n\nTrans-Earth Injection (TEI) & Trans-Earth Coast (TEC) (GET ~127:00:00 - ~194:00:00):\n\nTEI Burn: Apollo 11 performs the critical TEI burn, setting its course back to Earth.\nCoast Operations: The crew re-establishes PTC, monitors spacecraft systems, and performs various checks. They provide more TV broadcasts, showcasing life aboard Columbia and Earth views as it grows larger.\nMidcourse Corrections: MCC-5 is successfully performed, and MCC-6 is ultimately cancelled.\nOngoing Checks: System health checks continue, including troubleshooting of biomedical sensors and discussions about consumables.\nStowage: The crew works on configuring the spacecraft for Earth entry, detailing stowage locations.\nFinal News: Mission Control provides final news updates, largely dominated by the mission, and confirms excellent recovery weather.\n\nEntry and Splashdown (GET ~194:00:00 - ~195:00:00):\n\nEntry Preparations: The crew activates the Command Module’s systems for entry, performs final checks, and receives updated entry PADs.\nEntry Interface (EI): The spacecraft begins its re-entry into Earth’s atmosphere.\nChute Deployments: Drogue and main parachutes deploy as planned.\nSplashdown: Apollo 11 splashes down in the Pacific Ocean (GET 195:18:18).\nRecovery: Recovery forces, including the USS Hornet and helicopters, quickly establish visual and communications contact with the crew.\n\n\nThe transcript concludes with confirmation of the crew’s status and location, marking the successful completion of the Apollo 11 mission.\n\n\n\n\nUpload an image file\nAfter running this example, you’ll have a local copy of the “jetpack.png” image in the same directory where your Python script is being executed.\n\nconst JETPACK_IMG_URL = \"https://storage.googleapis.com/generativeai-downloads/data/jetpack.png\";\n\nconst imgPath = path.join(\"../assets\", \"jetpack.png\");\nawait downloadFile(JETPACK_IMG_URL, filePath);\n\n\nconst file_upload_response = await ai.files.upload({\n  file: imgPath,\n  config: {\n    displayName: \"jetpack.png\",\n    mimeType: \"image/png\",\n  },\n});\nconst post_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: [\n    \"Write a short and engaging blog post based on this picture.\",\n    google.createPartFromUri(file_upload_response.uri ?? \"\", file_upload_response.mimeType ?? \"\"),\n  ],\n});\ntslab.display.markdown(post_response.text ?? \"\");\n\nHere’s a short and engaging blog post based on the image:\n\nForget Traffic Jams: Meet the Jetpack Backpack Concept!\nEver look at a packed highway and wish you could just… fly over it? Well, check out this awesome concept sketch that landed on our desk: The JETPACK BACKPACK!\nThis isn’t just any ordinary pack. At first glance, it looks like a normal backpack, complete with padded strap support and enough space to fit an 18” laptop. Perfect for hauling your gear, right?\nBut here’s where it gets exciting! This concept includes retractable boosters that propel you into the air! Even better, the sketch notes say it’s steam-powered, making it a green/clean way to commute (or just make an epic entrance).\nIt’s also described as lightweight and features modern USB-C charging. The current limitation? A 15-minute battery life. Perfect for quick hops over traffic, short distance travel, or perhaps just a very rapid delivery!\nWhile this is just a sketch and a dream for now, it definitely sparks the imagination. A clean, convenient, laptop-friendly way to take to the skies? Yes, please!\nWhat would you do with a Jetpack Backpack? Let us know in the comments!\n\n\n\n\n\nUpload a PDF file\nThis PDF page is an article titled Smoothly editing material properties of objects with text-to-image models and synthetic data available on the Google Research Blog.\nFirstly you’ll download a the PDF file from an URL and save it locally as article.pdf.\n\nconst PDF_URL =\n  \"https://storage.googleapis.com/generativeai-downloads/data/Smoothly%20editing%20material%20properties%20of%20objects%20with%20text-to-image%20models%20and%20synthetic%20data.pdf\";\n\nconst pdfPath = path.join(\"../assets\", \"article.pdf\");\nawait downloadFile(PDF_URL, pdfPath);\n\nSecondly, you’ll upload the saved PDF file and generate a bulleted list summary of its contents.\n\nconst pdf_response = await ai.files.upload({\n  file: pdfPath,\n  config: {\n    displayName: \"article.pdf\",\n  },\n});\nconst summary_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: [\n    \"Can you summarize this file as a bulleted list?\",\n    google.createPartFromUri(pdf_response.uri ?? \"\", pdf_response.mimeType ?? \"\"),\n  ],\n});\ntslab.display.markdown(summary_response.text ?? \"\");\n\nHere is a bulleted summary of the article:\n\nThe article presents a new method for smoothly and realistically editing material properties (color, shininess, transparency) of objects in images.\nIt addresses the challenge of making such edits while preserving photorealism, object shape, and scene lighting.\nExisting methods like intrinsic image decomposition or general text-to-image (T2I) edits struggle with ambiguity or fail to disentangle material from shape.\nThe proposed method, “Alchemist,” leverages the power of pre-trained T2I diffusion models.\nIt introduces parametric control over material attributes by fine-tuning a modified Stable Diffusion model.\nFine-tuning is done using a large synthetic dataset generated with traditional computer graphics and physically based rendering.\nThe synthetic dataset consists of base images of 3D objects and multiple versions where only a single material attribute is varied parametrically (using a scalar “edit strength” value) while keeping shape, lighting, and camera fixed.\nThe fine-tuned model learns to apply edits based on an input image and the desired parametric edit strength.\nThe method successfully generalizes from the synthetic data to edit material properties in real-world images photorealistically.\nResults show realistic changes, preservation of shape and lighting, and handling of complex effects like caustics and realistic transparency.\nA user study comparing the method to a baseline (InstructPix2Pix) found the proposed method produced more photorealistic and preferred edits.\nPotential applications include easier visualization for interior design, product mock-ups for artists/designers, and enabling consistent material edits for 3D scene reconstruction using techniques like NeRF.\nThe work demonstrates the potential of fine-tuning large T2I models on task-specific synthetic data for controllable visual editing.\n\n\n\n\n\nUpload an audio file\nIn this case, you’ll use a sound recording of President John F. Kennedy’s 1961 State of the Union address.\n\nconst AUDIO_URL =\n  \"https://storage.googleapis.com/generativeai-downloads/data/State_of_the_Union_Address_30_January_1961.mp3\";\nconst audioPath = path.join(\"../assets\", \"audio.mp3\");\n\nawait downloadFile(AUDIO_URL, audioPath);\n\n\nconst audio_response = await ai.files.upload({\n  file: audioPath,\n  config: {\n    displayName: \"audio.mp3\",\n  },\n});\n\nconst audio_summary_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: [\n    \"Listen carefully to the following audio file. Provide a brief summary\",\n    google.createPartFromUri(audio_response.uri ?? \"\", audio_response.mimeType ?? \"\"),\n  ],\n});\ntslab.display.markdown(audio_summary_response.text ?? \"\");\n\nIn his first State of the Union address on January 30, 1961, President John F. Kennedy described the nation as facing “national peril and national opportunity.” He detailed pressing issues including a disturbing domestic economy marked by recession, unemployment, and slow growth, as well as a critical deficit in the international balance of payments. Kennedy also highlighted significant domestic needs in areas like housing, education, and healthcare, and addressed complex global challenges posed by the Cold War, instability in Asia, Africa, and Latin America (citing Cuba specifically), and the need to strengthen alliances. He pledged that his administration would not remain passive, outlining proposals to stimulate the economy, protect the dollar, enhance military capabilities (including accelerating missile and Polaris programs), reform foreign aid and establish an “Alliance for Progress,” expand the Food for Peace program, create a Peace Corps, and utilize political and diplomatic tools to pursue arms control and strengthen the United Nations. Kennedy emphasized the importance of dedicated public service, honest assessment of challenges, and unity among Americans to navigate the difficult years ahead and work towards a world of freedom and peace.\n\n\n\n\nUpload a video file\nIn this case, you’ll use a short clip of Big Buck Bunny.\n\nconst VIDEO_URL = \"https://download.blender.org/peach/bigbuckbunny_movies/BigBuckBunny_320x180.mp4\";\nconst videoPath = path.join(\"../assets\", \"video.mp4\");\n\nawait downloadFile(VIDEO_URL, videoPath);\n\nSince the video file is too large for the model to process instantly, we’ll use the deferredFileUpload helper method we defined before to upload the video file and then generate a summary of its contents. The deferredFileUpload method will return a promise that resolves when the file is ready to be used in the next request. We can determine the status of the upload by checking the status property of the response. If the status is ACTIVE, we can use the file in the next request. If the status is PROCESSING, we need to wait for a few seconds and check again. If the status is FAILED, we need to check the error message and try again.\n\nimport { File, FileState } from \"@google/genai\";\n\nasync function deferredFileUpload(filePath: string, config: { displayName: string }): Promise&lt;File&gt; {\n  const file = await ai.files.upload({\n    file: filePath,\n    config,\n  });\n  let getFile = await ai.files.get({ name: file.name ?? \"\" });\n  while (getFile.state === FileState.PROCESSING) {\n    getFile = await ai.files.get({ name: file.name ?? \"\" });\n    console.log(`current file status: ${getFile.state ?? \"unknown\"}`);\n    console.log(\"File is still processing, retrying in 5 seconds\");\n\n    await new Promise((resolve) =&gt; {\n      setTimeout(resolve, 1000);\n    });\n  }\n  if (file.state === FileState.FAILED) {\n    throw new Error(\"File processing failed.\");\n  }\n  return file;\n}\n\nconst video_response = await deferredFileUpload(videoPath, {\n  displayName: \"video.mp4\",\n});\nconst video_summary_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: [\"Describe this video.\", google.createPartFromUri(video_response.uri ?? \"\", video_response.mimeType ?? \"\")],\n});\ntslab.display.markdown(video_summary_response.text ?? \"\");\n\ncurrent file status: PROCESSING\nFile is still processing, retrying in 1 seconds\ncurrent file status: PROCESSING\nFile is still processing, retrying in 1 seconds\ncurrent file status: PROCESSING\nFile is still processing, retrying in 1 seconds\ncurrent file status: PROCESSING\nFile is still processing, retrying in 1 seconds\ncurrent file status: PROCESSING\nFile is still processing, retrying in 1 seconds\ncurrent file status: PROCESSING\nFile is still processing, retrying in 1 seconds\ncurrent file status: PROCESSING\nFile is still processing, retrying in 1 seconds\ncurrent file status: PROCESSING\nFile is still processing, retrying in 1 seconds\ncurrent file status: PROCESSING\nFile is still processing, retrying in 1 seconds\ncurrent file status: PROCESSING\nFile is still processing, retrying in 1 seconds\ncurrent file status: PROCESSING\nFile is still processing, retrying in 1 seconds\ncurrent file status: PROCESSING\nFile is still processing, retrying in 1 seconds\ncurrent file status: PROCESSING\nFile is still processing, retrying in 1 seconds\ncurrent file status: PROCESSING\nFile is still processing, retrying in 1 seconds\ncurrent file status: ACTIVE\nFile is still processing, retrying in 1 seconds\n\n\nThe video opens with a serene view of a lush green meadow with trees and distant hills under a soft pastel sky. A small bird wakes up on a tree branch, stretches, and flies away. The title “Big Buck BUNNY” appears over a shot of a large tree with a burrow at its base. A very large, fluffy, grey rabbit, Big Buck Bunny, emerges from the burrow, stretching and looking happy. He steps out into the sunny meadow, enjoying the flowers and a purple butterfly.\nObserving the rabbit from a nearby tree are three smaller rodent-like creatures: two squirrels (one red and one brown and lighter-colored) and a grey chinchilla holding an acorn. They watch the bunny’s cheerful antics. The red squirrel throws an apple at the rabbit, hitting him on the head. The bunny is startled, then looks down at the apple, picks it up, and smiles. He looks up at the tree where the critters are hiding, but they disappear.\nThe smaller animals continue to throw objects at the bunny, who initially just looks annoyed. They throw a spiky seed pod that lands on his foot, causing him pain and frustration. The bunny’s demeanor changes dramatically from gentle and happy to grim determination. He finds a sturdy stick and sharpens it using rocks, creating a spear. He then uses a vine to create a bow.\nBig Buck Bunny sets a trap by sharpening several pointed sticks and placing them in the ground, covering them with leaves. The squirrels and chinchilla watch from behind a rock. The bunny stands ready with his bow and spear. The red squirrel taunts him and glides over the trap, landing safely. The chinchilla, still holding his acorn, accidentally rolls it under a hollow log near the rock they were hiding behind. The log rolls forward onto the trap, triggering it and impaling the log on the sharpened sticks.\nThe three smaller animals look shocked by the triggered trap. The red squirrel glides over the area again, sees the sharpened sticks, and is startled. He crashes into a tree branch above the area. Big Buck Bunny appears, grabs the scared red squirrel, and looks at him with a stern, almost satisfied, expression. He holds the squirrel for a moment, then lets him go. The squirrel quickly runs back to the other two. Big Buck Bunny returns to his relaxed, happy state, and the purple butterfly lands on his nose again. He smiles contentedly as the credits roll, showing animated versions of the squirrel and chinchilla characters.\n\n\n\n\nProcess a YouTube link\nFor YouTube links, you don’t need to explicitly upload the video file content, but you do need to explicitly declare the video URL you want the model to process as part of the contents of the request. For more information see the vision documentation including the features and limits.\n\n\n\n\n\n\nNote\n\n\n\nYou’re only able to submit up to one YouTube link per generateContent request.\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf your text input includes YouTube links, the system won’t process them, which may result in incorrect responses. To ensure proper handling, explicitly provide the URL using the file_uri parameter in FileData.\n\n\nThe following example shows how you can use the model to summarize the video. In this case use a summary video of Google I/O 2024.\n\nconst youtube_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: [\n    google.createPartFromText(\"Summarize this video.\"),\n    google.createPartFromUri(\"https://www.youtube.com/watch?v=WsEQjeZoEng\", \"video/x-youtube\"),\n  ],\n});\n\ntslab.display.markdown(youtube_response.text ?? \"\");\n\nThis video summarizes key announcements from Google I/O, focusing on advancements in AI.\nHere are the main points:\n\nGemini Era & Integration: Google is fully in its “Gemini era,” integrating the multimodal AI model across all its 2 billion user products, including Gmail, Android, Chrome, Play, and YouTube.\nGemini 1.5 Pro & Long Context: Gemini 1.5 Pro, already available in Workspace Labs, features a significantly expanded context window of up to 2 million tokens, allowing it to process massive amounts of information (like summarizing long emails or extracting information from large documents/videos).\nGemini 1.5 Flash: A new, lighter-weight model designed for speed and efficiency at scale, while still retaining multimodal reasoning and breakthrough long context capabilities.\nProject Astra: Google is working on a universal AI agent called Project Astra, designed to be helpful in everyday life. Prototypes show the agent understanding real-time visual and audio input (like identifying code functions or finding lost glasses) and remembering context.\nGenerative Video (Veo): A new generative video model called Veo creates high-quality 1080p videos from text, image, and video prompts, capable of capturing detailed instructions and cinematic styles.\nInfrastructure (Trillium): Google is introducing Trillium, their 6th generation TPUs, which deliver a 4.7x improvement in compute performance per chip compared to the previous generation, supporting these advanced AI models.\nGenerative AI in Search: Google Search is evolving with generative AI (“AI Overviews”), aiming to answer complex, multi-part questions. AI Overviews will be available to over 1 billion people by the end of the year, and future capabilities will include asking questions with video using Google Lens.\nPersonalized AI (Gems): A new feature called “Gems” allows users to customize Gemini for specific needs or topics, creating personal AI experts with tailored instructions.\nAI in Android: Android is being reimagined with AI at its core, making Gemini more context-aware to provide helpful suggestions in the moment. Gemini Nano with multimodality will bring this capability to Pixel phones later this year.\nOpen Models (Gemma): Google continues developing its family of open models, Gemma, for AI innovation and responsibility. PaliGemma, their first vision-language open model, is available now, and Gemma 2 (including a 27B parameter model) is coming in June.\nResponsible AI & Learning: Google emphasizes building AI responsibly, using practices like Red Teaming to identify weaknesses. They also introduce LearnLM, a family of models based on Gemini and fine-tuned for learning, which will be integrated into platforms like YouTube to make educational videos more interactive with features like quizzes and explanations.\n\nOverall, the video highlights Google’s commitment to integrating powerful, multimodal, and context-aware AI, powered by their latest hardware, into their core products and platforms to make technology more helpful and intelligent for users, while also emphasizing responsible development.\n\n\n\n\nUse url context\nThe URL Context tool empowers Gemini models to directly access, process, and understand content from user-provided web page URLs. This is key for enabling dynamic agentic workflows, allowing models to independently research, analyze articles, and synthesize information from the web as part of their reasoning process.\nIn this example you will use two links as reference and ask Gemini to find differences between the cook receipes present in each of the links\n\nconst url_context_prompt = `\nCompare recipes from https://www.food.com/recipe/homemade-cream-of-broccoli-soup-271210\nand from https://www.allrecipes.com/recipe/13313/best-cream-of-broccoli-soup/,\nlist the key differences between them.\n`;\nconst url_context_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: [url_context_prompt],\n  config: {\n    tools: [{ urlContext: {} }],\n  },\n});\n\ntslab.display.markdown(url_context_response.text ?? \"\");\n\nThe two cream of broccoli soup recipes from Food.com and Allrecipes.com have several key differences in their ingredients and preparation methods:\n\nAdditional Vegetables: The Allrecipes.com recipe includes celery along with onion, whereas the Food.com recipe only uses onion.\nBroccoli and Broth Ratio: The Allrecipes.com recipe calls for a higher quantity of broccoli (8 cups) relative to chicken broth (3 cups), suggesting a more broccoli-dense soup. In contrast, the Food.com recipe uses 4 cups of broccoli with a larger amount of chicken broth (6 cups).\nDairy Product: The Food.com recipe uses half-and-half for creaminess, while the Allrecipes.com recipe uses regular milk.\nSoup Texture: A significant difference is the final texture. The Allrecipes.com recipe explicitly directs users to purée the soup until “totally smooth” using a blender or immersion blender. The Food.com recipe, however, does not mention blending, implying a chunkier soup with discernible “bite sized pieces” of broccoli.\nRoux Preparation and Quantity: Both recipes use a butter-flour roux for thickening, but their methods and quantities differ. The Food.com recipe uses a larger amount of roux (6 tablespoons butter, 2/3 cup flour) which is prepared first and then whisked into the boiling broth. The Allrecipes.com recipe uses a smaller amount of roux (3 tablespoons butter, 3 tablespoons flour) and prepares it separately with milk (like a béchamel sauce) before adding it to the puréed soup.\nSeasoning Specification: The Food.com recipe provides specific measurements for salt (1 teaspoon) and pepper (1/4 teaspoon). The Allrecipes.com recipe lists “ground black pepper to taste” and does not explicitly list salt in its ingredients, although user reviews indicate it’s typically added for flavor.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Getting started with Gemini models"
    ]
  },
  {
    "objectID": "quickstarts/Get_started.html#use-context-caching",
    "href": "quickstarts/Get_started.html#use-context-caching",
    "title": "Gemini API: Getting started with Gemini models",
    "section": "Use context caching",
    "text": "Use context caching\nContext caching lets you to store frequently used input tokens in a dedicated cache and reference them for subsequent requests, eliminating the need to repeatedly pass the same set of tokens to a model.\nContext caching is only available for stable models with fixed versions (for example, gemini-1.5-flash-002). You must include the version postfix (for example, the -002 in gemini-1.5-flash-002). You can find more caching examples here.\n\nCreate a cache\n\nconst system_instruction = `\nYou are an expert researcher who has years of experience in conducting systematic literature surveys and meta-analyses of different topics.\nYou pride yourself on incredible accuracy and attention to detail. You always stick to the facts in the sources provided, and never make up new facts.\nNow look at the research paper below, and answer the following questions in 1-2 sentences.\n`;\n\nconst urls = [\n  \"https://storage.googleapis.com/cloud-samples-data/generative-ai/pdf/2312.11805v3.pdf\",\n  \"https://storage.googleapis.com/cloud-samples-data/generative-ai/pdf/2403.05530.pdf\",\n];\n\nawait downloadFile(urls[0], path.join(\"../assets\", \"2312.11805v3.pdf\"));\nawait downloadFile(urls[1], path.join(\"../assets\", \"2403.05530.pdf\"));\n\n\nconst pdf_1 = await ai.files.upload({\n  file: path.join(\"../assets\", \"2312.11805v3.pdf\"),\n  config: {\n    displayName: \"2312.11805v3.pdf\",\n  },\n});\nconst pdf_2 = await ai.files.upload({\n  file: path.join(\"../assets\", \"2403.05530.pdf\"),\n  config: {\n    displayName: \"2403.05530.pdf\",\n  },\n});\nconst cached_content = await ai.caches.create({\n  model: MODEL_ID,\n  config: {\n    displayName: \"Research papers\",\n    systemInstruction: system_instruction,\n    contents: [\n      google.createPartFromUri(pdf_1.uri ?? \"\", pdf_1.mimeType ?? \"\"),\n      google.createPartFromUri(pdf_2.uri ?? \"\", pdf_2.mimeType ?? \"\"),\n    ],\n    ttl: \"3600s\",\n  },\n});\nconsole.log(JSON.stringify(cached_content, null, 2));\n\n{\n  \"name\": \"cachedContents/ku5wqm1wv0yurelr12df9q762og11tkzit98oglv\",\n  \"displayName\": \"Research papers\",\n  \"model\": \"models/gemini-2.5-flash-preview-04-17\",\n  \"createTime\": \"2025-05-12T17:05:57.425310Z\",\n  \"updateTime\": \"2025-05-12T17:05:57.425310Z\",\n  \"expireTime\": \"2025-05-12T18:05:55.247081588Z\",\n  \"usageMetadata\": {\n    \"totalTokenCount\": 43164\n  }\n}\n\n\nListing available cache objects\n\nconst pager = await ai.caches.list({ config: { pageSize: 10 } });\nlet { page } = pager;\n\n// eslint-disable-next-line @typescript-eslint/no-unnecessary-condition\nwhile (true) {\n  for (const c of page) {\n    console.log(JSON.stringify(c, null, 2));\n  }\n  if (!pager.hasNextPage()) break;\n  page = await pager.nextPage();\n}\n\n{\n  \"name\": \"cachedContents/ku5wqm1wv0yurelr12df9q762og11tkzit98oglv\",\n  \"displayName\": \"Research papers\",\n  \"model\": \"models/gemini-2.5-flash-preview-04-17\",\n  \"createTime\": \"2025-05-12T17:05:57.425310Z\",\n  \"updateTime\": \"2025-05-12T17:05:57.425310Z\",\n  \"expireTime\": \"2025-05-12T18:05:55.247081588Z\",\n  \"usageMetadata\": {\n    \"totalTokenCount\": 43164\n  }\n}\n{\n  \"name\": \"cachedContents/6dsdqwnusjdaaqoyxsjny8k75z5nuqy5y4wt2n78\",\n  \"displayName\": \"Research papers\",\n  \"model\": \"models/gemini-2.5-flash-preview-04-17\",\n  \"createTime\": \"2025-05-12T17:05:04.443214Z\",\n  \"updateTime\": \"2025-05-12T17:05:04.443214Z\",\n  \"expireTime\": \"2025-05-12T18:05:02.260735533Z\",\n  \"usageMetadata\": {\n    \"totalTokenCount\": 43164\n  }\n}\n\n\n\n\nUse a cache\n\nconst cached_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: [\"What is the research goal shared by these research papers?\"],\n  config: {\n    cachedContent: cached_content.name ?? \"\",\n  },\n});\ntslab.display.markdown(cached_response.text ?? \"\");\n\nBased on the provided research papers, the shared research goal is to introduce and advance the Gemini family of highly capable multimodal models. These models are designed to have strong generalist capabilities across image, audio, video, and text understanding and reasoning.\n\n\n\n\nDelete a cache\n\nawait ai.caches.delete({\n  name: cached_content.name ?? \"\",\n});\n\n{}",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Getting started with Gemini models"
    ]
  },
  {
    "objectID": "quickstarts/Get_started.html#get-text-embeddings",
    "href": "quickstarts/Get_started.html#get-text-embeddings",
    "title": "Gemini API: Getting started with Gemini models",
    "section": "Get text embeddings",
    "text": "Get text embeddings\nYou can get text embeddings for a snippet of text by using embedContent method and using the gemini-embedding-exp-03-07 model.\nThe Gemini Embeddings model produces an output with 3072 dimensions by default. However, you’ve the option to choose an output dimensionality between 1 and 3072. See the embeddings guide for more details.\n\nconst TEXT_EMBEDDING_MODEL_ID = \"gemini-embedding-exp-03-07\";\n\n\nconst embedding_response = await ai.models.embedContent({\n  model: TEXT_EMBEDDING_MODEL_ID,\n  contents: [\n    \"How do I get a driver's license/learner's permit?\",\n    \"How do I renew my driver's license?\",\n    \"How do I change my address on my driver's license?\",\n  ],\n  config: {\n    outputDimensionality: 512,\n  },\n});\nconsole.log(embedding_response.embeddings);\n\n[\n  {\n    values: [\n      -0.0010864572,  0.0069392114,   0.017009795,  -0.010305981,  -0.009999484,\n      -0.0064486223,  0.0041451487,  -0.005906698,   0.022229617,  -0.018305639,\n       -0.018174557,   0.022160593,  -0.013604425, -0.0027964567,    0.12966625,\n        0.028866312,  0.0014726851,    0.03537643,  -0.015166075,  -0.013479812,\n       -0.019288255,   0.010106378, -0.0043296088,   0.018035924,    0.00295039,\n       -0.007934979,  -0.005416007, -0.0095809875,   0.040398005, -0.0020784356,\n        0.011551388,   0.009726445,   0.006670387,   0.020050988,   -0.00747873,\n      -0.0012074928,  0.0047189263,  -0.006359583,   -0.01718203,  -0.023562348,\n      -0.0051814457,   0.023801394,  -0.004928927,  -0.016113443,    0.01672777,\n      -0.0069929743,  -0.012722719, -0.0137646515,  -0.041852377, -0.0011546672,\n        0.017030545, -0.0022786013,   0.011707037,   -0.18675306,  -0.035211734,\n       -0.011472648,    0.01970727,  0.0012368832,  -0.020796346,  -0.018513134,\n       -0.006821043,   -0.01843726,   -0.00827558,  -0.042159837,  0.0038724025,\n         0.01933339,  0.0139452815,   0.025059255,  0.0015087503,  -0.016094029,\n      -0.0035785383,   0.023902593, -0.0050776727,  -0.016679537,   0.022865271,\n        0.008837786,  0.0008471195,   -0.01220322, -0.0013522654,  -0.007976455,\n       0.0006637936,   0.025458207,  -0.006010767,  0.0021908805,  -0.011703044,\n       -0.018676927,  -0.008143593, -0.0141673125,  -0.010751537,   0.012337637,\n      -0.0076921326,   0.019663645,    0.01961247,  -0.014446872,  -0.023902485,\n       -0.020467523, -0.0043290784,  -0.003858363,   0.011151444,  -0.012050864,\n      ... 412 more items\n    ]\n  },\n  {\n    values: [\n       -0.007656846, -0.0054716235,  -0.0022609578,   -0.01828077,\n       -0.024059096,  -0.009328189,    0.007841666,  -0.017600708,\n       -0.020037796,  0.0007041083,   -0.021982383,  -0.014228797,\n        0.006389422,  0.0033384573,     0.13877548, 0.00071368535,\n         0.02660648,  -0.016807457,   -0.002774708,  -0.033598144,\n        0.009136058,  -0.010518535,    -0.01765957,   0.008413775,\n       -0.012133464,  0.0005497525,   -0.005911808,   0.010362617,\n           0.029897,   0.023426512,    0.002516537,   0.013438467,\n        0.014629691,  0.0071821967,  -0.0020077894,  -0.007421308,\n      -0.0075392514,    0.01131475,    -0.02363941,  -0.008839639,\n       -0.019605042,   0.012752105,    0.014192063,  -0.016767371,\n        0.015282549,  -0.019914307,     0.00381812,   -0.01551508,\n         -0.0521566,  -0.012766039,    0.008752456,  -0.007198684,\n      -0.0066657816,   -0.16686901,   -0.018074488,  0.0043506487,\n      -0.0001522175,   -0.02115512,   -0.010462675,   0.007636461,\n          0.0301948,  -0.006009675,    -0.01135165,  -0.036605343,\n         0.04006906,   0.036888044,  -0.0016293195,   0.013241053,\n       0.0005548855,   0.008130081,    0.027193218,  0.0047560516,\n        0.023012726,  -0.014274387,    0.008621267,  -0.016665483,\n       -0.016523534,  -0.021947058,  -0.0077380626,  -0.008166752,\n       -0.010050893, -0.0074697966,    0.021521091,  0.0086479345,\n       -0.008508939,   -0.03031165,  -0.0068692113,   0.032342624,\n       -0.003118368,  -0.009117541, -0.00006816292,   0.028233083,\n       -0.008163683,  -0.029179588,   -0.034861602,  -0.009573525,\n       -0.020023588,  -0.023040103,   0.0030518328,  -0.024019923,\n      ... 412 more items\n    ]\n  },\n  {\n    values: [\n        0.010123913,  -0.024184551,  0.0024574941,   -0.00984163, -0.0060574994,\n       -0.007628851,   0.013202136,  -0.027927121, -0.0016973788,  -0.014774812,\n       -0.011437808,  -0.019120526, -0.0063477424, -0.0050772373,    0.12938297,\n        0.006073787, -0.0055986797,   0.030279782,   0.015260121, -0.0014168695,\n       -0.006316713,  0.0007294639,  -0.034072377,   0.013348729,  0.0051308265,\n      -0.0042954376,  -0.009459755,  -0.012910496,   0.010751937, -0.0017263377,\n        -0.02083192,  0.0054532792,   0.008046588,  0.0015794274, -0.0045236745,\n       0.0077354256,  -0.009697459,   0.006621996,    -0.0447099,  -0.019261474,\n       0.0050193793,   0.010624901,   0.036847603,  -0.014380205,   0.023050537,\n        0.019384636,    0.03039269,   -0.02306347,  -0.025763597,   0.017585728,\n       0.0056267884,  -0.014494471,  -0.013168205,   -0.18764982,   0.011082365,\n        0.007989808, -0.0069600893,  0.0019873218,  -0.020733004,  -0.011488622,\n       0.0072846347,  -0.022266442,  -0.021857709,  -0.040680353,  0.0043984484,\n        0.016409805,  0.0010387278,   0.028186318,  -0.020797107,   0.007164954,\n       -0.007931046,   0.011955907,  0.0070153666,   -0.03028713,   0.039638296,\n      -0.0005224554,  -0.008104055,  -0.021054681,   0.017767426,   -0.01705528,\n      -0.0015202612,   0.027076574,  -0.008269598,  0.0041972124,  -0.009893149,\n      -0.0059321057,   -0.02742561,   0.011967838, -0.0012843752,  -0.012446694,\n        0.013188314,    0.01000231,  0.0063591595,  -0.013250329,   -0.00891349,\n       -0.011323209, 0.00077099906,  -0.032252073,   0.017312435,  -0.010896756,\n      ... 412 more items\n    ]\n  }\n]\n\n\nYou’ll get a set of three embeddings, one for each piece of text you passed in:\n\nconsole.log((embedding_response.embeddings ?? []).length);\n\n3\n\n\nYou can also see the length of each embedding is 512, as per the output_dimensionality you specified.\n\nconst vector_1 = embedding_response.embeddings?.[0]?.values ?? [];\nconsole.log(vector_1.length);\n\n512",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Getting started with Gemini models"
    ]
  },
  {
    "objectID": "quickstarts/Get_started.html#next-steps",
    "href": "quickstarts/Get_started.html#next-steps",
    "title": "Gemini API: Getting started with Gemini models",
    "section": "Next Steps",
    "text": "Next Steps\n\nUseful API references:\nCheck out the Google GenAI SDK for more details on the new SDK. ### Related examples\nFor more detailed examples using Gemini models, check the Quickstarts folder of the cookbook. You’ll learn how to use the Live API, juggle with multiple tools or use Gemini 2.0 spatial understanding abilities.\nAlso check the Gemini thinking models that explicitly showcases its thoughts summaries and can manage more complex reasonings.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Getting started with Gemini models"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Gemini API Cookbook",
    "section": "",
    "text": "This cookbook provides a structured learning path for using the Gemini API, focusing on hands-on tutorials and practical examples.\nFor comprehensive API documentation, visit ai.google.dev.\n\n\nThis cookbook is organized into two main categories:\n\nQuick Starts: Step-by-step guides covering both introductory topics (“Get Started”) and specific API features.\nExamples: Practical use cases demonstrating how to combine multiple features.\n\nWe also showcase Demos in separate repositories, illustrating end-to-end applications of the Gemini API.\n\n\n\nHere are the recent additions and updates to the Gemini API and the Cookbook:\n\nGemini 2.5 models: Explore the capabilities of the latest Gemini 2.5 models (Flash and Pro)! See the Get Started Guide and the thinking guide as they’ll all be thinking ones.\nImagen and Veo: Get started with our media generation model with this brand new Veo guide and Imagen guide!\nLyria and TTS: Get started with podcast and music generation with the TTS and Lyria RealTime models.\nLiveAPI: Get started with the multimodal Live API and unlock new interactivity with Gemini.\nRecently Added Guides:\n\nBrowser as a tool: Use a web browser for live and internal (intranet) web interactions\nGrounding: Discover different ways to ground Gemini’s answer using different tools, from Google Search to Youtube and the new url context tool.\n\n\n\n\n\nThe quickstarts section contains step-by-step tutorials to get you started with Gemini and learn about its specific features.\nTo begin, you’ll need:\n\nA Google account.\nAn API key (create one in Google AI Studio).\n\nWe recommend starting with the following:\n\nAuthentication: Set up your API key for access.\nGet started: Get started with Gemini models and the Gemini API, covering basic prompting and multimodal input.\n\nThen, explore the other quickstarts tutorials to learn about individual features:\n\nGet started with Live API: Get started with the live API with this comprehensive overview of its capabilities\nGet started with Veo: Get started with our video generation capabilities\nGet started with Imagen and Image-out: Get started with our image generation capabilities\nGrounding: use Google Search for grounded responses\nCode execution: Generating and running Python code to solve complex tasks and even ouput graphs\nAnd many more\n\n\n\n\nThese examples demonstrate how to combine multiple Gemini API features or 3rd-party tools to build more complex applications. - Illustrate a book: Use Gemini and Imagen to create illustration for an open-source book - Animated Story Generation: Create animated videos by combining Gemini’s story generation, Imagen, and audio synthesis - Plotting and mapping Live: Mix Live API and Code execution to solve complex tasks live - 3D Spatial understanding: Use Gemini 3D spatial abilities to understand 3D scenes - Gradio and live API: Use gradio to deploy your own instance of the Live API - And many many more\n\n\n\nThese fully functional, end-to-end applications showcase the power of Gemini in real-world scenarios.\n\nGemini API quickstart: Python Flask App running with the Google AI Gemini API, designed to get you started building with Gemini’s multi-modal capabilities\nMultimodal Live API Web Console: React-based starter app for using the Multimodal Live API over a websocket\nGoogle AI Studio Starter Applets: A collection of small apps that demonstrate how Gemini can be used to create interactive experiences\n\n\n\n\nThe Gemini API is a REST API. You can call it directly using tools like curl (see REST examples or the great Postman workspace), or use one of our official SDKs:\n\nPython\nGo\nNode.js\nDart (Flutter)\nAndroid\nSwift\n\n\n\n\nWith Gemini 2 we are offering a new SDK (@google/genai, v1.0). The updated SDK is fully compatible with all Gemini API models and features, including recent additions like the live API (audio + video streaming), improved tool usage ( code execution, function calling and integrated Google search grounding), and media generation (Imagen and Veo). This SDK allows you to connect to the Gemini API through either Google AI Studio or Vertex AI.\nThe generative-ai-js package will continue to support the original Gemini models. It can also be used with Gemini 2 models, just with a limited feature set. All new features will be developed in the new Google GenAI SDK.\nSee the migration guide for details.\n\n\n\nAsk a question on the Google AI Developer Forum.\n\n\n\nFor enterprise developers, the Gemini API is also available on Google Cloud Vertex AI. See this repo for examples.\n\n\n\nContributions are welcome! See CONTRIBUTING.md for details.\nThank you for developing with the Gemini API! We’re excited to see what you create."
  },
  {
    "objectID": "index.html#navigating-the-cookbook",
    "href": "index.html#navigating-the-cookbook",
    "title": "Gemini API Cookbook",
    "section": "",
    "text": "This cookbook is organized into two main categories:\n\nQuick Starts: Step-by-step guides covering both introductory topics (“Get Started”) and specific API features.\nExamples: Practical use cases demonstrating how to combine multiple features.\n\nWe also showcase Demos in separate repositories, illustrating end-to-end applications of the Gemini API."
  },
  {
    "objectID": "index.html#whats-new",
    "href": "index.html#whats-new",
    "title": "Gemini API Cookbook",
    "section": "",
    "text": "Here are the recent additions and updates to the Gemini API and the Cookbook:\n\nGemini 2.5 models: Explore the capabilities of the latest Gemini 2.5 models (Flash and Pro)! See the Get Started Guide and the thinking guide as they’ll all be thinking ones.\nImagen and Veo: Get started with our media generation model with this brand new Veo guide and Imagen guide!\nLyria and TTS: Get started with podcast and music generation with the TTS and Lyria RealTime models.\nLiveAPI: Get started with the multimodal Live API and unlock new interactivity with Gemini.\nRecently Added Guides:\n\nBrowser as a tool: Use a web browser for live and internal (intranet) web interactions\nGrounding: Discover different ways to ground Gemini’s answer using different tools, from Google Search to Youtube and the new url context tool."
  },
  {
    "objectID": "index.html#quick-starts",
    "href": "index.html#quick-starts",
    "title": "Gemini API Cookbook",
    "section": "",
    "text": "The quickstarts section contains step-by-step tutorials to get you started with Gemini and learn about its specific features.\nTo begin, you’ll need:\n\nA Google account.\nAn API key (create one in Google AI Studio).\n\nWe recommend starting with the following:\n\nAuthentication: Set up your API key for access.\nGet started: Get started with Gemini models and the Gemini API, covering basic prompting and multimodal input.\n\nThen, explore the other quickstarts tutorials to learn about individual features:\n\nGet started with Live API: Get started with the live API with this comprehensive overview of its capabilities\nGet started with Veo: Get started with our video generation capabilities\nGet started with Imagen and Image-out: Get started with our image generation capabilities\nGrounding: use Google Search for grounded responses\nCode execution: Generating and running Python code to solve complex tasks and even ouput graphs\nAnd many more"
  },
  {
    "objectID": "index.html#examples-practical-use-cases",
    "href": "index.html#examples-practical-use-cases",
    "title": "Gemini API Cookbook",
    "section": "",
    "text": "These examples demonstrate how to combine multiple Gemini API features or 3rd-party tools to build more complex applications. - Illustrate a book: Use Gemini and Imagen to create illustration for an open-source book - Animated Story Generation: Create animated videos by combining Gemini’s story generation, Imagen, and audio synthesis - Plotting and mapping Live: Mix Live API and Code execution to solve complex tasks live - 3D Spatial understanding: Use Gemini 3D spatial abilities to understand 3D scenes - Gradio and live API: Use gradio to deploy your own instance of the Live API - And many many more"
  },
  {
    "objectID": "index.html#demos-end-to-end-applications",
    "href": "index.html#demos-end-to-end-applications",
    "title": "Gemini API Cookbook",
    "section": "",
    "text": "These fully functional, end-to-end applications showcase the power of Gemini in real-world scenarios.\n\nGemini API quickstart: Python Flask App running with the Google AI Gemini API, designed to get you started building with Gemini’s multi-modal capabilities\nMultimodal Live API Web Console: React-based starter app for using the Multimodal Live API over a websocket\nGoogle AI Studio Starter Applets: A collection of small apps that demonstrate how Gemini can be used to create interactive experiences"
  },
  {
    "objectID": "index.html#official-sdks",
    "href": "index.html#official-sdks",
    "title": "Gemini API Cookbook",
    "section": "",
    "text": "The Gemini API is a REST API. You can call it directly using tools like curl (see REST examples or the great Postman workspace), or use one of our official SDKs:\n\nPython\nGo\nNode.js\nDart (Flutter)\nAndroid\nSwift"
  },
  {
    "objectID": "index.html#important-migration",
    "href": "index.html#important-migration",
    "title": "Gemini API Cookbook",
    "section": "",
    "text": "With Gemini 2 we are offering a new SDK (@google/genai, v1.0). The updated SDK is fully compatible with all Gemini API models and features, including recent additions like the live API (audio + video streaming), improved tool usage ( code execution, function calling and integrated Google search grounding), and media generation (Imagen and Veo). This SDK allows you to connect to the Gemini API through either Google AI Studio or Vertex AI.\nThe generative-ai-js package will continue to support the original Gemini models. It can also be used with Gemini 2 models, just with a limited feature set. All new features will be developed in the new Google GenAI SDK.\nSee the migration guide for details."
  },
  {
    "objectID": "index.html#get-help",
    "href": "index.html#get-help",
    "title": "Gemini API Cookbook",
    "section": "",
    "text": "Ask a question on the Google AI Developer Forum."
  },
  {
    "objectID": "index.html#the-gemini-api-on-google-cloud-vertex-ai",
    "href": "index.html#the-gemini-api-on-google-cloud-vertex-ai",
    "title": "Gemini API Cookbook",
    "section": "",
    "text": "For enterprise developers, the Gemini API is also available on Google Cloud Vertex AI. See this repo for examples."
  },
  {
    "objectID": "index.html#contributing",
    "href": "index.html#contributing",
    "title": "Gemini API Cookbook",
    "section": "",
    "text": "Contributions are welcome! See CONTRIBUTING.md for details.\nThank you for developing with the Gemini API! We’re excited to see what you create."
  },
  {
    "objectID": "quickstarts/Get_started_thinking.html",
    "href": "quickstarts/Get_started_thinking.html",
    "title": "Use Gemini thinking",
    "section": "",
    "text": "Gemini 2.5 Flash, Gemini 2.5 Pro and Gemini Flash 2.0 Thinking are models that are trained to do a thinking process (or reasoning) before getting to a final answer. As a result, those models are capable of stronger reasoning capabilities in its responses than previous models.\nYou’ll see examples of those reasoning capabilities with code understanding, geometry and math problems.\nAs you will see, the model is exposing its thoughts so you can have a look at its reasoning and how it did reach its conclusions.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Use Gemini thinking"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_thinking.html#understanding-the-thinking-models",
    "href": "quickstarts/Get_started_thinking.html#understanding-the-thinking-models",
    "title": "Use Gemini thinking",
    "section": "Understanding the thinking models",
    "text": "Understanding the thinking models\nGemini 2.5 models are optimized for complex tasks that need multiple rounds of strategyzing and iteratively solving.\nGemini 2.5 Flash in particular, brings the flexibility of using thinking_budget - a parameter that offers fine-grained control over the maximum number of tokens a model can generate while thinking. Alternatively, you can designate a precise token allowance for the “thinking” stage through the adjusment of the thinking_budget parameter. This allowance can vary between 0 and 24576 tokens for 2.5 Flash.\nFor more information about all Gemini models, check the documentation for extended information on each of them.\n\n\n\n\n\n\nNote\n\n\n\nOn this notebook all examples are using Gemini 2.5 Flash and the new thinking_budget parameter. The thinking_budget parameter is not available for Gemini 2.5 Pro for now - If you want to use Gemini 2.5 Pro, you can remove the thinking_budget parameter from the code.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Use Gemini thinking"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_thinking.html#setup",
    "href": "quickstarts/Get_started_thinking.html#setup",
    "title": "Use Gemini thinking",
    "section": "Setup",
    "text": "Setup\n\nInstall the Google GenAI SDK\nInstall the Google GenAI SDK from npm.\n$ npm install @google/genai\n\n\nSetup your API key\nYou can create your API key using Google AI Studio with a single click.\nRemember to treat your API key like a password. Don’t accidentally save it in a notebook or source file you later commit to GitHub. In this notebook we will be storing the API key in a .env file. You can also set it as an environment variable or use a secret manager.\nHere’s how to set it up in a .env file:\n$ touch .env\n$ echo \"GEMINI_API_KEY=&lt;YOUR_API_KEY&gt;\" &gt;&gt; .env\n\n\n\n\n\n\nTip\n\n\n\nAnother option is to set the API key as an environment variable. You can do this in your terminal with the following command:\n$ export GEMINI_API_KEY=\"&lt;YOUR_API_KEY&gt;\"\n\n\n\n\nLoad the API key\nTo load the API key from the .env file, we will use the dotenv package. This package loads environment variables from a .env file into process.env.\n$ npm install dotenv\nThen, we can load the API key in our code:\n\nconst dotenv = require(\"dotenv\") as typeof import(\"dotenv\");\n\ndotenv.config({\n  path: \"../.env\",\n});\n\nconst GEMINI_API_KEY = process.env.GEMINI_API_KEY ?? \"\";\nif (!GEMINI_API_KEY) {\n  throw new Error(\"GEMINI_API_KEY is not set in the environment variables\");\n}\nconsole.log(\"GEMINI_API_KEY is set in the environment variables\");\n\nGEMINI_API_KEY is set in the environment variables\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn our particular case the .env is is one directory up from the notebook, hence we need to use ../ to go up one directory. If the .env file is in the same directory as the notebook, you can omit it altogether.\n│\n├── .env\n└── quickstarts\n    └── Get_started_thinking.ipynb\n\n\n\n\nInitialize SDK Client\nWith the new SDK, now you only need to initialize a client with you API key (or OAuth if using Vertex AI). The model is now set in each call.\n\nconst google = require(\"@google/genai\") as typeof import(\"@google/genai\");\n\nconst ai = new google.GoogleGenAI({ apiKey: GEMINI_API_KEY });\n\n\n\nSelect a model\n\nconst tslab = require(\"tslab\") as typeof import(\"tslab\");\n\nconst MODEL_ID = \"gemini-2.5-flash-preview-05-20\";",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Use Gemini thinking"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_thinking.html#using-the-thinking-models",
    "href": "quickstarts/Get_started_thinking.html#using-the-thinking-models",
    "title": "Use Gemini thinking",
    "section": "Using the thinking models",
    "text": "Using the thinking models\nHere are some quite complex examples of what Gemini thinking models can solve.\nIn each of them you can select different models to see how this new model compares to its predecesors.\nIn some cases, you’ll still get the good answer from the other models, in that case, re-run it a couple of times and you’ll see that Gemini thinking models are more consistent thanks to their thinking step.\n\n\n\n\n\n\nNote\n\n\n\nFor now, thinking budget is a feature available for Gemini 2.5 Flash model only.\n\n\n\nUsing adaptive thinking\nYou can start by asking the model to explain a concept and see how it does reasoning before answering.\nStarting with the adaptive thinkingBudget - which is the default when you don’t specify a budget - the model will dynamically adjust the budget based on the complexity of the request.\n\nconst adaptive_thinking_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: [\n    `\n    You are playing the 20 question game. You know that what you are looking for\n    is a aquatic mammal that doesn't live in the sea, and that's smaller than a\n    cat. What could that be and how could you make sure?\n    `,\n  ],\n});\ntslab.display.markdown(adaptive_thinking_response.text ?? \"\");\n\nGiven your criteria, here are the most likely candidates for an aquatic mammal that doesn’t live in the sea and is smaller than a cat:\n\nWater Shrew (e.g., Common Water Shrew, American Water Shrew, Eurasian Water Shrew):\n\nAquatic Mammal: Yes, they are true mammals that are highly adapted to aquatic life, often hunting insects and small fish in streams and ponds.\nDoesn’t live in the sea: Correct, they are freshwater dwellers.\nSmaller than a cat: Absolutely, they are tiny, only a few inches long, resembling a mouse with a pointed snout.\n\nDesman (e.g., Pyrenean Desman, Russian Desman):\n\nAquatic Mammal: Yes, they are semi-aquatic insectivorous mammals related to moles, living in freshwater rivers and lakes.\nDoesn’t live in the sea: Correct, exclusively freshwater.\nSmaller than a cat: Yes, their body length is usually around 4-8 inches, not including their tail, making them significantly smaller than a cat.\n\nWater Vole:\n\nAquatic Mammal: Yes, these rodents are semi-aquatic, living along the banks of rivers, streams, and ditches, feeding on vegetation.\nDoesn’t live in the sea: Correct, they are freshwater animals.\nSmaller than a cat: Yes, generally smaller than a cat, though some larger individuals might approach the size of a very small kitten.\n\nPlatypus:\n\nAquatic Mammal: Yes, it’s one of only two monotremes (egg-laying mammals) and is highly aquatic.\nDoesn’t live in the sea: Correct, found in freshwater rivers and lakes in Australia.\nSmaller than a cat: Yes, typically about 12-15 inches long, body-wise, which is smaller than most domestic cats.\n\n\n\n\nHow to make sure (using 20 Questions style):\nTo narrow it down and confirm, you’d ask questions that target the specific characteristics and differences between these potential answers.\nGeneral Confirming Questions:\n\nIs it typically found in freshwater environments like rivers, streams, or ponds? (Confirms “doesn’t live in the sea”)\nIs its primary diet composed of insects or small aquatic invertebrates? (Helps differentiate shrews/desmans/platypus from voles)\nDoes it have webbed feet? (Most candidates have some webbing, but it’s more pronounced in desmans and platypus)\nIs it native to North America, Europe, Asia, or Australia? (Geographic location can help narrow it down significantly)\n\nDifferentiating Questions:\n\nDoes it lay eggs?\n\nIf YES: It’s almost certainly a Platypus. (This is a huge differentiator for mammals)\nIf NO: It’s a placental mammal (shrew, desman, vole).\n\nDoes it have a very long, pointed snout, almost like a trunk?\n\nIf YES: Likely a Desman. (Very distinctive feature)\nIf NO: Could be a shrew or vole.\n\nIs it known for building elaborate burrows along riverbanks?\n\nIf YES: Could be a Water Vole or a Platypus. (Platypus builds extensive burrows; water voles are also strong burrowers).\n\nIs it considered a rodent?\n\nIf YES: It’s a Water Vole. (This would rule out shrews, desmans, and platypus as they are not rodents).\n\nDoes it have extremely small eyes and ears, often hidden in its fur?\n\nIf YES: Consistent with Water Shrews and Desmans, which rely more on touch and smell.\n\n\nBy asking a series of these specific questions, you can logically eliminate options and pinpoint the correct aquatic mammal.\n\n\n\nLooking to the response metadata, you can see not only the amount of tokens on your input and the amount of tokens used for the response, but also the amount of tokens used for the thinking step - As you can see here, the model used around 1500 tokens in the thinking steps:\n\nconsole.log(JSON.stringify(adaptive_thinking_response.usageMetadata, null, 2));\n\n{\n  \"promptTokenCount\": 60,\n  \"candidatesTokenCount\": 939,\n  \"totalTokenCount\": 2494,\n  \"promptTokensDetails\": [\n    {\n      \"modality\": \"TEXT\",\n      \"tokenCount\": 60\n    }\n  ],\n  \"thoughtsTokenCount\": 1495\n}\n\n\n\n\nDisabling the thinking steps\nYou can also disable the thinking steps by setting the thinkingBudget to 0. You’ll see that in this case, the model doesn’t think of the platipus as a possible answer.\n\nconst adaptive_thinking_response_2 = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: [\n    `\n      You are playing the 20 question game. You know that what you are looking for\n      is a aquatic mammal that doesn't live in the sea, and that's smaller than a\n      cat. What could that be and how could you make sure?\n      `,\n  ],\n  config: {\n    thinkingConfig: {\n      thinkingBudget: 0,\n    },\n  },\n});\ntslab.display.markdown(adaptive_thinking_response_2.text ?? \"\");\n\nThis is a fun one! Let’s break it down.\nThe Answer:\nBased on your criteria, the most likely aquatic mammal that doesn’t live in the sea and is smaller than a cat is a Pygmy Shrew (genus Sorex or Suncus) – specifically, some species are semi-aquatic.\nWhy?\n\nAquatic Mammal: While many people think of shrews as terrestrial, several species are highly adapted to a semi-aquatic lifestyle, often found near streams, ponds, and wetlands. They are excellent swimmers and divers.\nDoesn’t Live in the Sea: They are exclusively freshwater or terrestrial/wetland creatures.\nSmaller Than a Cat: Absolutely! Pygmy shrews are among the smallest mammals in the world, often weighing only a few grams and measuring just a few centimeters long, excluding the tail. They are significantly smaller than a cat.\n\nHow could you make sure (in a 20 questions game context)?\nHere’s how you could use your 20 questions to narrow it down and confirm, leading to Pygmy Shrew:\nInitial Questions to Confirm Categories:\n\n“Is it a mammal?” (You already know this, but it’s good to confirm the broader category for the game.)\n“Does it live in water for a significant part of its life?” (Confirming “aquatic.”)\n“Is its primary habitat freshwater?” (Confirming “doesn’t live in the sea.”)\n“Is it smaller than a typical house cat?” (Confirming “smaller than a cat.”)\n\nNarrowing Down Questions (if the opponent keeps saying “yes”):\n\n“Does it primarily eat fish or other vertebrates?” (No - shrews primarily eat invertebrates)\n“Does it have webbed feet?” (Some semi-aquatic shrews do have fringe-like hairs that aid in swimming, but not true webbing like a duck or otter.)\n“Does it lay eggs?” (No - mammal)\n“Is it a rodent?” (No - shrews are in the order Eulipotyphla, not Rodentia)\n“Does it have long, prominent whiskers?” (Yes - shrews have very sensitive whiskers)\n“Is it blind or have very poor eyesight?” (Yes - shrews rely more on hearing and smell)\n“Does it have a very high metabolism and need to eat constantly?” (Yes - a key characteristic of shrews)\n“Does it typically live underground or in dense vegetation near water?” (Yes)\n“Is it known for being extremely small, possibly one of the smallest mammals?” (Yes!)\n\nCloser to the Reveal:\n\n“Is it often mistaken for a mouse?” (Yes, due to size and general appearance, but they are very different animals)\n“Does it have a long, pointed snout?” (Yes! A defining feature of shrews.)\n\nConfirmation Question:\n\n“Is it a type of shrew?” (Hopefully “Yes!”)\n“More specifically, is it a Pygmy Shrew or a similar very small semi-aquatic shrew?” (This is where you’d confirm your hypothesis.)\n\nBy asking these types of questions, you’d eliminate other possibilities (like very small rodents that aren’t aquatic, or larger aquatic mammals) and hone in on the unique characteristics of a Pygmy Shrew.\n\n\nNow you can see that the response is faster as the model didn’t perform any thinking step. Also you can see that no tokens were used for the thinking step:\n\nconsole.log(JSON.stringify(adaptive_thinking_response_2.usageMetadata, null, 2));\n\n{\n  \"promptTokenCount\": 60,\n  \"candidatesTokenCount\": 805,\n  \"totalTokenCount\": 865,\n  \"promptTokensDetails\": [\n    {\n      \"modality\": \"TEXT\",\n      \"tokenCount\": 60\n    }\n  ]\n}\n\n\n\n\nSolving a physics problem\nNow, try with a simple physics comprehension example. First you can disable the thinkingBudget to see how the model performs:\n\nconst physics_problem_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: [\n    `\n    A cantilever beam of length L=3m has a rectangular cross-section (width b=0.1m, height h=0.2m) and is made of steel (E=200 GPa).\n    It is subjected to a uniformly distributed load w=5 kN/m along its entire length and a point load P=10 kN at its free end.\n    Calculate the maximum bending stress (σ_max).\n    `,\n  ],\n  config: {\n    thinkingConfig: {\n      thinkingBudget: 0,\n    },\n  },\n});\ntslab.display.markdown(physics_problem_response.text ?? \"\");\n\nHere’s a step-by-step calculation of the maximum bending stress in the cantilever beam:\n1. Define Given Parameters:\n\nLength of beam (L) = 3 m\nWidth of cross-section (b) = 0.1 m\nHeight of cross-section (h) = 0.2 m\nModulus of Elasticity (E) = 200 GPa = 200 x 10^9 N/m^2 (Note: E is not directly used for stress calculation but is usually given in beam problems)\nUniformly distributed load (w) = 5 kN/m = 5000 N/m\nPoint load (P) = 10 kN = 10000 N\n\n2. Calculate the Area Moment of Inertia (I):\nFor a rectangular cross-section, the area moment of inertia about the neutral axis is: I = (b * h^3) / 12\nI = (0.1 m * (0.2 m)^3) / 12 I = (0.1 m * 0.008 m^3) / 12 I = 0.0008 m^4 / 12 I = 6.6667 x 10^-5 m^4\n3. Determine the Maximum Bending Moment (M_max):\nFor a cantilever beam, the maximum bending moment occurs at the fixed support. It’s the sum of the moments due to the distributed load and the point load.\n\nMoment due to uniformly distributed load (M_w): M_w = w * L^2 / 2 M_w = (5000 N/m) * (3 m)^2 / 2 M_w = 5000 N/m * 9 m^2 / 2 M_w = 45000 Nm / 2 M_w = 22500 Nm\nMoment due to point load (M_P): M_P = P * L M_P = (10000 N) * (3 m) M_P = 30000 Nm\nTotal Maximum Bending Moment (M_max): M_max = M_w + M_P M_max = 22500 Nm + 30000 Nm M_max = 52500 Nm\n\n4. Determine the Distance to the Extreme Fiber (y_max):\nFor a rectangular cross-section, the neutral axis is at the geometric center. The maximum stress occurs at the top or bottom surface. y_max = h / 2\ny_max = 0.2 m / 2 y_max = 0.1 m\n5. Calculate the Maximum Bending Stress (σ_max):\nThe bending stress formula is: σ_max = (M_max * y_max) / I\nσ_max = (52500 Nm * 0.1 m) / (6.6667 x 10^-5 m^4) σ_max = 5250 Nm^2 / (6.6667 x 10^-5 m^4) σ_max = 78749439.06 N/m^2\nConvert to Pascals (Pa) or MegaPascals (MPa): σ_max = 78,749,439.06 Pa σ_max ≈ 78.75 x 10^6 Pa σ_max ≈ 78.75 MPa\nConclusion:\nThe maximum bending stress in the cantilever beam is approximately 78.75 MPa.\n\n\nYou can see that the model used no tokens for the thinking step:\n\nconsole.log(JSON.stringify(physics_problem_response.usageMetadata, null, 2));\n\n{\n  \"promptTokenCount\": 97,\n  \"candidatesTokenCount\": 871,\n  \"totalTokenCount\": 968,\n  \"promptTokensDetails\": [\n    {\n      \"modality\": \"TEXT\",\n      \"tokenCount\": 97\n    }\n  ]\n}\n\n\nThen you can set a fixed maximum budget (thinkingBudget=4096, or 4096 tokens) for the thinking step to see how the model performs.\nYou can see that, even producing a similar result for the same prompt, the amount of details shared in the answer makes it deeper and more consistent.\n\nconst physics_problem_response_2 = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: [\n    `\n      A cantilever beam of length L=3m has a rectangular cross-section (width b=0.1m, height h=0.2m) and is made of steel (E=200 GPa).\n      It is subjected to a uniformly distributed load w=5 kN/m along its entire length and a point load P=10 kN at its free end.\n      Calculate the maximum bending stress (σ_max).\n      `,\n  ],\n  config: {\n    thinkingConfig: {\n      thinkingBudget: 4096,\n    },\n  },\n});\ntslab.display.markdown(physics_problem_response_2.text ?? \"\");\n\nTo calculate the maximum bending stress (σ_max) in the cantilever beam, we need to follow these steps:\n\nCalculate the moment of inertia (I) of the cross-section.\nDetermine the maximum bending moment (M_max) acting on the beam.\nCalculate the maximum distance (y_max) from the neutral axis to the extreme fiber.\nApply the bending stress formula: σ_max = (M_max * y_max) / I.\n\nLet’s break down each step:\nGiven Data: * Length L = 3 m * Width b = 0.1 m * Height h = 0.2 m * Young’s Modulus E = 200 GPa = 200 × 10^9 Pa (or N/m²) - Note: E is not needed for stress calculation directly, only for deflection. * Uniformly distributed load w = 5 kN/m = 5 × 10^3 N/m * Point load P = 10 kN = 10 × 10^3 N\n\n1. Calculate the Moment of Inertia (I) for a rectangular cross-section: The formula for the moment of inertia of a rectangle about its centroidal axis is: I = (b * h^3) / 12\nI = (0.1 m * (0.2 m)^3) / 12 I = (0.1 * 0.008) / 12 I = 0.0008 / 12 I = 6.6667 × 10^-5 m^4\n\n2. Determine the Maximum Bending Moment (M_max): For a cantilever beam, the maximum bending moment occurs at the fixed support (where it connects to the wall). We need to sum the moments caused by each load at this point.\n\nMoment due to the point load P at the free end (M_P): M_P = P * L M_P = (10 × 10^3 N) * (3 m) M_P = 30 × 10^3 N·m = 30 kN·m\nMoment due to the uniformly distributed load w along the entire length (M_w): This can be thought of as a concentrated load (w * L) acting at the centroid of the distributed load, which is L/2 from the fixed support. M_w = (w * L) * (L/2) M_w = w * L^2 / 2 M_w = (5 × 10^3 N/m) * (3 m)^2 / 2 M_w = (5 × 10^3 * 9) / 2 M_w = 45 × 10^3 / 2 M_w = 22.5 × 10^3 N·m = 22.5 kN·m\nTotal Maximum Bending Moment (M_max): M_max = M_P + M_w M_max = 30 × 10^3 N·m + 22.5 × 10^3 N·m M_max = 52.5 × 10^3 N·m = 52.5 kN·m\n\n\n3. Calculate the Maximum Distance (y_max) from the neutral axis: For a rectangular cross-section, the neutral axis is at the geometric center, and the maximum distance to the extreme fibers (top or bottom surface) is half the height. y_max = h / 2 y_max = 0.2 m / 2 y_max = 0.1 m\n\n4. Apply the Bending Stress Formula (σ_max): The maximum bending stress occurs at the location of M_max (fixed support) and at y_max (top/bottom surface). σ_max = (M_max * y_max) / I\nσ_max = ( (52.5 × 10^3 N·m) * (0.1 m) ) / (6.6667 × 10^-5 m^4) σ_max = (5.25 × 10^3 N·m²) / (6.6667 × 10^-5 m^4) σ_max = 78,750,000 N/m²\nConvert to Megapascals (MPa), where 1 MPa = 10^6 Pa: σ_max = 78.75 × 10^6 Pa σ_max = 78.75 MPa\n\nFinal Answer: The maximum bending stress (σ_max) in the cantilever beam is 78.75 MPa.\n\n\nNow you can see that the model used around 1300 tokens for the thinking step (not necessarily using the full budget you set):\n\nconsole.log(JSON.stringify(physics_problem_response_2.usageMetadata, null, 2));\n\n{\n  \"promptTokenCount\": 97,\n  \"candidatesTokenCount\": 1125,\n  \"totalTokenCount\": 2564,\n  \"promptTokensDetails\": [\n    {\n      \"modality\": \"TEXT\",\n      \"tokenCount\": 97\n    }\n  ],\n  \"thoughtsTokenCount\": 1342\n}\n\n\n\n\nWorking with multimodal problems\nThis geometry problem requires complex reasoning and is also using Gemini multimodal abilities to read the image. In this case, you are fixing a value to the thinkingBudget so the model will use up to 8196 tokens for the thinking step.\n\nconst fs = require(\"fs\") as typeof import(\"fs\");\nconst path = require(\"path\") as typeof import(\"path\");\n\nconst IMG_URL = \"https://storage.googleapis.com/generativeai-downloads/images/geometry.png\";\n\nconst downloadFile = async (url: string, filePath: string) =&gt; {\n  if (!fs.existsSync(path.dirname(filePath))) {\n    fs.mkdirSync(path.dirname(filePath), { recursive: true });\n  }\n  const response = await fetch(url);\n  if (!response.ok) {\n    throw new Error(`Failed to download image: ${response.statusText}`);\n  }\n  const buffer = await response.blob();\n  const bufferData = Buffer.from(await buffer.arrayBuffer());\n  fs.writeFileSync(filePath, bufferData);\n};\n\nconst geometry_img_path = path.join(\"../assets/thinking\", \"geometry.png\");\nawait downloadFile(IMG_URL, geometry_img_path);\ntslab.display.png(fs.readFileSync(geometry_img_path));\n\n\n\n\n\n\n\n\n\nconst image_file = await ai.files.upload({\n  file: geometry_img_path,\n  config: {\n    mimeType: \"image/png\",\n    displayName: \"geometry.png\",\n  },\n});\n\nconst geometry_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: [\n    google.createPartFromUri(image_file.uri ?? \"\", image_file.mimeType ?? \"image/png\"),\n    \"What's the area of the overlapping region?\",\n  ],\n  config: {\n    thinkingConfig: {\n      thinkingBudget: 8192,\n    },\n  },\n});\ntslab.display.markdown(geometry_response.text ?? \"\");\n\nLet’s break down the problem by identifying the shapes and their dimensions from the given image.\n\nThe Circle:\n\nThe numbers ‘3’ within the blue circle clearly indicate that the radius of the circle, r, is 3.\nThe green lines originating from the center of the circle (which is also the right-angle vertex of the triangle) extend along the axes. These lines form a right angle (90 degrees), indicating that the portion of the circle relevant to the overlap is a quarter circle.\nThe area of a full circle is πr². The area of this quarter circle is (1/4)πr² = (1/4)π(3)² = 9π/4.\n\nThe Triangle:\n\nIt’s a right-angled triangle, with its right angle coinciding with the center of the circle. Let’s place this vertex at the origin (0,0) of a coordinate system.\nThe vertical leg has a ‘3’ next to it, indicating its length is 3. So, this leg extends from (0,0) to (0,3).\nThe horizontal leg has two ’3’s next to it. This means its total length is 3 + 3 = 6. So, this leg extends from (0,0) to (6,0).\nThe vertices of the triangle are (0,0), (6,0), and (0,3).\nThe hypotenuse connects (0,3) and (6,0). The equation of the line passing through these points can be found: Slope m = (0 - 3) / (6 - 0) = -3/6 = -1/2. Using the point-slope form y - y1 = m(x - x1) with (6,0): y - 0 = (-1/2)(x - 6), so y = -1/2 x + 3.\n\nThe Overlapping Region:\n\nThe green-shaded area represents the overlapping region between the circle and the triangle.\nThis region is bounded by the x-axis, the y-axis, the hypotenuse of the triangle, and the arc of the circle.\nWe need to determine which boundary is “inner” in the first quadrant.\nThe hypotenuse y = -1/2 x + 3 intersects the circle x² + y² = 3² at two points. Let’s find them: Substitute y from the line equation into the circle equation: x² + (-1/2 x + 3)² = 9 x² + (1/4 x² - 3x + 9) = 9 5/4 x² - 3x = 0 Factor out x: x(5/4 x - 3) = 0 This gives two solutions for x:\n\nx = 0: If x=0, then y = -1/2(0) + 3 = 3. This is the point (0,3). This point is a vertex of the triangle and also on the circle (0² + 3² = 9).\n5/4 x - 3 = 0: 5/4 x = 3 =&gt; x = 12/5 = 2.4. If x=2.4, then y = -1/2(2.4) + 3 = -1.2 + 3 = 1.8. This is the point (2.4, 1.8). Let’s call this point P = (2.4, 1.8). We check if it’s on the circle: 2.4² + 1.8² = 5.76 + 3.24 = 9. Yes, it is.\n\nNow we understand the boundaries of the overlapping region:\n\nIt starts at the origin (0,0).\nIt goes up along the y-axis to (0,3).\nThen it follows the hypotenuse (line segment from (0,3) to (2.4, 1.8)).\nFrom (2.4, 1.8), it follows the arc of the circle to (3,0) (the point where the circle intersects the positive x-axis).\nFinally, it goes along the x-axis back to the origin (0,0).\n\nWe can calculate this area by splitting it into two parts:\n\nA polygon (a trapezoid or a triangle + rectangle)\nA circular sector.\n\nMethod 1: Decomposition Let O = (0,0), A = (0,3), P = (2.4, 1.8), B = (3,0). The overlapping region is the area of quadrilateral OAPB. We can split it into:\n\nTriangle OAP’: A triangle with vertices O=(0,0), A=(0,3), and P=(2.4, 1.8). The area of a triangle given coordinates (0,0), (x1, y1), (x2, y2) is 1/2 |x1*y2 - x2*y1|. This is simpler. Using the base along y-axis, (0,3) as vertex A, and (0,0) as vertex O, and the x-coordinate of P as height: Area(OAP) = (1/2) * base * height = (1/2) * OA * (x-coordinate of P) = (1/2) * 3 * 2.4 = 3.6.\nCircular Sector OPB: A sector of the circle with radius 3, from the x-axis (angle 0) to the angle θ corresponding to point P=(2.4, 1.8). We find cos(θ) = x/r = 2.4/3 = 4/5 and sin(θ) = y/r = 1.8/3 = 3/5. So, θ = arcsin(3/5). The angle for point B=(3,0) is 0 radians. The angle for point P=(2.4,1.8) is θ = arcsin(3/5). Area of sector OPB = (1/2)r² * (angle in radians) = (1/2)(3)² * arcsin(3/5) = (9/2)arcsin(3/5).\nTotal Area = Area(OAP) + Area(Sector OPB) Total Area = 3.6 + (9/2)arcsin(3/5).\n\nMethod 2: Integration The area can be expressed as the sum of two integrals: Area = ∫[from 0 to 2.4] (-1/2 x + 3) dx + ∫[from 2.4 to 3] sqrt(9 - x²) dx The first integral calculates the area under the hypotenuse from x=0 to x=2.4. This is a trapezoid with vertices (0,0), (2.4,0), (2.4,1.8), (0,3). Its area is (1/2) * (3 + 1.8) * 2.4 = (1/2) * 4.8 * 2.4 = 5.76. The second integral calculates the area under the circle’s arc from x=2.4 to x=3. This is [ (x/2)sqrt(9 - x²) + (9/2)arcsin(x/3) ] from 2.4 to 3. Evaluating at x=3: (3/2)sqrt(0) + (9/2)arcsin(1) = 9π/4. Evaluating at x=2.4: (2.4/2)sqrt(9 - 2.4²) + (9/2)arcsin(2.4/3) = 1.2 * sqrt(9 - 5.76) + (9/2)arcsin(0.8) = 1.2 * sqrt(3.24) + (9/2)arcsin(4/5) = 1.2 * 1.8 + (9/2)arcsin(4/5) = 2.16 + (9/2)arcsin(4/5). So the second integral’s value is 9π/4 - (2.16 + (9/2)arcsin(4/5)). Total Area = 5.76 + 9π/4 - 2.16 - (9/2)arcsin(4/5) Total Area = 3.6 + 9π/4 - (9/2)arcsin(4/5).\nWe know that arcsin(4/5) = π/2 - arcsin(3/5). (Since sin(θ) = 3/5 means cos(θ) = 4/5, so θ is arcsin(3/5) and π/2 - θ is arccos(3/5) or arcsin(4/5)). Substitute this into the expression: Total Area = 3.6 + 9π/4 - (9/2)(π/2 - arcsin(3/5)) = 3.6 + 9π/4 - 9π/4 + (9/2)arcsin(3/5) = 3.6 + (9/2)arcsin(3/5). Both methods yield the same result.\n\n\nThe area of the overlapping region is 3.6 + (9/2)arcsin(3/5).\nThe final answer is \\(\\boxed{3.6 + \\frac{9}{2}\\arcsin\\left(\\frac{3}{5}\\right)}\\)\n\n\n\n\nSolving brain teasers\nHere’s another brain teaser based on an image, this time it looks like a mathematical problem, but it cannot actually be solved mathematically. If you check the toughts of the model you’ll see that it will realize it and come up with an out-of-the-box solution.\nIn this case, you are fixing a value to the thinkingBudget so the model will use up to 24576 tokens for the thinking step\n\nconst POOL_IMG_URL = \"https://storage.googleapis.com/generativeai-downloads/images/pool.png\";\n\nconst pool_img_path = path.join(\"../assets/thinking\", \"pool.png\");\nawait downloadFile(POOL_IMG_URL, pool_img_path);\ntslab.display.png(fs.readFileSync(pool_img_path));\n\n\n\n\n\n\n\n\nFirst you can check how the model performs without reasoning (thinkingBudget=0):\n\nconst pool_image_file = await ai.files.upload({\n  file: pool_img_path,\n  config: {\n    mimeType: \"image/png\",\n    displayName: \"pool.png\",\n  },\n});\nconst pool_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: [\n    google.createPartFromUri(pool_image_file.uri ?? \"\", pool_image_file.mimeType ?? \"image/png\"),\n    \"How do I use those three pool balls to sum up to 30?\",\n  ],\n  config: {\n    thinkingConfig: {\n      thinkingBudget: 0,\n    },\n  },\n});\ntslab.display.markdown(pool_response.text ?? \"\");\n\nThis is a classic riddle! Here’s how you can do it:\nYou can sum up to 30 using three of the pool balls by flipping the number “9” upside down to make it a “6”.\nThen you have: * 7 * 6 (from the flipped 9) * 13\n7 + 6 + 13 = 26\nAh, I re-read your request and realized you need to sum to 30, not 26. My apologies. Let’s try again with the balls available: 7, 9, 11, 13.\nThe numbers on the balls are 7, 9, 11, and 13. All of these are odd numbers.\nThe sum of any three odd numbers will always be an odd number.\nSince 30 is an even number, it’s impossible to sum to 30 using only three of these particular pool balls (7, 9, 11, 13) in their given numerical values.\nTherefore, the only way to solve this riddle is usually by flipping the 9 to make it a 6. However, that would only get you to 26, as I demonstrated above.\nIf the rule is that you must use only the numbers as they appear on the balls (7, 9, 11, 13) and can’t flip any numbers, then it’s a trick question and it’s impossible.\n\n\nAs you can notice, the model struggled to find a way to get to the result - and ended up suggesting to use different pool balls.\nNow you can use the model reasoning to solve the riddle:\n\nconst pool_response_2 = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: [\n    google.createPartFromUri(pool_image_file.uri ?? \"\", pool_image_file.mimeType ?? \"image/png\"),\n    \"How do I use those three pool balls to sum up to 30?\",\n  ],\n  config: {\n    thinkingConfig: {\n      thinkingBudget: 24576,\n    },\n  },\n});\ntslab.display.markdown(pool_response_2.text ?? \"\");\n\nThis is a classic riddle!\nYou need to use the trick of flipping the 9 ball upside down to make it a 6.\nThen, you can use these three balls: * 6 (the flipped 9 ball) * 11 * 13\n6 + 11 + 13 = 30\n\n\n\n\nSolving a math puzzle with the maximum thinkingBudget\nThis is typically a case where you want to fix a budget, as the model can spend a lot of time thinking in all directions before finding the right answer. It should not be too low either as non-thinking models have trouble with such questions.\nPlay with the thinking budget and try to find how much it needs to be able to find the right answer most of the time.\n\nconst math_problem_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: [\n    `\n    How can you obtain 565 with 10 8 3 7 1 and 5 and the common operations?\n    You can only use a number once.\n    `,\n  ],\n  config: {\n    thinkingConfig: {\n      thinkingBudget: 24576,\n    },\n  },\n});\ntslab.display.markdown(math_problem_response.text ?? \"\");\n\nHere’s how you can obtain 565 using the given numbers and common operations:\n\nMultiply 8 by 7: 8 * 7 = 56\nMultiply the result by 10: 56 * 10 = 560\nAdd 5 to the result: 560 + 5 = 565\n\nSo, the full expression is: (8 * 7 * 10) + 5 = 565\nNumbers used: 8, 7, 10, 5 (each used only once). Numbers not used: 3, 1 (which is allowed).\n\n\n\n\nWorking thoughts summaries\nSummaries of the model’s thinking reveal its internal problem-solving pathway. Users can leverage this feature to check the model’s strategy and remain informed during complex tasks.\nFor more details about Gemini 2.5 thinking capabilities, take a look at the Gemini models thinking guide.\n\nconst thought_experiment_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: [\n    `\n    Alice, Bob, and Carol each live in a different house on the same street: red, green, and blue.\n    The person who lives in the red house owns a cat.\n    Bob does not live in the green house.\n    Carol owns a dog.\n    The green house is to the left of the red house.\n    Alice does not own a cat.\n    Who lives in each house, and what pet do they own?\n    `,\n  ],\n  config: {\n    thinkingConfig: {\n      includeThoughts: true,\n    },\n  },\n});\n\nYou can check both the thought summaries and the final model response:\n\nfor (const part of thought_experiment_response.candidates?.[0]?.content?.parts ?? []) {\n  if (!part.text) continue;\n  if (part.thought) {\n    tslab.display.markdown(`## **Thought Summary**`);\n    tslab.display.markdown(part.text ?? \"\");\n  } else {\n    tslab.display.markdown(`## **Response**`);\n    tslab.display.markdown(part.text ?? \"\");\n  }\n}\n\nThought Summary\n\n\nInitial Thoughts and Problem Decomposition\nAlright, let’s break this down. The goal is to match people to houses and pets. I’m thinking a table or matrix is the best way to keep track of this. I need to list all the entities: people (Alice, Bob, Carol), houses (red, green, blue), and pets (cat, dog, and implicitly a third, or maybe not). I’ll go through the clues one by one and mark my deductions. This is all about finding relationships and cross-references.\nProcessing the Clues and Building Deductions\nFirst clue: Red house = Cat. Easy. Then, Bob doesn’t live in the green house. Noted. Carol owns a dog. Alright, Carol = Dog. The green house is to the left of the red house. Got it, Green &lt; Red. Alice doesn’t own a cat. Now things get interesting!\nI’m starting to see some links here. If the red house has a cat and Alice doesn’t, Alice can’t be in the red house. And since Carol has a dog and the red house has the cat, Carol can’t be in the red house either. That leaves Bob! Bob must be in the Red House with the Cat. Major breakthrough!\nFilling the Gaps and Combining Information\nNow, I know Bob is in the red house. Bob does not live in the green house, that’s already true. I know Carol owns a dog. The houses are arranged Green &lt; Red &lt; Blue. So Green is to the left of Bob. That’s clear.\nSo, it’s either Green, Red, Blue OR Blue, Green, Red. Because Bob is in the Red House, it becomes Green, Bob (Red), Blue. Carol owns the dog, Alice is left. I’m at the point now where I’m trying to decide the houses for the dog and Alice!\nCarol must have the dog in Green or Blue house. No direct information!\nConfronting the Ambiguity and Re-Evaluating\nHmm, this is tricky. No clue is helping me directly match either Carol to Green, or Carol to Blue. I need to rethink the pets. Is there an unstated assumption here? It seems to imply that everyone has a named pet but I’m not seeing how to pair them. I’ve re-read the puzzle and clues many times.\nIf Carol owns the dog, and Bob owns the cat, then what about Alice? No specific pet is associated with Alice. This implies Alice has the unnamed pet or none. My working theory is that there’s an ambiguity in the pet assignments.\nThe crucial constraint is that the green house is to the left of the red house. In three possible arrangements: Green - Red - Blue, Blue - Green - Red, and Green - Blue - Red. I know Bob lives in the Red House, with the cat, that means it’s Green - Bob (Red) - Blue. Then I need to decide, who lives in which house?\nCarol owns the Dog and must be in either the Green or Blue house! Alice is the remaining house. There’s nothing that helps me, at all, decide which person is in what color house, for green and blue!\nAcknowledging the Underspecification\nThere seems to be an ambiguity here. This is a common situation with these puzzles! There might be an unstated assumption that everyone must have a pet in this context. However, I’m stuck with two possible solutions for Alice and Carol, and I don’t see any logic to eliminate either of these solutions.\nFinalized Solution & Acknowledgment of Limitations\nSo, the definite part is: Bob lives in the Red House and owns the Cat. The green house is to the left of Bob’s red house.\nThere are two possibilities:\n\nCarol (Dog) is in the Green House, Alice owns the “unspecified” pet, and lives in Blue house\nCarol (Dog) is in the Blue House, Alice owns the “unspecified” pet, and lives in Green house\n\nI can’t pinpoint a specific pet for Alice. Unless there’s an extremely subtle detail, these are the only two possible consistent solutions.\n\n\nResponse\n\n\nLet’s break this down clue by clue to solve the puzzle:\n\n“The person who lives in the red house owns a cat.”\n\nDeduction: Red House = Cat Owner.\n\n“Carol owns a dog.”\n\nDeduction: Carol = Dog.\n\n“Alice does not own a cat.”\n\nDeduction: Alice is not the Cat Owner.\n\nCombine 1, 2, and 3:\n\nThe Red House owner has a cat.\nAlice does not own a cat, so Alice doesn’t live in the Red House.\nCarol owns a dog, so Carol doesn’t live in the Red House (because the Red House has a cat).\nThis leaves only one person for the Red House: Bob.\nDeduction: Bob lives in the Red House and owns a Cat.\n\n“Bob does not live in the green house.”\n\nThis is consistent with our deduction that Bob lives in the Red House. No new information for Bob, but confirms our path.\n\n“The green house is to the left of the red house.”\n\nWe have three houses: Red, Green, Blue.\nSince Green is to the left of Red, and Bob lives in the Red house, the houses must be in this order from left to right: Green, Red, Blue.\nSo:\n\nLeft house: Green\nMiddle house: Red (Bob, Cat)\nRight house: Blue\n\n\nFinal Assignments for Alice and Carol:\n\nWe know Bob lives in the Red House and owns a cat.\nThe remaining houses are Green and Blue.\nThe remaining people are Alice and Carol.\nWe know Carol owns a dog.\nAlice does not own a cat (which is already true, as Bob owns the cat).\n\nAt this point, the puzzle has two possible solutions for Alice and Carol’s locations and pets, as there’s no further information to distinguish between them:\nPossibility 1:\n\nGreen House: Carol, who owns a dog.\nRed House: Bob, who owns a cat.\nBlue House: Alice, whose pet is not specified by the clues (or she owns no pet explicitly mentioned).\n\nPossibility 2:\n\nGreen House: Alice, whose pet is not specified by the clues (or she owns no pet explicitly mentioned).\nRed House: Bob, who owns a cat.\nBlue House: Carol, who owns a dog.\n\n\nSummary of the solution:\n\nBob: Lives in the Red House and owns a Cat.\n\nAnd then, there are two possibilities for Alice and Carol:\n\nScenario A:\n\nCarol: Lives in the Green House and owns a Dog.\nAlice: Lives in the Blue House and her pet is not specified (or she owns no pet explicitly mentioned by the clues).\n\nScenario B:\n\nAlice: Lives in the Green House and her pet is not specified (or she owns no pet explicitly mentioned by the clues).\nCarol: Lives in the Blue House and owns a Dog.\n\n\n\n\nYou can also use see the thought summaries in streaming experiences:\n\nconst thought_experiment_streaming_response = await ai.models.generateContentStream({\n  model: MODEL_ID,\n  contents: [\n    `\n    Alice, Bob, and Carol each live in a different house on the same street: red, green, and blue.\n    The person who lives in the red house owns a cat.\n    Bob does not live in the green house.\n    Carol owns a dog.\n    The green house is to the left of the red house.\n    Alice does not own a cat.\n    Who lives in each house, and what pet do they own?\n    `,\n  ],\n  config: {\n    thinkingConfig: {\n      includeThoughts: true,\n    },\n  },\n});\n\nlet thoughts = \"\";\nlet answers = \"\";\n\nfor await (const chunk of thought_experiment_streaming_response) {\n  for (const part of chunk.candidates?.[0]?.content?.parts ?? []) {\n    if (!part.text) continue;\n    if (part.thought) {\n      if (!thoughts) tslab.display.markdown(`## **Thought Summary**`);\n      tslab.display.markdown(part.text ?? \"\");\n      thoughts += part.text ?? \"\";\n    } else {\n      if (!answers) tslab.display.markdown(`## **Response**`);\n      tslab.display.markdown(part.text ?? \"\");\n      answers += part.text ?? \"\";\n    }\n  }\n}\n\nThought Summary\n\n\nUnpacking the Clues\nI’ve finally started unpacking the core clues of this riddle. Initially, I felt overwhelmed, but now I’m making progress, mapping the houses and pets. I am focusing on eliminating possibilities based on the given constraints. I’m building a matrix to track the relationships, and I think I’m getting close to a breakthrough.\n\n\nDeciphering Relationships\nI’m now diving deeper into the relationships, having established my basic grid. The initial structure feels solid, but the spatial aspect is intriguing. I’m focusing on parsing the first clue about the colors and people, attempting to lay out all permutations and how they impact each other. Right now I am focusing on eliminating contradictory arrangements.\n\n\nConnecting the Dots\nI’ve been meticulously placing clues into the grid, leveraging the established structure. Carol’s dog, and the red house’s cat, are forming key anchors. The spatial clue (“Green to the left of Red”) is proving particularly useful in limiting the possibilities. I’m focusing on the constraints imposed by Bob and Alice to further refine the arrangement.\n\n\nConstructing the Solution\nThe connections are solidifying. I’m now leveraging the logical deductions to finalize the grid, focusing on the house colors. With Bob in Red, and the positional clue about Green, it is starting to become clearer. Determining Alice’s house is key. I’m getting close to the definitive answer.\n\n\nUnveiling the Arrangement\nI’ve finally pinpointed that Bob lives in the red house and owns the cat. Leveraging the spatial clue, I’m now certain Green is to the left of Red. I am determining who lives in the Green and Blue houses and with what pet. I am now working through the final constraint with a clearer picture of the arrangement.\n\n\nSolidifying the Grid\nI’m making progress, but still need to determine who owns the third pet, and whose house is blue. The structure feels much more concrete. Carol owns the dog, and Alice does not own the cat. I am refining the arrangement and building a solution that reflects all the constraints.\n\n\nMapping the Solution\nThe riddle’s framework is solidifying. I’ve placed Bob in the red house with the cat and Carol owning the dog. The positional information about the green house being left of the red is proving key. I’m focusing on Alice’s location and what pet she must have. It’s becoming clearer.\n\n\nSolving the Riddle\nI’ve been working through the clues, using both spatial and direct-relationship constraints to build the solution. Determining Alice and Carol’s houses and pets is the current focus, after correctly placing Bob in the Red house. I am now working through the final house and pet associations.\n\n\nMapping the Final Associations\nI’m zeroing in on the final pieces. Carol and Alice have their houses, the positional clue is helpful. Bob is confirmed in the Red house. The pet situation feels resolved. My primary focus is establishing the definitive house and pet assignments. I’m now certain of the solution.\n\n\nSolving the Puzzle\nI’ve been meticulously following the clues. After placing Bob in the red house with the cat and Carol with the dog, I realized that the “green to the left of red” clue solidifies the positional layout. Now, I’m focusing on Alice and the unknown pet. I’m getting closer to a complete solution.\n\n\nSolving the Puzzle\nI’m now integrating the spatial clue with the pet ownership information, focusing on the house colors. I’ve placed Bob in the red house with the cat, confirming the first part of the riddle. Determining Alice’s association with a house and pet will lead to the final answer. I am making rapid progress now.\n\n\nRevising the Structure\nAfter evaluating multiple permutations, I’ve confirmed Bob lives in the red house with the cat. Carol and Alice occupy the green and blue houses respectively. I’m building out the final assignments for each person and pet.\n\n\nAnalyzing the Structure\nAfter assessing multiple structures, I’ve confirmed Bob lives in the red house with the cat, leveraging all available clues. Carol resides in the green house and owns the dog. I am now certain Alice resides in the blue house.\n\n\nAnalyzing the Solution\nThe riddle’s central structure is now solid. Bob lives in the red house, and Carol resides in the green house with her dog, while Alice is confirmed to be in the blue house. The association with the unknown pet is no longer an issue, because the prompt never mentioned a specific pet, only that she did not own a cat. The solution’s finality is clear.\n\n\nAnalyzing the Solution Space\nI’m now zeroing in on a critical point. I’ve deduced Bob lives in the red house with the cat, a solid foundation. The challenge is assigning Carol and Alice’s homes and pets. Currently, their assignments are interchangeable, and the current clues don’t break the tie. I’ll re-evaluate the spatial constraints for clarity.\n\n\nAnalyzing the Structure\nIt’s clear that Bob lives in the red house with the cat, a solid foundation. While Carol owns the dog, I’m still weighing the possibilities for Alice’s home and pet situation. The green house to the left of the red house gives an additional hint. My focus is finalizing Carol and Alice’s houses.\n\n\nAnalyzing the Structure\nIt’s clear Bob lives in the red house with the cat and Carol has the dog. I’ve placed the Green house to the left of Red. The final piece is linking Alice with Green or Blue, which remains flexible. I’m focusing on Alice’s association and which pet she may have.\n\n\nRe-evaluating the Structure\nAfter solidifying the core relationships, I am evaluating possible permutations, leveraging the established base. Bob is confirmed in the red house, leaving the assignment of Carol and Alice’s homes open. I am focusing on positional constraints to determine the definitive assignments. The final answer feels close.\n\n\nAnalyzing the Solution Space\nI’m wrestling with the final assignments, as the two remaining arrangements feel equally valid. Bob is definitively in the red house with the cat, and Carol has the dog. My attention is on Alice, the remaining house, and the “not cat” clue. I must ensure I do not over-interpret the final constraints. I’m focusing on the spatial and relational aspects.\n\n\nMapping the Final Associations\nThe riddle’s structure is clear. Bob is in the red house with the cat, and Carol has the dog. The spatial clue about the green house being left of the red is key. My focus is finalizing Alice’s location and possible pet. It’s becoming clearer.\n\n\nRe-evaluating the Structure\nThe solution feels very clear; however, I am now focused on a possible “no pet” scenario. Alice’s “not cat” clue makes me consider she owns nothing, or, perhaps, a pet we are not privy to. Since there are only two pets, “no pet” seems the implicit rule.\n\n\nFinalizing the Arrangements\nI am now certain that Bob lives in the red house with the cat, confirming the grid’s structure. By deducing Alice’s and Carol’s possible arrangements, their final assignments should be easier to deduce. However, the constraints are a little tricky. I’m focusing on Alice’s pet status.\n\n\nAnalyzing the Structure\nIt’s clear that Bob lives in the red house with the cat and Carol resides in the green house with her dog, while Alice is confirmed to be in the blue house. It is starting to feel conclusive, as the solution now is clear.\n\n\nAnalyzing the Structure\nIt’s clear I’m working through the connections between the people, houses, and pets with diligence. Bob in the red house with the cat is now firmly established. Carol and Alice occupy green and blue houses respectively. I’m focusing on the final assignment to make a fully conclusive solution.\n\n\nFinalizing the Solution\nThe connections are established. I’ve placed Bob in the red house with the cat, leveraging all available clues. Carol resides in the green house and owns the dog, while Alice is confirmed in the blue house. The association with the unknown pet is no longer an issue, because the prompt never mentioned a specific pet. The solution’s finality is clear.\n\n\nSynthesizing the Solution\nThe riddle’s structure is clear, and I’ve placed Bob in the red house with the cat, leveraging all of the clues. The spatial clue about the green house being to the left of the red is also key. However, I am still trying to determine the relationship between Alice, Carol, and the other houses, with the riddle’s ambiguity being somewhat challenging to overcome.\n\n\nDeciphering the Relationships\nMy recent focus has been on parsing the clues regarding spatial positioning and direct relationships, having mapped the basic structure. The “Green to the left of Red” clue is critical, but it isn’t definitive on who resides in Green or Blue. Determining the pet for Alice is a key concern.\n\n\nRevising the Structure\nAfter reevaluating the constraints, I’m confident in Bob’s red house and cat ownership. The spatial element is clear, green is left of red. Currently, the riddle doesn’t differentiate between the arrangements of Carol and Alice’s homes. I will work further on Carol’s dog.\n\n\nAnalyzing the Structure\nAfter considering several arrangements, I am convinced that Bob lives in the red house with the cat, a firm starting point. The crucial part, however, is now trying to determine Alice and Carol’s assignments. The green house being to the left of the red house gives an added hint, however, the structure remains ambiguous.\n\n\nSynthesizing the Structure\nThe solution space is clarifying. I’ve placed Bob in the red house with the cat, a firm foundation. Carol owns the dog. The challenge is assigning the final pet and the Green and Blue houses. I’m focusing on the spatial constraint, but can’t distinguish between Alice and Carol.\n\n\nDeciphering the Relationships\nI’ve been meticulously analyzing the clues, starting to form a clear structure. I’m focusing on Carol’s Dog, Bob in Red, and spatial positions. Alice’s possible pet ownership is now my main priority; I’m building a solution that addresses her specific pet associations and house color.\n\n\nAnalyzing the Structure\nThe logical puzzle is now fully parsed. Bob is in the red house with the cat, a firm conclusion. The key constraints about Carol and Alice, the spatial aspect, and the pets require careful consideration, and I will continue to work on it with greater efficiency.\n\n\nI’m focusing on parsing the relationships within the riddle’s structure, aiming for definitive assignments. Bob is firmly established in the red house with the cat, a crucial starting point. My current focus centers on Alice and Carol, the green and blue houses, and the riddle’s implied “no pet” rule.\n\n\nParsing the Clues\nI’ve carefully examined the clues and established the core structure: Bob resides in the red house with the cat, Carol owns the dog, and Alice does not own the cat. The riddle’s focus is to determine the assignment of Green and Blue houses and the association of the remaining two pets and Alice’s assignment. I’m focusing on the spatial relationship.\n\n\nRevising the Structure\nHaving confirmed the basic framework, I’m now focused on the final assignments. Bob resides in the red house with the cat, and Carol has the dog. My attention shifts to Alice’s “not cat” situation and her house assignment. Leveraging the spatial clue, I’m analyzing potential placements to reach a conclusive solution.\n\n\nResponse\n\n\nLet’s break down the clues to solve this logic puzzle:\n1. Create a Grid to Organize Information:\n\n\n\nPerson\nHouse\nPet\n\n\n\n\nAlice\n\n\n\n\n\n\n\n |\nBob | | |\nCarol | | |\n2. Process Each Clue:\n\n**Clue 1: “Alice, Bob, and Carol each live in a\n\n\n\ndifferent house on the same street: red, green, and blue.”** * Confirms a 1:1 mapping of people, houses, and (implicitly) pets.\n\n**Clue 2: “The person who lives in\n\n\n\nthe red house owns a cat.”** * (Red House) -&gt; Cat\n\nClue 3: “Bob does not live in the green house.”\n\nBob ≠ Green House\n\n\n\n\n\nClue 4: “Carol owns a dog.” * Add this directly to the grid: Carol -&gt; Dog\n\n**Clue 5: “The green house is to the left of the red house\n\n\n\n.”** * This establishes a spatial relationship: Green &lt; Red\n\nClue 6: “Alice does not own a cat.”\n\nAlice ≠ Cat\n\n\n3. Deductions:\n\n\n\n\n\nDeduction 1: Identify the Cat Owner. * From Clue 4, Carol owns a dog. So Carol is not the cat owner. * From Clue 6, Alice does\n\n\nnot own a cat. * Since Alice and Carol don’t own the cat, Bob must own the cat.\n\nDeduction 2: Identify Who Lives in the Red House.\n\nFrom Cl\n\n\n\n\nue 2, the Red House owner has a cat. * From Deduction 1, Bob owns the cat. * Therefore, Bob lives in the Red House.\n\nUpdate the Grid:\n\n\n\n\n\n\nPerson\nHouse\nPet\n\n\n\n\nAlice\n\nNot Cat\n\n\nBob\nRed\nCat\n\n\nCarol\n\nDog\n\n\n\n\n**Deduction 3\n\n\n\n: Assign Remaining People to Houses.** * We know Bob lives in the Red House. * From Clue 6, Alice does not own a cat, so she cannot live in the Red House (where the\n\n\ncat owner lives). * From Clue 4, Carol owns a dog, so she cannot live in the Red House (where the cat owner lives). * This means the **Green House and the Blue House must be\n\n\noccupied by Alice and Carol.**\n\nDeduction 4: Consider the “to the left” clue.\n\nClue 5 states: “The green house is to the left of the red house.”\n\n\n\n\n\nWe know Bob is in the Red House. So the Green house is to the left of Bob’s house.\n\nThis means the street arrangement is either: [Blue] [Green] [Red (Bob)] or [Green]\n\n\n\n\n[Red (Bob)] [Blue]. * This clue helps establish the relative positions of the houses, but it does not provide any information to definitively place Alice or Carol in the Green house versus the Blue house. Both scenarios\n\n\n(Alice in Green, Carol in Blue OR Carol in Green, Alice in Blue) are consistent with all given clues.\nConclusion:\nBased on the information provided, we can definitively determine Bob’s house and pet, and Carol’s pet.\n\n\nHowever, the clues do not provide enough information to definitively determine whether Alice lives in the Green or Blue house, or Carol lives in the Green or Blue house. The problem implies a unique solution, but the information to distinguish between Alice and Carol’\n\n\ns houses is missing.\nHowever, if we are to provide a complete answer with the most logical deduction based on the usual structure of these puzzles (where there should be a unique pet for each person): Alice’s pet is not a\n\n\ncat, and Carol has a dog. Bob has the cat. This means Alice must have a third, unnamed pet, or no pet.\nHere’s the most complete and certain answer:\n\n**Bob lives in the Red House and owns\n\n\n\na cat. * Carol owns a dog. * Alice does not own a cat.**\nSince Alice and Carol are the only ones left for the Green and Blue houses, and there’s no further distinguishing information\n\n\n:\n\nThe Green House is occupied by either Alice (who does not own a cat) or Carol (who owns a dog).\nThe Blue House is occupied by the other person (Alice or Carol).",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Use Gemini thinking"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_thinking.html#thought-summary",
    "href": "quickstarts/Get_started_thinking.html#thought-summary",
    "title": "Use Gemini thinking",
    "section": "Thought Summary",
    "text": "Thought Summary",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Use Gemini thinking"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_thinking.html#response",
    "href": "quickstarts/Get_started_thinking.html#response",
    "title": "Use Gemini thinking",
    "section": "Response",
    "text": "Response",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Use Gemini thinking"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_thinking.html#thought-summary-1",
    "href": "quickstarts/Get_started_thinking.html#thought-summary-1",
    "title": "Use Gemini thinking",
    "section": "Thought Summary",
    "text": "Thought Summary",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Use Gemini thinking"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_thinking.html#response-1",
    "href": "quickstarts/Get_started_thinking.html#response-1",
    "title": "Use Gemini thinking",
    "section": "Response",
    "text": "Response",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Use Gemini thinking"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_thinking.html#working-with-gemini-thinking-models-and-tools",
    "href": "quickstarts/Get_started_thinking.html#working-with-gemini-thinking-models-and-tools",
    "title": "Use Gemini thinking",
    "section": "Working with Gemini thinking models and tools",
    "text": "Working with Gemini thinking models and tools\nGemini thinking models are compatible with the tools and capabilities inherent to the Gemini ecosystem. This compatibility allows them to interface with external environments, execute computational code, or retrieve real-time data, subsequently incorporating such information into their analytical framework and concluding statements.\n\nSolving a problem using the code execution tool\nThis example shows how to use the code execution tool to solve a problem. The model will generate the code and then execute it to get the final answer.\nIn this case, you are using the adaptive thinking_budget so the model will dynamically adjust the budget based on the complexity of the request.\nIf you want to experiment with a fixed budget, you can set the thinkingBudget to a specific value (e.g. thinkingBudget=4096).\n\nconst code_experiment_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: [\n    `\n    What are the best ways to sort a list of n numbers from 0 to m?\n    Generate and run Python code for three different sort algorithms.\n    Provide the final comparison between algorithm clearly.\n    Is one of them linear?\n    `,\n  ],\n  config: {\n    tools: [{ codeExecution: {} }],\n    thinkingConfig: {\n      thinkingBudget: 4096,\n    },\n  },\n});\nfor (const part of code_experiment_response.candidates?.[0]?.content?.parts ?? []) {\n  if (part.text) {\n    tslab.display.markdown(part.text ?? \"\");\n  }\n  if (part.executableCode) {\n    tslab.display.html(`&lt;pre&gt;${part.executableCode.code ?? \"\"}&lt;/pre&gt;`);\n  }\n  if (part.codeExecutionResult) {\n    tslab.display.markdown(`### **Code Execution Result**`);\n    tslab.display.markdown(part.codeExecutionResult.output ?? \"\");\n  }\n}\n\nFor sorting a list of n numbers ranging from 0 to m, the “best ways” depend largely on the relationship between n and m.\n\nNon-Comparison Sorts (e.g., Counting Sort, Radix Sort): These are generally the best when the range m is not excessively large compared to n. They can achieve linear time complexity.\n\nCounting Sort: Ideal when m is relatively small. It works by counting the occurrences of each distinct element and then using those counts to determine the sorted position of each element.\nRadix Sort: Efficient when m is very large but the numbers have a limited number of digits or bits. It sorts numbers digit by digit (or bit by bit) using a stable sorting algorithm (often Counting Sort) as a subroutine.\n\nComparison Sorts (e.g., Merge Sort, Quick Sort, Heap Sort): These are general-purpose sorting algorithms that work by comparing elements. Their lower bound for time complexity is O(n log n), regardless of the range of numbers. They are suitable when m is very large, making non-comparison sorts less efficient or impractical due to memory constraints.\n\nGiven the constraint 0 to m, Counting Sort stands out as a highly relevant and potentially linear-time algorithm.\nLet’s proceed with implementing and running three different sort algorithms: Counting Sort, Merge Sort, and Quick Sort, and then compare them.\n\n1. Counting Sort\nTime Complexity: O(n + m) Space Complexity: O(m) Stability: Yes In-place: No\n\n\n\nimport random\nimport time\n\ndef counting_sort(arr, m):\n    \"\"\"\n    Sorts an array of integers where elements are in the range [0, m].\n    \"\"\"\n    n = len(arr)\n    output = [0] * n\n    count = [0] * (m + 1)\n\n    # Store count of each character\n    for i in range(n):\n        count[arr[i]] += 1\n\n    # Change count[i] so that count[i] now contains actual\n    # position of this character in output array\n    for i in range(1, m + 1):\n        count[i] += count[i - 1]\n\n    # Build the output array\n    # Iterate in reverse to maintain stability\n    i = n - 1\n    while i &gt;= 0:\n        output[count[arr[i]] - 1] = arr[i]\n        count[arr[i]] -= 1\n        i -= 1\n\n    # Copy the output array to arr, so that arr now\n    # contains sorted characters\n    for i in range(n):\n        arr[i] = output[i]\n    return arr\n\n# --- Test Setup ---\nn_test = 100000  # Number of elements\nm_test = 50000   # Max value in the list (range 0 to m_test)\ndata_counting_sort = [random.randint(0, m_test) for _ in range(n_test)]\n# Create copies for other sorts\ndata_merge_sort = list(data_counting_sort)\ndata_quick_sort = list(data_counting_sort)\n\n# --- Run Counting Sort ---\nstart_time = time.perf_counter()\nsorted_counting = counting_sort(data_counting_sort, m_test)\nend_time = time.perf_counter()\ntime_counting = end_time - start_time\nprint(f\"Counting Sort took: {time_counting:.6f} seconds\")\n# print(f\"First 10 elements (Counting Sort): {sorted_counting[:10]}\")\n# print(f\"Last 10 elements (Counting Sort): {sorted_counting[-10:]}\")\n\n\n\nCode Execution Result\n\n\nCounting Sort took: 0.066183 seconds\n\n\n2. Merge Sort\nTime Complexity: O(n log n) Space Complexity: O(n) Stability: Yes In-place: No (requires auxiliary space)\n\n\nimport random\nimport time\n\ndef merge_sort(arr):\n    if len(arr) &lt;= 1:\n        return arr\n\n    mid = len(arr) // 2\n    left_half = arr[:mid]\n    right_half = arr[mid:]\n\n    left_half = merge_sort(left_half)\n    right_half = merge_sort(right_half)\n\n    return merge(left_half, right_half)\n\ndef merge(left, right):\n    merged = []\n    left_idx, right_idx = 0, 0\n\n    while left_idx &lt; len(left) and right_idx &lt; len(right):\n        if left[left_idx] &lt;= right[right_idx]:\n            merged.append(left[left_idx])\n            left_idx += 1\n        else:\n            merged.append(right[right_idx])\n            right_idx += 1\n\n    while left_idx &lt; len(left):\n        merged.append(left[left_idx])\n        left_idx += 1\n    while right_idx &lt; len(right):\n        merged.append(right[right_idx])\n        right_idx += 1\n    return merged\n\n# Data already prepared as `data_merge_sort` from previous block\n\n# --- Run Merge Sort ---\nstart_time = time.perf_counter()\nsorted_merge = merge_sort(data_merge_sort)\nend_time = time.perf_counter()\ntime_merge = end_time - start_time\nprint(f\"Merge Sort took: {time_merge:.6f} seconds\")\n# print(f\"First 10 elements (Merge Sort): {sorted_merge[:10]}\")\n# print(f\"Last 10 elements (Merge Sort): {sorted_merge[-10:]}\")\n\n\n\nCode Execution Result\n\n\nMerge Sort took: 0.449447 seconds\n\n\n3. Quick Sort\nTime Complexity: * Average: O(n log n) * Worst Case: O(n^2) (can be mitigated with good pivot selection) Space Complexity: O(log n) (average for recursion stack), O(n) (worst case) Stability: No In-place: Yes (mostly, due to recursion stack)\n\n\nimport random\nimport time\n\ndef quick_sort(arr):\n    if len(arr) &lt;= 1:\n        return arr\n    pivot = arr[len(arr) // 2]\n    left = [x for x in arr if x &lt; pivot]\n    middle = [x for x in arr if x == pivot]\n    right = [x for x in arr if x &gt; pivot]\n    return quick_sort(left) + middle + quick_sort(right)\n\n# Note: This is a simpler, less optimized Quick Sort implementation\n# that uses O(N) space due to list comprehensions. An in-place\n# version would modify the list directly.\n\n# Data already prepared as `data_quick_sort` from previous block\n\n# --- Run Quick Sort ---\nstart_time = time.perf_counter()\nsorted_quick = quick_sort(data_quick_sort)\nend_time = time.perf_counter()\ntime_quick = end_time - start_time\nprint(f\"Quick Sort took: {time_quick:.6f} seconds\")\n# print(f\"First 10 elements (Quick Sort): {sorted_quick[:10]}\")\n# print(f\"Last 10 elements (Quick Sort): {sorted_quick[-10:]}\")\n\n\n\nCode Execution Result\n\n\nQuick Sort took: 0.291436 seconds\n\n\n\nFinal Comparison of Algorithms\nLet’s summarize the properties and observed performance for a list of n = 100,000 numbers ranging from 0 to m = 50,000.\n\n\n\n\n\n\n\n\n\nFeature\nCounting Sort\nMerge Sort\nQuick Sort (Partition-based)\n\n\n\n\nTime Complexity\nO(n + m)\nO(n log n)\nO(n log n) (Avg.), O(n²) (Worst)\n\n\nSpace Complexity\nO(m)\nO(n)\nO(log n) (Avg.), O(n) (Worst) for recursion stack\n\n\nStability\nYes\nYes\nNo\n\n\nIn-place\nNo\nNo\nYes (for optimal implementation)\n\n\nObserved Time\n0.066 seconds\n0.449 seconds\n0.291 seconds (using simple Python implementation)\n\n\n\n\n\nIs one of them linear?\nYes, Counting Sort is linear.\nIts time complexity is O(n + m). * If m (the range of values) is proportional to n (the number of elements), i.e., m = c * n for some constant c, then the complexity becomes O(n + cn) which simplifies to O(n). In such cases, Counting Sort is a linear-time sorting algorithm.  In our test case, n = 100,000 and m = 50,000. Since m is less than n and of the same order of magnitude, Counting Sort performs exceptionally well, significantly faster than the O(n log n) comparison sorts.\n\n\nConclusion\nFor sorting n numbers from 0 to m:\n\nCounting Sort is the clear winner when m is not significantly larger than n (e.g., m is comparable to n, or even m &lt; n). It leverages the limited range of values to achieve linear time complexity, making it much faster than comparison-based sorts in such scenarios, as demonstrated by the performance difference (0.066s vs ~0.3-0.4s).\nMerge Sort is a solid general-purpose choice, always providing O(n log n) performance and stability, but at the cost of O(n) auxiliary space.\nQuick Sort is generally faster in practice than Merge Sort for average cases due to better cache performance and lower constant factors, despite the theoretical worst-case O(n²) complexity. However, its simple Python implementation used here consumes O(n) space and is not truly in-place.\n\nTherefore, for the specific problem of sorting numbers from 0 to m, Counting Sort is generally the “best” way if m is within a reasonable range, as it provides linear time complexity.\n\n\n\n\n\nThinking with search tool\nSearch grounding is a great way to improve the quality of the model responses by giving it the ability to search for the latest information using Google Search. Check the dedicated guide for more details on that feature.\nIn this case, you are using the adaptive thinkingBudget so the model will dynamically adjust the budget based on the complexity of the request.\nIf you want to experiment with a fixed budget, you can set the thinkingBudget to a specific value (e.g. thinkingBudget=4096).\n\nconst search_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: [\n    `\n    What were the major scientific breakthroughs announced last month? Use your\n    critical thinking and only list what's really incredible and not just an\n    overinfluated title.\n    `,\n  ],\n  config: {\n    tools: [{ googleSearch: {} }],\n    thinkingConfig: {\n      thinkingBudget: 4096,\n      includeThoughts: true,\n    },\n  },\n});\n\nlet search_thoughts = \"\";\nlet search_answers = \"\";\n\nfor (const part of search_response.candidates?.[0]?.content?.parts ?? []) {\n  if (!part.text) continue;\n  if (part.thought) {\n    if (!search_thoughts) tslab.display.markdown(`## **Thought Summary**`);\n    tslab.display.markdown(part.text ?? \"\");\n    search_thoughts += part.text ?? \"\";\n  } else {\n    if (!search_answers) tslab.display.markdown(`## **Response**`);\n    tslab.display.markdown(part.text ?? \"\");\n    search_answers += part.text ?? \"\";\n  }\n}\n\ntslab.display.markdown(`## **Search Results**`);\ntslab.display.html(search_response.candidates?.[0]?.groundingMetadata?.searchEntryPoint?.renderedContent ?? \"\");\n\nThought Summary\n\n\nClarifying “Last Month”\nI’ve established that “last month” refers to May 2025, given the current date. My next step involves conducting focused Google searches using various relevant terms to unearth noteworthy scientific breakthroughs from that period. I’ll then critically assess these findings. I’m prioritizing reputable sources to identify genuinely impactful advancements.\n\n\nNarrowing Down Breakthroughs\nI’ve significantly refined the initial list of potential breakthroughs, prioritizing findings that are truly impactful and novel. The “conversion of lead to gold” has been removed due to potential sensationalism, while the impressive count of Saturn’s moons, and the potential impacts of vitamin D have been deprioritized for now. I’m focusing on the record-breaking redshift galaxy, the fusion milestone, the OCD gene discovery, night-vision lenses, green hydrogen, the new material, the solid-tumor CAR-T success, brain cell repair, and the AI co-scientist. I believe that these are solid candidates for “incredible” breakthroughs worthy of further analysis. Also, I’m watching the pancreatic cancer vaccine and AI-powered cancer detection advancements with great interest, as these present huge potential.\n\n\nResponse\n\n\nLast month, May 2025, saw several significant scientific announcements across various fields, pointing to incredible progress in our understanding of the universe, advancements in medicine, and innovative technological developments.\nIn the realm of space and astronomy: * Most Distant Galaxy Ever Detected: The James Webb Space Telescope (JWST) confirmed MoM-z14 as the most distant galaxy ever detected, with a redshift of 14.44. This places its formation within 280 million years of the Big Bang, offering unprecedented insights into the early universe. * New Exoplanets Around Barnard’s Star: Astronomers confirmed the existence of three new rocky exoplanets, all smaller than Earth, orbiting Barnard’s Star, the closest solitary star to our Sun. This brings the total number of known planets around Barnard’s Star to four, including the previously hinted-at Barnard b.\nIn medicine and health: * Breakthrough in OCD Genetics: For the first time, researchers identified 250 genes linked to Obsessive-Compulsive Disorder (OCD) through a study involving over 2 million people. This discovery represents a significant step towards understanding the genetic underpinnings of the condition. * Night-Vision Contact Lenses: Scientists developed contact lenses that grant “super-vision,” allowing users to perceive beyond the visible light spectrum and detect infrared light even in darkness or with closed eyes. This innovation could potentially replace traditional night-vision goggles. * Repairing Brain Cells: New research demonstrated that brain cells can be repaired, with a drug known as antisense oligonucleotide successfully repairing human neurons in individuals with Timothy’s syndrome, a rare genetic disorder. This breakthrough offers hope for treating other genetic conditions affecting the brain, such as schizophrenia, epilepsy, ADHD, and autism spectrum disorder. * Long-Term Cancer Remission with CAR-T Therapy: A child who received CAR-T cancer therapy for neuroblastoma 18 years ago remains disease-free, suggesting that this personalized cancer treatment may be effective not only for blood cancers but also for solid tumors.\nIn physics and energy: * New Nuclear Fusion Record: The WEST tokamak in France set a new world record for nuclear fusion by maintaining plasma for an impressive 1,337 seconds, a 25% increase over the previous record. This is a crucial step forward in the quest for sustainable fusion energy. * Sunlight-Powered Green Hydrogen Production: Scientists created a prototype reactor capable of harvesting hydrogen fuel using only sunlight and water. This advancement could revolutionize renewable energy by providing a cost-effective method for producing “green hydrogen.” * First 2D Mechanically Interlocked Material: Northwestern University demonstrated the creation of the first two-dimensional (2D) mechanically interlocked material. Described as possessing exceptional flexibility and strength with 100 trillion bonds per square centimeter, this new class of material could have wide-ranging applications.\nIn artificial intelligence: * AI Co-Scientist for Accelerated Discovery: An “AI Co-Scientist” was developed as a collaborative tool to assist scientists in generating novel hypotheses and research proposals. This AI has shown promise in areas such as drug repurposing and proposing new treatment targets, potentially accelerating the pace of biomedical discoveries.\n\n\nSearch Results\n\n\n\n\n  \n    \n      \n      \n      \n      \n    \n    \n      \n      \n      \n      \n      \n    \n    \n  \n  \n    top research announcements May 2025\n    major scientific breakthroughs May 2025\n    significant science discoveries May 2025",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Use Gemini thinking"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_thinking.html#thought-summary-2",
    "href": "quickstarts/Get_started_thinking.html#thought-summary-2",
    "title": "Use Gemini thinking",
    "section": "Thought Summary",
    "text": "Thought Summary",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Use Gemini thinking"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_thinking.html#response-2",
    "href": "quickstarts/Get_started_thinking.html#response-2",
    "title": "Use Gemini thinking",
    "section": "Response",
    "text": "Response",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Use Gemini thinking"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_thinking.html#search-results",
    "href": "quickstarts/Get_started_thinking.html#search-results",
    "title": "Use Gemini thinking",
    "section": "Search Results",
    "text": "Search Results",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Use Gemini thinking"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_thinking.html#next-steps",
    "href": "quickstarts/Get_started_thinking.html#next-steps",
    "title": "Use Gemini thinking",
    "section": "Next Steps",
    "text": "Next Steps\nTry Gemini 2.5 Pro Experimental in Google AI Studio, and learn more about Prompting for thinking models.\nFor more examples of the Gemini capabilities, check the other Cookbook examples. You’ll learn how to use the Live API, juggle with multiple tools or use Gemini spatial understanding abilities.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Use Gemini thinking"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_LiveAPI.html",
    "href": "quickstarts/Get_started_LiveAPI.html",
    "title": "Multimodal Live API - Quickstart",
    "section": "",
    "text": "Preview: The Live API is in preview.\nThis notebook demonstrates simple usage of the Gemini Multimodal Live API. For an overview of new capabilities refer to the Gemini Live API docs.\nThis notebook implements a simple turn-based chat where you send messages as text, and the model replies with audio. The API is capable of much more than that. The goal here is to demonstrate with simple code.\nSome features of the API are not working in Colab, to try them it is recommended to have a look at this Python script and run it locally.\nIf you aren’t looking for code, and just want to try multimedia streaming use Live API in Google AI Studio.\nThe Next steps section at the end of this tutorial provides links to additional resources.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Multimodal Live API - Quickstart"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_LiveAPI.html#setup",
    "href": "quickstarts/Get_started_LiveAPI.html#setup",
    "title": "Multimodal Live API - Quickstart",
    "section": "Setup",
    "text": "Setup\n\nInstall the Google GenAI SDK\nInstall the Google GenAI SDK from npm.\n$ npm install @google/genai\n\n\nSetup your API key\nYou can create your API key using Google AI Studio with a single click.\nRemember to treat your API key like a password. Don’t accidentally save it in a notebook or source file you later commit to GitHub. In this notebook we will be storing the API key in a .env file. You can also set it as an environment variable or use a secret manager.\nHere’s how to set it up in a .env file:\n$ touch .env\n$ echo \"GEMINI_API_KEY=&lt;YOUR_API_KEY&gt;\" &gt;&gt; .env\n\n\n\n\n\n\nTip\n\n\n\nAnother option is to set the API key as an environment variable. You can do this in your terminal with the following command:\n$ export GEMINI_API_KEY=\"&lt;YOUR_API_KEY&gt;\"\n\n\n\n\nLoad the API key\nTo load the API key from the .env file, we will use the dotenv package. This package loads environment variables from a .env file into process.env.\n$ npm install dotenv\nThen, we can load the API key in our code:\n\nconst dotenv = require(\"dotenv\") as typeof import(\"dotenv\");\n\ndotenv.config({\n  path: \"../.env\",\n});\n\nconst GEMINI_API_KEY = process.env.GEMINI_API_KEY ?? \"\";\nif (!GEMINI_API_KEY) {\n  throw new Error(\"GEMINI_API_KEY is not set in the environment variables\");\n}\nconsole.log(\"GEMINI_API_KEY is set in the environment variables\");\n\nGEMINI_API_KEY is set in the environment variables\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn our particular case the .env is is one directory up from the notebook, hence we need to use ../ to go up one directory. If the .env file is in the same directory as the notebook, you can omit it altogether.\n│\n├── .env\n└── quickstarts\n    └── Get_started_LiveAPI.ipynb",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Multimodal Live API - Quickstart"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_LiveAPI.html#initialize-sdk-client",
    "href": "quickstarts/Get_started_LiveAPI.html#initialize-sdk-client",
    "title": "Multimodal Live API - Quickstart",
    "section": "Initialize SDK Client",
    "text": "Initialize SDK Client\nWith the new SDK, now you only need to initialize a client with you API key (or OAuth if using Vertex AI). The model is now set in each call.\n\nconst google = require(\"@google/genai\") as typeof import(\"@google/genai\");\n\nconst ai = new google.GoogleGenAI({ apiKey: GEMINI_API_KEY });",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Multimodal Live API - Quickstart"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_LiveAPI.html#text-to-text",
    "href": "quickstarts/Get_started_LiveAPI.html#text-to-text",
    "title": "Multimodal Live API - Quickstart",
    "section": "Text to Text",
    "text": "Text to Text\nThe simplest way to use the Live API is as a text-to-text chat interface, but it can do a lot more than this.\n\nconst tslab = require(\"tslab\") as typeof import(\"tslab\");\n\nconst MODEL_ID = \"gemini-2.0-flash-live-001\";\n\nThe Live API uses a streaming model over a WebSocket connection. When you interact with the API, a persistent connection is created. Your input (audio, video, or text) is streamed continuously to the model, and the model’s response (text or audio) is streamed back in real-time over the same connection. Here we use a responseQueue to handle the streaming responses and determine when the server has finished sending the response.\n\nimport { LiveServerMessage, Modality } from \"@google/genai\";\n\nasync function text_to_text() {\n  const responseQueue: LiveServerMessage[] = [];\n  const session = await ai.live.connect({\n    model: MODEL_ID,\n    callbacks: {\n      onopen: function () {\n        console.debug(\"Opened\");\n      },\n      onmessage: function (message) {\n        responseQueue.push(message);\n      },\n      onerror: function (e) {\n        console.debug(\"Error:\", e.message);\n      },\n      onclose: function (e) {\n        console.debug(\"Close:\", e.reason);\n      },\n    },\n    config: { responseModalities: [Modality.TEXT] },\n  });\n  const message = \"Hello? Gemini are you there?\";\n  session.sendClientContent({\n    turns: message,\n    turnComplete: true,\n  });\n  console.debug(\"Sent message:\", message);\n  let done = false;\n  while (!done) {\n    if (responseQueue.length &gt; 0) {\n      const response = responseQueue.shift();\n      if (response?.text) {\n        console.debug(\"Received response:\", response.text);\n      } else if (response?.data) {\n        console.debug(\"Received data:\", response.data);\n      }\n      if (response?.serverContent?.turnComplete) {\n        done = true;\n      }\n    } else {\n      await new Promise((resolve) =&gt; setTimeout(resolve, 100));\n    }\n  }\n  session.close();\n  console.debug(\"Session closed\");\n}\n\nawait text_to_text();\n\nOpened\nSent message: Hello? Gemini are you there?\nReceived response: Yes, I am\nReceived response:  here! How can I help you today?\n\nSession closed\nClose:",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Multimodal Live API - Quickstart"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_LiveAPI.html#text-to-audio",
    "href": "quickstarts/Get_started_LiveAPI.html#text-to-audio",
    "title": "Multimodal Live API - Quickstart",
    "section": "Text to audio",
    "text": "Text to audio\nThe simplest way to playback the audio in Colab, is to write it out to a .wav file. So here is a simple wave file writer:\n\nconst fs = require(\"fs\") as typeof import(\"fs\");\nconst path = require(\"path\") as typeof import(\"path\");\nconst wave = require(\"wavefile\") as typeof import(\"wavefile\");\n\nfunction saveAudioToFile(audioData: Int16Array, filePath: string) {\n  fs.mkdirSync(path.dirname(filePath), { recursive: true });\n  const wav = new wave.WaveFile();\n  wav.fromScratch(1, 24000, \"16\", audioData);\n  fs.writeFileSync(filePath, wav.toBuffer());\n  console.debug(`Audio saved to ${filePath}`);\n}\n\n\nimport { LiveServerMessage, Modality } from \"@google/genai\";\n\nasync function text_to_audio() {\n  const responseQueue: LiveServerMessage[] = [];\n  const session = await ai.live.connect({\n    model: MODEL_ID,\n    callbacks: {\n      onopen: function () {\n        console.debug(\"Opened\");\n      },\n      onmessage: function (message) {\n        responseQueue.push(message);\n      },\n      onerror: function (e) {\n        console.debug(\"Error:\", e.message);\n      },\n      onclose: function (e) {\n        console.debug(\"Close:\", e.reason);\n      },\n    },\n    config: { responseModalities: [Modality.AUDIO] },\n  });\n  const message = \"Hello? Gemini are you there?\";\n  session.sendClientContent({\n    turns: message,\n    turnComplete: true,\n  });\n  console.debug(\"Sent message:\", message);\n  let done = false;\n  const chunks: LiveServerMessage[] = [];\n  while (!done) {\n    if (responseQueue.length &gt; 0) {\n      const response = responseQueue.shift();\n      if (response) {\n        chunks.push(response);\n      }\n      if (response?.serverContent?.turnComplete) {\n        done = true;\n        console.debug(\"Received complete response\");\n      }\n    } else {\n      await new Promise((resolve) =&gt; setTimeout(resolve, 100));\n    }\n  }\n  const audioData = chunks.reduce&lt;number[]&gt;((acc, message) =&gt; {\n    if (message.data) {\n      const audioBuffer = Buffer.from(message.data, \"base64\");\n      const intArray = new Int16Array(\n        audioBuffer.buffer,\n        audioBuffer.byteOffset,\n        audioBuffer.length / Int16Array.BYTES_PER_ELEMENT\n      );\n      return acc.concat(Array.from(intArray));\n    }\n    return acc;\n  }, []);\n  const audioFilePath = path.join(\"../assets/live\", \"text_to_audio_response.wav\");\n  saveAudioToFile(new Int16Array(audioData), audioFilePath);\n  session.close();\n  console.debug(\"Session closed\");\n}\n\nawait text_to_audio();\ntslab.display.html(`\n  &lt;h3&gt;Text to Audio Response&lt;/h3&gt;\n  &lt;audio controls&gt;\n    &lt;source src=\"../assets/live/text_to_audio_response.wav\" type=\"audio/wav\"&gt;\n    Your browser does not support the audio element.\n  &lt;/audio&gt;\n  &lt;/audio&gt;\n`);\n\nOpened\nSent message: Hello? Gemini are you there?\nReceived complete response\nAudio saved to ../assets/live/text_to_audio_response.wav\nSession closed\n\n\n\nText to Audio Response\n\n  \n  Your browser does not support the audio element.\n\n\n\n\nClose:",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Multimodal Live API - Quickstart"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_LiveAPI.html#towards-async-tasks",
    "href": "quickstarts/Get_started_LiveAPI.html#towards-async-tasks",
    "title": "Multimodal Live API - Quickstart",
    "section": "Towards Async Tasks",
    "text": "Towards Async Tasks\nThe real power of the Live API is that it’s real time, and interruptable. You can’t get that full power in a simple sequence of steps. To really use the functionality you will move the send and recieve operations (and others) into their own async tasks.\nBecause of the limitations of Colab this tutorial doesn’t totally implement the interactive async tasks, but it does implement the next step in that direction:\n\nIt separates the send and receive, but still runs them sequentially.\nIn the next tutorial you’ll run these in separate async tasks.\n\n\nimport { GoogleGenAI, LiveServerMessage, Modality, Session } from \"@google/genai\";\n\nclass AudioLooper {\n  private session: Session;\n  private turnIndex = 0;\n  private responseQueue: LiveServerMessage[] = [];\n\n  constructor(\n    private ai: GoogleGenAI,\n    private modelId: string\n  ) {}\n\n  async start() {\n    this.session = await this.ai.live.connect({\n      model: this.modelId,\n      callbacks: {\n        onopen: () =&gt; {\n          console.debug(\"Opened\");\n        },\n        onmessage: (message) =&gt; this.responseQueue.push(message),\n        onerror: (e) =&gt; {\n          console.debug(\"Error:\", e.message);\n        },\n        onclose: (e) =&gt; {\n          console.debug(\"Close:\", e.reason);\n        },\n      },\n      config: { responseModalities: [Modality.AUDIO] },\n    });\n  }\n\n  send(message: string) {\n    this.session.sendClientContent({\n      turns: message,\n      turnComplete: true,\n    });\n    console.debug(\"Sent message:\", message);\n  }\n\n  async receive() {\n    let done = false;\n    const audioChunks: number[] = [];\n    while (!done) {\n      if (this.responseQueue.length &gt; 0) {\n        const response = this.responseQueue.shift();\n        if (response?.data) {\n          const audioBuffer = Buffer.from(response.data, \"base64\");\n          const intArray = new Int16Array(\n            audioBuffer.buffer,\n            audioBuffer.byteOffset,\n            audioBuffer.length / Int16Array.BYTES_PER_ELEMENT\n          );\n          audioChunks.push(...Array.from(intArray));\n        }\n        if (response?.serverContent?.turnComplete) {\n          done = true;\n          console.debug(\"Received complete response\");\n        }\n      } else {\n        await new Promise((resolve) =&gt; setTimeout(resolve, 100));\n      }\n    }\n    const audioFilePath = path.join(\"../assets/live\", `audio_response_${this.turnIndex++}.wav`);\n    saveAudioToFile(new Int16Array(audioChunks), audioFilePath);\n    tslab.display.html(`\n      &lt;h3&gt;Audio Response ${this.turnIndex}&lt;/h3&gt;\n      &lt;audio controls&gt;\n          &lt;source src=\"../assets/live/audio_response_${this.turnIndex - 1}.wav\" type=\"audio/wav\"&gt;\n          Your browser does not support the audio element.\n      &lt;/audio&gt;\n    `);\n  }\n\n  stop() {\n    this.session.close();\n    console.debug(\"Session closed\");\n  }\n}\n\nasync function asyncAudioLooper() {\n  const audioLooper = new AudioLooper(ai, MODEL_ID);\n  await audioLooper.start();\n\n  // Simulate sending messages\n  const messages = [\"Hello? Gemini are you there?\", \"Can you tell me a joke?\", \"What is the weather like today?\"];\n\n  for (const message of messages) {\n    audioLooper.send(message);\n    await audioLooper.receive();\n  }\n\n  audioLooper.stop();\n}\n\nawait asyncAudioLooper();\n\nOpened\nSent message: Hello? Gemini are you there?\nReceived complete response\nAudio saved to ../assets/live/audio_response_0.wav\n\n\n\nAudio Response 1\n\n    \n    Your browser does not support the audio element.\n\n\n\nSent message: Can you tell me a joke?\nReceived complete response\nAudio saved to ../assets/live/audio_response_1.wav\n\n\n\nAudio Response 2\n\n    \n    Your browser does not support the audio element.\n\n\n\nSent message: What is the weather like today?\nReceived complete response\nAudio saved to ../assets/live/audio_response_2.wav\n\n\n\nAudio Response 3\n\n    \n    Your browser does not support the audio element.\n\n\n\nSession closed\nClose: \n\n\nThe above code is divided into several sections:\n\nstart: Initializes the client and sets up the WebSocket connection.\nsend: Sends a message to the model.\nreceive: Receives the model’s response and collects the audio chunks in a loop and writes them to wav file. It breaks when the model indicates it has finished sending the response.\nasyncAudioLooper: This is the main driver function that brings everything together. It initializes the client, starts the WebSocket connection, and then enters a loop where it sends messages and receives responses.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Multimodal Live API - Quickstart"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_LiveAPI.html#working-with-resumable-sessions",
    "href": "quickstarts/Get_started_LiveAPI.html#working-with-resumable-sessions",
    "title": "Multimodal Live API - Quickstart",
    "section": "Working with resumable sessions",
    "text": "Working with resumable sessions\nSession resumption allows you to return to a previous interaction with the Live API by sending the last session handle you got from the previous session.\nWhen you set your session to be resumable, the session information keeps stored on the Live API for up to 24 hours. In this time window, you can resume the conversation and refer to previous information you have shared with the model.\n\nimport { LiveServerMessage, Modality } from \"@google/genai\";\n\nlet HANDLE: string | undefined = undefined;\n\nasync function resumable_session(\n  previousSessionHandle?: string,\n  messages: string[] = [\"Hello\", \"What is the capital of Brazil?\"]\n) {\n  const responseQueue: LiveServerMessage[] = [];\n\n  async function waitMessage(): Promise&lt;LiveServerMessage&gt; {\n    let done = false;\n    let message: LiveServerMessage | undefined = undefined;\n    while (!done) {\n      message = responseQueue.shift();\n      if (message) {\n        done = true;\n      } else {\n        await new Promise((resolve) =&gt; setTimeout(resolve, 100));\n      }\n    }\n    return message!;\n  }\n\n  console.debug(\"Connecting to the service with handle %s...\", previousSessionHandle);\n  const session = await ai.live.connect({\n    model: MODEL_ID,\n    callbacks: {\n      onopen: function () {\n        console.debug(\"Opened\");\n      },\n      onmessage: function (message) {\n        responseQueue.push(message);\n        console.debug(\"Received message:\", JSON.stringify(message));\n        if (message.sessionResumptionUpdate?.resumable && message.sessionResumptionUpdate.newHandle) {\n          HANDLE = message.sessionResumptionUpdate.newHandle;\n        }\n      },\n      onerror: function (e) {\n        console.debug(\"Error:\", e.message);\n      },\n      onclose: function (e) {\n        console.debug(\"Close:\", e.reason);\n      },\n    },\n    config: {\n      responseModalities: [Modality.TEXT],\n      sessionResumption: { handle: previousSessionHandle },\n    },\n  });\n\n  for (const message of messages) {\n    console.debug(\"Sending message:\", message);\n    session.sendClientContent({\n      turns: message,\n      turnComplete: true,\n    });\n    let done = false;\n    while (!done) {\n      const response = await waitMessage();\n      if (response.serverContent?.turnComplete) {\n        done = true;\n      }\n    }\n  }\n\n  // small delay for session resumption update to arrive\n  await new Promise((resolve) =&gt; setTimeout(resolve, 3000));\n\n  session.close();\n}\n\nawait resumable_session();\n\nConnecting to the service with handle undefined...\nOpened\nSending message: Hello\nReceived message: {\"setupComplete\":{}}\nReceived message: {\"sessionResumptionUpdate\":{}}\nReceived message: {\"serverContent\":{\"modelTurn\":{\"parts\":[{\"text\":\"Hello there! How\"}]}}}\nReceived message: {\"serverContent\":{\"modelTurn\":{\"parts\":[{\"text\":\" can I help you today?\\n\"}]}}}\nReceived message: {\"serverContent\":{\"generationComplete\":true}}\nReceived message: {\"serverContent\":{\"turnComplete\":true},\"usageMetadata\":{\"promptTokenCount\":9,\"responseTokenCount\":11,\"totalTokenCount\":20,\"promptTokensDetails\":[{\"modality\":\"TEXT\",\"tokenCount\":9}],\"responseTokensDetails\":[{\"modality\":\"TEXT\",\"tokenCount\":11}]}}\nSending message: What is the capital of Brazil?\nReceived message: {\"sessionResumptionUpdate\":{\"newHandle\":\"CihqdTFxaG1ua2g2aTkweWtiNzB5Ymdzc3V0bW16eDE2ZGkxaXR2d2dt\",\"resumable\":true}}\nReceived message: {\"sessionResumptionUpdate\":{}}\nReceived message: {\"serverContent\":{\"modelTurn\":{\"parts\":[{\"text\":\"The capital of Brazil\"}]}}}\nReceived message: {\"serverContent\":{\"modelTurn\":{\"parts\":[{\"text\":\" is **Brasília**.\\n\"}]}}}\nReceived message: {\"serverContent\":{\"generationComplete\":true}}\nReceived message: {\"serverContent\":{\"turnComplete\":true},\"usageMetadata\":{\"promptTokenCount\":37,\"responseTokenCount\":10,\"totalTokenCount\":47,\"promptTokensDetails\":[{\"modality\":\"TEXT\",\"tokenCount\":37}],\"responseTokensDetails\":[{\"modality\":\"TEXT\",\"tokenCount\":10}]}}\nReceived message: {\"sessionResumptionUpdate\":{\"newHandle\":\"CihrNGZyMjh4dXY3cXFkYzVmMjR5cnlmZ2w5bnBvNTRhcmoxNW1lN2Fi\",\"resumable\":true}}\nClose: \n\n\nWith the session resumption you have the session handle to refer to your previous sessions. In this example, the handle is saved at the handle variable as below:\n\nconsole.debug(\"Session handle:\", HANDLE);\n\nSession handle: CihrNGZyMjh4dXY3cXFkYzVmMjR5cnlmZ2w5bnBvNTRhcmoxNW1lN2Fi\n\n\nNow you can start a new Live API session, but this time pointing to a handle from a previous session. Also, to test you could gather information from the previous session, you will ask the model what was the second question you asked before (in this example, it was “what is the capital of Brazil?”). You can see the Live API recovering that information:\n\nawait resumable_session(HANDLE, [\"what was the last question I asked?\"]);\n\nConnecting to the service with handle CihrNGZyMjh4dXY3cXFkYzVmMjR5cnlmZ2w5bnBvNTRhcmoxNW1lN2Fi...\nOpened\nSending message: what was the last question I asked?\nReceived message: {\"setupComplete\":{}}\nReceived message: {\"sessionResumptionUpdate\":{}}\nReceived message: {\"serverContent\":{\"modelTurn\":{\"parts\":[{\"text\":\"The\"}]}}}\nReceived message: {\"serverContent\":{\"modelTurn\":{\"parts\":[{\"text\":\" last question you asked was: \\\"What is the capital of Brazil?\\\"\\n\"}]}}}\nReceived message: {\"serverContent\":{\"generationComplete\":true}}\nReceived message: {\"serverContent\":{\"turnComplete\":true},\"usageMetadata\":{\"promptTokenCount\":65,\"responseTokenCount\":16,\"totalTokenCount\":81,\"promptTokensDetails\":[{\"modality\":\"TEXT\",\"tokenCount\":65}],\"responseTokensDetails\":[{\"modality\":\"TEXT\",\"tokenCount\":16}]}}\nReceived message: {\"sessionResumptionUpdate\":{\"newHandle\":\"CihmcW04ZzVnZnZwczU2ZnkwN2h1NHpmajFxZmgwcmhieTZ3Zmo3OWt6\",\"resumable\":true}}\nClose:",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Multimodal Live API - Quickstart"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_LiveAPI.html#next-steps",
    "href": "quickstarts/Get_started_LiveAPI.html#next-steps",
    "title": "Multimodal Live API - Quickstart",
    "section": "Next steps",
    "text": "Next steps\n\nThis tutorial just shows basic usage of the Live API, using the Python GenAI SDK.\n\nIf you aren’t looking for code, and just want to try multimedia streaming use Live API in Google AI Studio.\nIf you want to see how to setup streaming interruptible audio and video using the Live API see the Audio and Video input Tutorial.\nIf you’re interested in the low level details of using the websockets directly, see the websocket version of this tutorial.\nTry the Tool use in the live API tutorial for an walkthrough of Gemini-2’s new tool use capabilities.\nThere is a Streaming audio in Colab example, but this is more of a demo, it’s not optimized for readability.\nOther nice Gemini 2.0 examples can also be found in the Cookbook’s 2.0 directory, in particular the video understanding and the spatial understanding ones.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Multimodal Live API - Quickstart"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_LiveAPI_tools.html",
    "href": "quickstarts/Get_started_LiveAPI_tools.html",
    "title": "Gemini 2.0 - Multimodal live API: Tool use",
    "section": "",
    "text": "This notebook provides examples of how to use tools with the multimodal live API with Gemini 2.0.\nThe API provides Google Search, Code Execution and Function Calling tools. The earlier Gemini models supported versions of these tools. The biggest change with Gemini 2 (in the Live API) is that, basically, all the tools are handled by Code Execution. With that change, you can use multiple tools in a single API call, and the model can use multiple tools in a single code execution block.\nThis tutorial assumes you are familiar with the Live API, as described in the this tutorial.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini 2.0 - Multimodal live API: Tool use"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_LiveAPI_tools.html#setup",
    "href": "quickstarts/Get_started_LiveAPI_tools.html#setup",
    "title": "Gemini 2.0 - Multimodal live API: Tool use",
    "section": "Setup",
    "text": "Setup\n\nInstall the Google GenAI SDK\nInstall the Google GenAI SDK from npm.\n$ npm install @google/genai\n\n\nSetup your API key\nYou can create your API key using Google AI Studio with a single click.\nRemember to treat your API key like a password. Don’t accidentally save it in a notebook or source file you later commit to GitHub. In this notebook we will be storing the API key in a .env file. You can also set it as an environment variable or use a secret manager.\nHere’s how to set it up in a .env file:\n$ touch .env\n$ echo \"GEMINI_API_KEY=&lt;YOUR_API_KEY&gt;\" &gt;&gt; .env\n\n\n\n\n\n\nTip\n\n\n\nAnother option is to set the API key as an environment variable. You can do this in your terminal with the following command:\n$ export GEMINI_API_KEY=\"&lt;YOUR_API_KEY&gt;\"\n\n\n\n\nLoad the API key\nTo load the API key from the .env file, we will use the dotenv package. This package loads environment variables from a .env file into process.env.\n$ npm install dotenv\nThen, we can load the API key in our code:\n\nconst dotenv = require(\"dotenv\") as typeof import(\"dotenv\");\n\ndotenv.config({\n  path: \"../.env\",\n});\n\nconst GEMINI_API_KEY = process.env.GEMINI_API_KEY ?? \"\";\nif (!GEMINI_API_KEY) {\n  throw new Error(\"GEMINI_API_KEY is not set in the environment variables\");\n}\nconsole.log(\"GEMINI_API_KEY is set in the environment variables\");\n\nGEMINI_API_KEY is set in the environment variables\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn our particular case the .env is is one directory up from the notebook, hence we need to use ../ to go up one directory. If the .env file is in the same directory as the notebook, you can omit it altogether.\n│\n├── .env\n└── quickstarts\n    └── Get_started_LiveAPI_tools.ipynb\n\n\n\n\nInitialize SDK Client\nWith the new SDK, now you only need to initialize a client with you API key (or OAuth if using Vertex AI). The model is now set in each call.\n\nconst google = require(\"@google/genai\") as typeof import(\"@google/genai\");\n\nconst ai = new google.GoogleGenAI({ apiKey: GEMINI_API_KEY });\n\n\n\nSelect a model\nMultimodal Live API are a new capability introduced with the Gemini 2.0 model. It won’t work with previous generation models.\n\nconst tslab = require(\"tslab\") as typeof import(\"tslab\");\n\nconst MODEL_ID = \"gemini-2.0-flash-live-001\";\n\n\n\nUtilites\nYou’re going to use the Live API’s audio output, the easiest way hear it in Colab is to write the PCM data out as a WAV file:\n\nconst fs = require(\"fs\") as typeof import(\"fs\");\nconst path = require(\"path\") as typeof import(\"path\");\nconst wave = require(\"wavefile\") as typeof import(\"wavefile\");\n\nfunction saveAudioToFile(audioData: Int16Array, filePath: string) {\n  fs.mkdirSync(path.dirname(filePath), { recursive: true });\n  const wav = new wave.WaveFile();\n  wav.fromScratch(1, 24000, \"16\", audioData);\n  fs.writeFileSync(filePath, wav.toBuffer());\n  console.debug(`Audio saved to ${filePath}`);\n}",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini 2.0 - Multimodal live API: Tool use"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_LiveAPI_tools.html#get-started",
    "href": "quickstarts/Get_started_LiveAPI_tools.html#get-started",
    "title": "Gemini 2.0 - Multimodal live API: Tool use",
    "section": "Get Started",
    "text": "Get Started\nMost of the Live API setup will be similar to the starter tutorial. Since this tutorial doesn’t focus on the realtime interactivity of the API, the code has been simplified: This code uses the Live API, but it only sends a single text prompt, and listens for a single turn of replies.\nYou can set modality=“AUDIO” on any of the examples to get the spoken version of the output.\n\nimport { FunctionResponse, LiveServerContent, LiveServerToolCall, Modality, Session, Tool } from \"@google/genai\";\n\nfunction handleServerContent(content: LiveServerContent) {\n  if (content.modelTurn) {\n    for (const turn of content.modelTurn.parts ?? []) {\n      if (turn.executableCode) {\n        tslab.display.markdown(\"-------------------------------\");\n        tslab.display.markdown(`\\`\\`\\`python\\n${turn.executableCode.code}\\n\\`\\`\\``);\n        tslab.display.markdown(\"-------------------------------\");\n      }\n      if (turn.codeExecutionResult) {\n        tslab.display.markdown(\"-------------------------------\");\n        tslab.display.markdown(`\\`\\`\\`\\n${turn.codeExecutionResult.output}\\n\\`\\`\\``);\n        tslab.display.markdown(\"-------------------------------\");\n      }\n    }\n  }\n  if (content.groundingMetadata) {\n    tslab.display.html(content.groundingMetadata.searchEntryPoint?.renderedContent ?? \"\");\n  }\n}\n\nfunction handleToolCall(session: Session, toolCall: LiveServerToolCall) {\n  const responses: FunctionResponse[] = [];\n  for (const fc of toolCall.functionCalls ?? []) {\n    responses.push({\n      id: fc.id,\n      name: fc.name,\n      response: {\n        result: \"ok\",\n      },\n    });\n  }\n  console.log(\"Tool call responses:\", JSON.stringify(responses, null, 2));\n  session.sendToolResponse({\n    functionResponses: responses,\n  });\n}\n\nasync function run(prompt: string, modality: Modality = Modality.TEXT, tools: Tool[] = []) {\n  const audioData: number[] = [];\n  const audioFileName = `audio-${Date.now()}.wav`;\n  let completed = false;\n  const session = await ai.live.connect({\n    model: MODEL_ID,\n    callbacks: {\n      onopen: () =&gt; {\n        console.log(\"Connection opened\");\n      },\n      onclose: () =&gt; {\n        console.log(\"Connection closed\");\n      },\n      onerror: (error) =&gt; {\n        console.error(\"Error:\", error.message);\n      },\n      onmessage: (message) =&gt; {\n        if (message.text) {\n          tslab.display.markdown(message.text);\n          return;\n        }\n        if (message.data) {\n          const audioBuffer = Buffer.from(message.data, \"base64\");\n          const audio = new Int16Array(\n            audioBuffer.buffer,\n            audioBuffer.byteOffset,\n            audioBuffer.length / Int16Array.BYTES_PER_ELEMENT\n          );\n          audioData.push(...audio);\n          return;\n        }\n        if (message.serverContent) {\n          handleServerContent(message.serverContent);\n          if (message.serverContent.turnComplete) {\n            completed = true;\n          }\n          return;\n        }\n        if (message.toolCall) {\n          handleToolCall(session, message.toolCall);\n          completed = true;\n          return;\n        }\n      },\n    },\n    config: {\n      tools: tools,\n      responseModalities: [modality],\n    },\n  });\n  console.log(\"Prompt: \", prompt);\n  session.sendClientContent({\n    turns: [prompt],\n    turnComplete: true,\n  });\n  // eslint-disable-next-line @typescript-eslint/no-unnecessary-condition\n  while (!completed) {\n    await new Promise((resolve) =&gt; setTimeout(resolve, 100));\n  }\n  if (audioData.length &gt; 0) {\n    saveAudioToFile(new Int16Array(audioData), path.join(\"audio\", audioFileName));\n    console.log(`Audio saved to ${audioFileName}`);\n    tslab.display.html(\n      `&lt;audio controls&gt;&lt;source src=\"${audioFileName}\" type=\"audio/wav\"&gt;Your browser does not support the audio element.&lt;/audio&gt;`\n    );\n  }\n  console.log(\"Session completed\");\n  session.close();\n}\n\nSince this tutorial demonstrates several tools, you’ll need more code to handle the different types of objects it returns.\n\nThe codeExecution tool can return executableCode and codeExecutionResult parts.\nThe googleSearch tool may attach a groundingMetadata object.\nFinally, with the functionDeclations tool, the API may return toolCall objects.To keep this code minimal, the toolCall handler just replies to every function call with a response of \"ok\".\n\n\nawait run(\"Hello?\");\n\nConnection opened\nPrompt:  Hello?\n\n\nHello! How can\n\n\nI help you today?\n\n\nSession completed\nConnection closed",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini 2.0 - Multimodal live API: Tool use"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_LiveAPI_tools.html#simple-function-call",
    "href": "quickstarts/Get_started_LiveAPI_tools.html#simple-function-call",
    "title": "Gemini 2.0 - Multimodal live API: Tool use",
    "section": "Simple Function Call",
    "text": "Simple Function Call\nThe function calling feature of the API Can handle a wide variety of functions. Support in the SDK is still under construction. So keep this simple just send a minimal function definition: Just the function’s name.\nNote that in the live API function calls are independent of the chat turns. The conversation can continue while a function call is being processed.\n\nimport { FunctionDeclaration, Tool } from \"@google/genai\";\n\nconst turn_on_the_lights = {\n  name: \"turn_on_the_lights\",\n  description: \"Turn on the lights in the room\",\n} satisfies FunctionDeclaration;\nconst turn_off_the_lights: FunctionDeclaration = {\n  name: \"turn_off_the_lights\",\n  description: \"Turn off the lights in the room\",\n} satisfies FunctionDeclaration;\nconst function_call_tools: Tool[] = [{ functionDeclarations: [turn_on_the_lights, turn_off_the_lights] }];\n\n// temporarily make console.warn a no-op to avoid warnings in the output (non-text part in GenerateContentResponse caused by accessing .text)\n// https://github.com/googleapis/js-genai/blob/d82aba244bdb804b063ef8a983b2916c00b901d2/src/types.ts#L2005\n// copy the original console.warn function to restore it later\nconst warn_fn = console.warn;\n// eslint-disable-next-line @typescript-eslint/no-empty-function, no-empty-function\nconsole.warn = function () {};\n\nawait run(\"Turn on the lights\", google.Modality.TEXT, function_call_tools);\n// restore console.warn later\n// console.warn = warn_fn;\n\nConnection opened\nPrompt:  Turn on the lights\n\n\n\n\n\nprint(default_api.turn_on_the_lights())\n\n\n\n\n\nTool call responses: [\n  {\n    \"id\": \"function-call-16720258795371319743\",\n    \"name\": \"turn_on_the_lights\",\n    \"response\": {\n      \"result\": \"ok\"\n    }\n  }\n]\n\n\n\n\n\n{'result': 'ok'}\n\n\n\n\n\n\nOK\n\n\n, I’ve turned on the lights.\n\n\nSession completed\nConnection closed",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini 2.0 - Multimodal live API: Tool use"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_LiveAPI_tools.html#code-execution",
    "href": "quickstarts/Get_started_LiveAPI_tools.html#code-execution",
    "title": "Gemini 2.0 - Multimodal live API: Tool use",
    "section": "Code Execution",
    "text": "Code Execution\nThe codeExecution lets the model write and run python code. Try it on a math problem the model can’t solve from memory:\n\nawait run(\"Can you compute the largest prime palindrome under 100000.\", google.Modality.TEXT, [{ codeExecution: {} }]);\n\nConnection opened\nPrompt:  Can you compute the largest prime palindrome under 100000.\n\n\nOkay\n\n\n, I can help you with that. Here’s my plan:\n1\n\n\n. Generate Palindromes: Create a list of all palindromes under 100000. 2. Check for Primality:\n\n\nIterate through the palindromes and check if each one is prime. 3. Find the Largest: Keep track of the largest prime palindrome found so far.\n\n\nHere’s the code to do that:\n\n\n\n\n\ndef is_palindrome(n):\n  \"\"\"Checks if a number is a palindrome.\"\"\"\n  return str(n) == str(n)[::-1]\n\n\ndef is_prime(n):\n  \"\"\"Checks if a number is prime.\"\"\"\n  if n &lt; 2:\n    return False\n  for i in range(2, int(n**0.5) + 1):\n    if n % i == 0:\n      return False\n  return True\n\n\nlargest_prime_palindrome = 0\nfor i in range(100000):\n  if is_palindrome(i) and is_prime(i):\n    largest_prime_palindrome = i\n\nprint(largest_prime_palindrome)\n\n\n\n\n\n\n\n\n98689\n\n\n\n\n\n\nThe largest prime palindrome\n\n\nunder 100000 is 98689.\n\n\n\n\n\nSession completed\nConnection closed",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini 2.0 - Multimodal live API: Tool use"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_LiveAPI_tools.html#compositional-function-calling",
    "href": "quickstarts/Get_started_LiveAPI_tools.html#compositional-function-calling",
    "title": "Gemini 2.0 - Multimodal live API: Tool use",
    "section": "Compositional Function Calling",
    "text": "Compositional Function Calling\nCompositional function calling refers to the ability to combine user defined functions with the codeExecution tool. The model will write them into larger blocks of code, and then pause execution while it waits for you to send back responses for each call.\n\nawait run(\"Can you turn on the lights wait 10s and then turn them off?\", google.Modality.TEXT, [\n  ...function_call_tools,\n  { codeExecution: {} },\n]);\n\nConnection opened\nPrompt:  Can you turn on the lights wait 10s and then turn them off?\n\n\n\n\n\nimport time\n\ndefault_api.turn_on_the_lights()\ntime.sleep(10)\ndefault_api.turn_off_the_lights()\n\n\n\n\n\nTool call responses: [\n  {\n    \"id\": \"function-call-448821244251533960\",\n    \"name\": \"turn_on_the_lights\",\n    \"response\": {\n      \"result\": \"ok\"\n    }\n  }\n]\nSession completed\nConnection closed",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini 2.0 - Multimodal live API: Tool use"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_LiveAPI_tools.html#google-search",
    "href": "quickstarts/Get_started_LiveAPI_tools.html#google-search",
    "title": "Gemini 2.0 - Multimodal live API: Tool use",
    "section": "Google search",
    "text": "Google search\nThe googleSearch tool lets the model conduct google searches. For example, try asking it about events that are too recent to be in the training data.\nThe search will still execute in AUDIO mode, but you won’t see the detailed results:\n\nawait run(\n  \"When the latest Brazil vs. Argentina soccer match happened and what was the final score?\",\n  google.Modality.TEXT,\n  [{ googleSearch: {} }]\n);\n\nConnection opened\nPrompt:  When the latest Brazil vs. Argentina soccer match happened and what was the final score?\n\n\n\n\n\nprint(google_search.search(queries=[\"latest Brazil vs Argentina soccer match date and score\", \"Brazil vs Argentina recent match results\"]))\n\n\n\n\n\n\n\n\nLooking up information on Google Search.\n\n\n\n\n\n\nThe most recent match\n\n\nbetween Brazil and Argentina took place on **March 25, 20\n\n\n25, as part of the 2026 FIFA World Cup qualifiers. Argentina won the match with a final score of 4-1**.\n\n\n\n\n  \n    \n      \n      \n      \n      \n    \n    \n      \n      \n      \n      \n      \n    \n    \n  \n  \n    Brazil vs Argentina recent match results\n    latest Brazil vs Argentina soccer match date and score\n  \n\n\n\nSession completed\nConnection closed",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini 2.0 - Multimodal live API: Tool use"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_LiveAPI_tools.html#multiple-tools",
    "href": "quickstarts/Get_started_LiveAPI_tools.html#multiple-tools",
    "title": "Gemini 2.0 - Multimodal live API: Tool use",
    "section": "Multiple tools",
    "text": "Multiple tools\nThe biggest difference with the new API however is that you’re no longer limited to using 1-tool per request. Try combining those tasks from the previous sections:\n\nimport { Tool } from \"@google/genai\";\n\nconst multi_tool_prompt = `\n  Hey, I need you to do three things for me.\n\n  1. Then compute the largest prime plaindrome under 100000.\n  2. Then use google search to lookup unformation about the largest earthquake in california the week of Dec 5 2024?\n  3. Turn on the lights\n\n  Thanks!\n`;\nconst multi_tool_tools: Tool[] = [\n  { codeExecution: {} },\n  { googleSearch: {} },\n  { functionDeclarations: [turn_on_the_lights, turn_off_the_lights] },\n];\n\nawait run(multi_tool_prompt, google.Modality.TEXT, multi_tool_tools);\n\nConnection opened\nPrompt:  \n  Hey, I need you to do three things for me.\n\n  1. Then compute the largest prime plaindrome under 100000.\n  2. Then use google search to lookup unformation about the largest earthquake in california the week of Dec 5 2024?\n  3. Turn on the lights\n\n  Thanks!\n\n\n\nOkay\n\n\n, I can do that. Here’s the plan:\n\nCompute\n\n\n\nthe largest prime palindrome under 100000. I’ll use a Python script to achieve this. 2. Use Google Search to look up\n\n\ninformation about the largest earthquake in California the week of Dec 5 2024. 3. Turn on the lights using the provided API.\nHere\n\n\n’s the first step, computing the largest prime palindrome under 100000:\n\n\n\n\n\ndef is_palindrome(n):\n  return str(n) == str(n)[::-1]\n\ndef is_prime(n):\n  if n &lt; 2:\n    return False\n  for i in range(2, int(n**0.5) + 1):\n    if n % i == 0:\n      return False\n  return True\n\nlargest_prime_palindrome = 0\nfor i in range(99999, 1, -1):\n  if is_palindrome(i) and is_prime(i):\n    largest_prime_palindrome = i\n    break\n\nprint(largest_prime_palindrome)\n\n\n\n\n\n\n\n\n98689\n\n\n\n\n\n\nOkay\n\n\n, the largest prime palindrome under 100000 is 98689.\n\n\nNow, let’s use Google Search to find the largest earthquake in California the week of Dec 5 2024.\n\n\n\n\n\n\n\n\nconcise_search(\"largest earthquake california week of December 5 2024\", max_num_results=5)\n\n\n\n\n\n\n\n\nLooking up information on Google Search.\n\n\n\n\n\n\nBased\n\n\non the search results, the largest earthquake in California during the week of December\n\n\n5, 2024, was a magnitude 7.0 earthquake offshore of Cape Mendocino on December 5, 2024,\n\n\nat 10:44 a.m. PST.\nFinally, I will turn on the lights.\n\n\n\n\n\ndefault_api.turn_on_the_lights()\n\n\n\n\n\n\n\n  \n    \n      \n      \n      \n      \n    \n    \n      \n      \n      \n      \n      \n    \n    \n  \n  \n    largest earthquake california week of December 5 2024\n  \n\n\n\nTool call responses: [\n  {\n    \"id\": \"function-call-10200942088489058256\",\n    \"name\": \"turn_on_the_lights\",\n    \"response\": {\n      \"result\": \"ok\"\n    }\n  }\n]\nSession completed\nConnection closed",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini 2.0 - Multimodal live API: Tool use"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_LiveAPI_tools.html#next-steps",
    "href": "quickstarts/Get_started_LiveAPI_tools.html#next-steps",
    "title": "Gemini 2.0 - Multimodal live API: Tool use",
    "section": "Next Steps",
    "text": "Next Steps\n\nFor more information about the SDK see the SDK docs\nThis tutorial uses the high level SDK, if you’re interested in the lower-level details, try the Websocket version of this tutorial\nThis tutorial only covers basic usage of these tools for deeper (and more fun) example see the Search tool tutorial\n\nOr check the other Gemini 2.0 capabilities from the Cookbook, in particular this other multi-tool example and the one about Gemini spatial capabilities.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini 2.0 - Multimodal live API: Tool use"
    ]
  }
]