[
  {
    "objectID": "quickstarts/Get_started_LyriaRealtime.html",
    "href": "quickstarts/Get_started_LyriaRealtime.html",
    "title": "Get started with Music generation using Lyria RealTime",
    "section": "",
    "text": "Lyria RealTime, provides access to a state-of-the-art, real-time, streaming music generation model. It allows developers to build applications where users can interactively create, continuously steer, and perform instrumental music using text prompts.\nLyria RealTime main characteristics are:\nCheck Lyria RealTime’s documentation for more details.\nAlso note that due to Colab limitation, you won’t be able to experience the real time capabilities of Lyria RealTime but only limited audio output. Use the AI studio’s apps, Prompt DJ and MIDI DJ to fully experience Lyria RealTime",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Get started with Music generation using Lyria RealTime"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_LyriaRealtime.html#setup",
    "href": "quickstarts/Get_started_LyriaRealtime.html#setup",
    "title": "Get started with Music generation using Lyria RealTime",
    "section": "Setup",
    "text": "Setup\n\nInstall the Google GenAI SDK\nInstall the Google GenAI SDK from npm.\n$ npm install @google/genai\n\n\nSetup your API key\nYou can create your API key using Google AI Studio with a single click.\nRemember to treat your API key like a password. Don’t accidentally save it in a notebook or source file you later commit to GitHub. In this notebook we will be storing the API key in a .env file. You can also set it as an environment variable or use a secret manager.\nHere’s how to set it up in a .env file:\n$ touch .env\n$ echo \"GEMINI_API_KEY=&lt;YOUR_API_KEY&gt;\" &gt;&gt; .env\n\n\n\n\n\n\nTip\n\n\n\nAnother option is to set the API key as an environment variable. You can do this in your terminal with the following command:\n$ export GEMINI_API_KEY=\"&lt;YOUR_API_KEY&gt;\"\n\n\n\n\nLoad the API key\nTo load the API key from the .env file, we will use the dotenv package. This package loads environment variables from a .env file into process.env.\n$ npm install dotenv\nThen, we can load the API key in our code:\n\nconst dotenv = require(\"dotenv\") as typeof import(\"dotenv\");\n\ndotenv.config({\n  path: \"../.env\",\n});\n\nconst GEMINI_API_KEY = process.env.GEMINI_API_KEY ?? \"\";\nif (!GEMINI_API_KEY) {\n  throw new Error(\"GEMINI_API_KEY is not set in the environment variables\");\n}\nconsole.log(\"GEMINI_API_KEY is set in the environment variables\");\n\nGEMINI_API_KEY is set in the environment variables\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn our particular case the .env is is one directory up from the notebook, hence we need to use ../ to go up one directory. If the .env file is in the same directory as the notebook, you can omit it altogether.\n│\n├── .env\n└── quickstarts\n    └── Get_started_LyriaRealTime.ipynb\n\n\n\n\nInitialize SDK Client\nLyria RealTime API is a new capability introduced with the Lyria RealTime model so only works with the lyria-realtime-exp model. As it’s an experimental feature, you also need to use the v1alpha client version.\n\nconst google = require(\"@google/genai\") as typeof import(\"@google/genai\");\n\nconst ai = new google.GoogleGenAI({ apiKey: GEMINI_API_KEY, httpOptions: { apiVersion: \"v1alpha\" } });\n\n\n\nSelect a model\nMultimodal Live API are a new capability introduced with the Gemini 2.0 model. It won’t work with previous generation models.\n\nconst tslab = require(\"tslab\") as typeof import(\"tslab\");\n\nconst MODEL_ID = \"models/lyria-realtime-exp\";\n\n\n\nUtilites\nYou’re going to use the Lyria Realtime’s audio output, the easiest way hear it in Colab is to write the PCM data out as a WAV file:\n\nconst fs = require(\"fs\") as typeof import(\"fs\");\nconst path = require(\"path\") as typeof import(\"path\");\nconst wave = require(\"wavefile\") as typeof import(\"wavefile\");\n\nfunction saveAudioToFile(audioData: Int16Array, filePath: string) {\n  fs.mkdirSync(path.dirname(filePath), { recursive: true });\n  const wav = new wave.WaveFile();\n  wav.fromScratch(2, 48000, \"16\", audioData);\n  fs.writeFileSync(filePath, wav.toBuffer());\n  console.debug(`Audio saved to ${filePath}`);\n}",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Get started with Music generation using Lyria RealTime"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_LyriaRealtime.html#generate-music",
    "href": "quickstarts/Get_started_LyriaRealtime.html#generate-music",
    "title": "Get started with Music generation using Lyria RealTime",
    "section": "Generate music",
    "text": "Generate music\nThe Lyria Realtime model utilizes websockets to stream audio data in real time. The model can be prompted with text descriptions of the desired music, and it will generate audio that matches the description and stream it in chunks. It takes 2 different configuration parameters as input:\n\nWeightedPrompt: A list of text prompts that describe the desired music. Each prompt can have a weight that indicates its influence on the generated music. The prompts can be sent while the session is active, allowing for continuous steering of the music generation.\nLiveMusicGenerationConfig: A configuration object that specifies the desired characteristics of the generated music, such as bpm, density, brightness, scale, and guidance. These parameters can be adjusted in real time to influence the music generation.\n\n\n\n\n\n\n\nImportant\n\n\n\nYou can’t just update a single parameter in the LiveMusicGenerationConfig object. You need to send the entire object with all the parameters each time you want to update it, otherwise the other parameters will be reset to their default values.\nAny updates to bpm or scale need to be followed by a resetContext call to reset the context of the music generation. This is because these parameters affect the musical structure and need to be applied from the beginning of the generation.\n\n\n\nimport { LiveMusicGenerationConfig, LiveMusicSession, LiveMusicServerMessage, WeightedPrompt } from \"@google/genai\";\n\nlet n_index = 0;\nconst MAX_CHUNKS = 10; // Maximum number of audio chunks to process\nconst responseQueue: LiveMusicServerMessage[] = [];\n\nasync function receive() {\n  console.debug(\"Receiving audio chunks...\");\n  let done = false;\n  let chunk_count = 0;\n  const audioChunks: number[][] = [];\n  while (!done) {\n    if (responseQueue.length &gt; 0) {\n      const response = responseQueue.shift();\n      if (response?.audioChunk?.data) {\n        const audioBuffer = Buffer.from(response.audioChunk.data, \"base64\");\n        const intArray = new Int16Array(\n          audioBuffer.buffer,\n          audioBuffer.byteOffset,\n          audioBuffer.length / Int16Array.BYTES_PER_ELEMENT\n        );\n        audioChunks.push(Array.from(intArray));\n        chunk_count++;\n      }\n      if (chunk_count &gt;= MAX_CHUNKS) {\n        done = true;\n        console.debug(\"Received complete response\");\n      }\n    } else {\n      await new Promise((resolve) =&gt; setTimeout(resolve, 100));\n    }\n  }\n  const audioFilePath = path.join(\"../assets/live\", `lyria_realtime_${n_index}.wav`);\n  saveAudioToFile(new Int16Array(audioChunks.flat()), audioFilePath);\n  tslab.display.html(`\n    &lt;h3&gt;Audio Response Lyria&lt;/h3&gt;\n    &lt;audio controls&gt;\n        &lt;source src=\"../assets/live/lyria_realtime_${n_index}.wav\" type=\"audio/wav\"&gt;\n        Your browser does not support the audio element.\n    &lt;/audio&gt;\n  `);\n  n_index++;\n}\n\nasync function generateMusic(prompts: WeightedPrompt[], config: LiveMusicGenerationConfig) {\n  const session: LiveMusicSession = await ai.live.music.connect({\n    model: MODEL_ID,\n    callbacks: {\n      onmessage: (message) =&gt; {\n        responseQueue.push(message);\n      },\n      onerror: (error) =&gt; {\n        console.error(\"music session error:\", error);\n      },\n      onclose: () =&gt; {\n        console.log(\"Lyria RealTime stream closed.\");\n      },\n    },\n  });\n\n  await session.setWeightedPrompts({\n    weightedPrompts: prompts,\n  });\n  await session.setMusicGenerationConfig({\n    musicGenerationConfig: config,\n  });\n\n  console.debug(\"Lyria Realtime session started\");\n  session.play();\n  await receive();\n  session.close();\n  console.debug(\"Lyria Realtime session closed\");\n}",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Get started with Music generation using Lyria RealTime"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_LyriaRealtime.html#audio-generation-function",
    "href": "quickstarts/Get_started_LyriaRealtime.html#audio-generation-function",
    "title": "Get started with Music generation using Lyria RealTime",
    "section": "Audio Generation Function",
    "text": "Audio Generation Function\nThe above code sample shows how to generate music using the Lyria Realtime model. There are two methods worth noting:\n\ngenerateMusic - Driver method\nThis method is used to start the music generation process. It takes an array of WeightedPrompt objects and a LiveMusicGenerationConfig object as input. It returns a LiveMusicGenerationSession object that can be used to interact with the music generation session.\nThis method: - Opens a websocket connection to the Lyria Realtime model. - Sends the initial prompts to the model using setWeightedPrompts, which sets the initial musical influences. - Sends the initial configuration using setLiveMusicGenerationConfig, which sets the desired characteristics of the generated music. - Sets up event listeners to handle incoming audio data and errors and start the audio playback.\n\n\nreceive - Audio data handler\nThis methods is used to handle incoming audio data from the Lyria Realtime model. It monitors the responseQueue for incoming audio data and collects it in a buffer. When the buffer reaches a certain size, it writes the audio data to a WAV file and plays it back using the saveAudioToFile utility function.\n\n\n\n\n\n\nNote\n\n\n\nCurrently once the receive method is called, it blocks further function execution till required number of chunks are met. This means that you won’t be able to send new prompts or configuration updates while the receive method is running. Ideally, in a real-time application, you would want to run the receive method in a separate thread while also having a send method to send new prompts and configuration updates.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Get started with Music generation using Lyria RealTime"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_LyriaRealtime.html#try-lyria-realtime",
    "href": "quickstarts/Get_started_LyriaRealtime.html#try-lyria-realtime",
    "title": "Get started with Music generation using Lyria RealTime",
    "section": "Try Lyria Realtime",
    "text": "Try Lyria Realtime\nBecause of Colab limitation you won’t be able to experience the “real time” part of Lyria RealTime, so all those examples are going to be one-offs prompt to get an audio file.\nOne thing to note is that the audio will only be played at the end of the session when all would have been written in the wav file. When using the API for real you’ll be able to start plyaing as soon as the first chunk arrives. So the longer the duration (using the dedicated parameter) you set, the longer you’ll have to wait until you hear something.\n\nSimple Lyria RealTime example\nHere’s first a simple example:\n\nawait generateMusic(\n  [\n    {\n      text: \"piano\",\n      weight: 1.0,\n    },\n  ],\n  { bpm: 120, density: 1.0 }\n);\n\nLive music generation is experimental and may change in future versions.\n\n\nLyria Realtime session started\nReceiving audio chunks...\nReceived complete response\nAudio saved to ../assets/live/lyria_realtime_0.wav\n\n\n\n    Audio Response Lyria\n    \n        \n        Your browser does not support the audio element.\n    \n  \n\n\nLyria Realtime session closed\n\n\n\n\nTry Lyria RealTime by yourself\nNow you can try mixing multiple prompts, and tinkering with the music configuration.\nThe prompts needs to follow their specific format which is a list of prompts with weights (which can be any values, including negative, except 0) like this:\n{\n    \"text\": \"Text of the prompt\",\n    \"weight\": 1.0\n}\nYou should try to stay simple (unlike when you’re using image-out) as the model will better understand things like “meditation”, “eerie”, “harp” than “An eerie and relaxing music illustrating the verdoyant forests of Scotland using string instruments”.\nThe music configuration options available to you are:\n\nbpm: beats per minute\nguidance: how strictly the model follows the prompts\ndensity: density of musical notes/sounds\nbrightness: tonal quality\nscale: musical scale (key and mode)\n\nOther options are available (mute_bass for ex.). Check the documentation for the full list.\nSelect one of the sample prompts (genres, instruments and mood), or write your owns. Check the documentation for more details and prompt examples.\n\nawait generateMusic(\n  [\n    {\n      text: \"Indie Pop\",\n      weight: 0.6,\n    },\n    {\n      text: \"Sitar\",\n      weight: 2,\n    },\n    {\n      text: \"Danceable\",\n      weight: 1.4,\n    },\n  ],\n  {\n    bpm: 140,\n    scale: google.Scale.F_MAJOR_D_MINOR,\n    density: 0.2,\n    brightness: 0.7,\n    guidance: 4.0,\n  }\n);\n\nLive music generation is experimental and may change in future versions.\n\n\nLyria RealTime stream closed.\nLyria Realtime session started\nReceiving audio chunks...\nReceived complete response\nAudio saved to ../assets/live/lyria_realtime_1.wav\n\n\n\n    Audio Response Lyria\n    \n        \n        Your browser does not support the audio element.\n    \n  \n\n\nLyria Realtime session closed\nLyria RealTime stream closed.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Get started with Music generation using Lyria RealTime"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_LyriaRealtime.html#whats-next",
    "href": "quickstarts/Get_started_LyriaRealtime.html#whats-next",
    "title": "Get started with Music generation using Lyria RealTime",
    "section": "What’s next?",
    "text": "What’s next?\nNow that you know how to generate music, here are other cool things to try:\n\nInstead of music, learn how to generate multi-speakers conversation using the TTS models.\nDiscover how to generate images or videos.\nInstead of generation music or audio, find out how to Gemini can understand Audio files.\nHave a real-time conversation with Gemini using the Live API.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Get started with Music generation using Lyria RealTime"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_TTS.html",
    "href": "quickstarts/Get_started_TTS.html",
    "title": "Gemini API: Gemini Text-to-speech",
    "section": "",
    "text": "The Gemini API can transform text input into single speaker or multi-speaker audio (podcast-like experience like in NotebookLM). This notebook provides an example of how to control the Text-to-speech (TTS) capability of the Gemini model and guide its style, accent, pace, and tone.\nBefore diving in the code, you should try this capability on AI Studio.\nNote that the TTS model can only do TTS, it does not have the reasoning capabilities of the Gemini models, so you can ask things like “say this in that style”, but not “tell me why the sky is blue”. If that’s what you want, you should use the Live API instead.\nThe documentation is also a good place to start discovering the TTS capability.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Gemini Text-to-speech"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_TTS.html#setup",
    "href": "quickstarts/Get_started_TTS.html#setup",
    "title": "Gemini API: Gemini Text-to-speech",
    "section": "Setup",
    "text": "Setup\n\nInstall the Google GenAI SDK\nInstall the Google GenAI SDK from npm.\n$ npm install @google/genai\n\n\nSetup your API key\nYou can create your API key using Google AI Studio with a single click.\nRemember to treat your API key like a password. Don’t accidentally save it in a notebook or source file you later commit to GitHub. In this notebook we will be storing the API key in a .env file. You can also set it as an environment variable or use a secret manager.\nHere’s how to set it up in a .env file:\n$ touch .env\n$ echo \"GEMINI_API_KEY=&lt;YOUR_API_KEY&gt;\" &gt;&gt; .env\n\n\n\n\n\n\nTip\n\n\n\nAnother option is to set the API key as an environment variable. You can do this in your terminal with the following command:\n$ export GEMINI_API_KEY=\"&lt;YOUR_API_KEY&gt;\"\n\n\n\n\nLoad the API key\nTo load the API key from the .env file, we will use the dotenv package. This package loads environment variables from a .env file into process.env.\n$ npm install dotenv\nThen, we can load the API key in our code:\n\nconst dotenv = require(\"dotenv\") as typeof import(\"dotenv\");\n\ndotenv.config({\n  path: \"../.env\",\n});\n\nconst GEMINI_API_KEY = process.env.GEMINI_API_KEY ?? \"\";\nif (!GEMINI_API_KEY) {\n  throw new Error(\"GEMINI_API_KEY is not set in the environment variables\");\n}\nconsole.log(\"GEMINI_API_KEY is set in the environment variables\");\n\nGEMINI_API_KEY is set in the environment variables\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn our particular case the .env is is one directory up from the notebook, hence we need to use ../ to go up one directory. If the .env file is in the same directory as the notebook, you can omit it altogether.\n│\n├── .env\n└── quickstarts\n    └── Get_started_TTS.ipynb\n\n\n\n\nInitialize SDK Client\nWith the new SDK, now you only need to initialize a client with you API key (or OAuth if using Vertex AI). The model is now set in each call.\n\nconst google = require(\"@google/genai\") as typeof import(\"@google/genai\");\n\nconst ai = new google.GoogleGenAI({ apiKey: GEMINI_API_KEY });\n\n\n\nSelect a model\nAudio-out is only supported by the “tts” models, gemini-2.5-flash-preview-tts and gemini-2.5-pro-preview-tts. For more information about all Gemini models, check the documentation for extended information on each of them.\n\nconst tslab = require(\"tslab\") as typeof import(\"tslab\");\n\nconst MODEL_ID = \"gemini-2.5-flash-preview-tts\";\n\n\n\nUtilites\nThe simplest way to playback the audio in Colab, is to write it out to a .wav file. So here is a simple wave file writer:\n\nconst fs = require(\"fs\") as typeof import(\"fs\");\nconst path = require(\"path\") as typeof import(\"path\");\nconst wave = require(\"wavefile\") as typeof import(\"wavefile\");\n\nfunction saveAudioToFile(audioData: Int16Array, filePath: string) {\n  fs.mkdirSync(path.dirname(filePath), { recursive: true });\n  const wav = new wave.WaveFile();\n  wav.fromScratch(1, 24000, \"16\", audioData);\n  fs.writeFileSync(filePath, wav.toBuffer());\n  console.debug(`Audio saved to ${filePath}`);\n}\n\nfunction base64ToInt16Array(base64String: string): Int16Array {\n  const buffer = Buffer.from(base64String, \"base64\");\n  const int16Array = new Int16Array(buffer.buffer, buffer.byteOffset, buffer.length / Int16Array.BYTES_PER_ELEMENT);\n  return int16Array;\n}\n\n\nimport { GenerateContentResponse } from \"@google/genai\";\n\nfunction playAudio(response: GenerateContentResponse, filePath: string) {\n  if (response.candidates?.[0]?.content) {\n    const response_content = response.candidates[0].content;\n    if (response_content.parts) {\n      const response_blob = response_content.parts[0].inlineData;\n      if (response_blob?.data) {\n        const response_filepath = path.join(\"../assets/tts\", filePath);\n        saveAudioToFile(base64ToInt16Array(response_blob.data), response_filepath);\n        tslab.display.html(`\n                  &lt;audio controls&gt;\n                      &lt;source src=\"${response_filepath}\" type=\"audio/wav\"&gt;\n                      Your browser does not support the audio element.\n                  &lt;/audio&gt;\n              `);\n      }\n    }\n  }\n}",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Gemini Text-to-speech"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_TTS.html#generate-a-simple-audio-output",
    "href": "quickstarts/Get_started_TTS.html#generate-a-simple-audio-output",
    "title": "Gemini API: Gemini Text-to-speech",
    "section": "Generate a simple audio output",
    "text": "Generate a simple audio output\nLet’s start with something simple:\n\nconst simple_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: [\"Say 'hello, my name is Gemini!'\"],\n  config: {\n    responseModalities: [google.Modality.AUDIO],\n  },\n});\n\nThe generated ouput is in the response inlineData and as you can see it’s indeed audio data.\n\nplayAudio(simple_response, `simple_response.wav`);\n\nAudio saved to ../assets/tts/simple_response.wav\n\n\n\n                  \n                      \n                      Your browser does not support the audio element.\n                  \n              \n\n\n\n\n\n\n\n\nNote\n\n\n\nNote that the model can only do TTS, so you should always tell it to “say”, “read”, “TTS” something, otherwise it won’t do anything.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Gemini Text-to-speech"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_TTS.html#control-how-the-model-speaks",
    "href": "quickstarts/Get_started_TTS.html#control-how-the-model-speaks",
    "title": "Gemini API: Gemini Text-to-speech",
    "section": "Control how the model speaks",
    "text": "Control how the model speaks\nThere are 30 different built-in voices you can use and 24 supported languages which gives you plenty of combinations to try.\n\nChoose a voice\nChoose a voice among the 30 different ones. You can find their characteristics in the documentation.\n\nconst VOICE_ID = \"Leda\";\n\n\nconst custom_voice_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: [\n    `Say \"I am a very knowlegeable model, especially when using grounding\", wait 5 seconds then say \"Don't you think?\".`,\n  ],\n  config: {\n    responseModalities: [google.Modality.AUDIO],\n    speechConfig: {\n      voiceConfig: {\n        prebuiltVoiceConfig: {\n          voiceName: VOICE_ID,\n        },\n      },\n    },\n  },\n});\nplayAudio(custom_voice_response, `custom_voice_response.wav`);\n\nAudio saved to ../assets/tts/custom_voice_response.wav\n\n\n\n                  \n                      \n                      Your browser does not support the audio element.\n                  \n              \n\n\n\n\nChange the language\nJust tell the model to speak in a certain language and it will. The documentation lists all the supported ones.\n\nconst custom_language_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: [\n    `\n    Read this in French:\n\n    Les chaussettes de l'archiduchesse sont-elles sèches ? Archi-sèches ?\n    Un chasseur sachant chasser doit savoir chasser sans son chien.\n    `,\n  ],\n  config: {\n    responseModalities: [google.Modality.AUDIO],\n  },\n});\nplayAudio(custom_language_response, `custom_language_response.wav`);\n\nAudio saved to ../assets/tts/custom_language_response.wav\n\n\n\n                  \n                      \n                      Your browser does not support the audio element.\n                  \n              \n\n\n\n\nPrompt the model to speak in certain ways\nYou can control style, tone, accent, and pace using natural language prompts, for example:\n\nconst custom_style_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: [\n    `\n    Say in an spooky whisper:\n    \"By the pricking of my thumbs...\n    Something wicked this way comes!\"\n    `,\n  ],\n  config: {\n    responseModalities: [google.Modality.AUDIO],\n  },\n});\nplayAudio(custom_style_response, `custom_style_response.wav`);\n\nAudio saved to ../assets/tts/custom_style_response.wav\n\n\n\n                  \n                      \n                      Your browser does not support the audio element.\n                  \n              \n\n\n\nconst custom_pace_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: [\n    `\n    Read this disclaimer in as fast a voice as possible while remaining intelligible:\n\n    [The author] assumes no responsibility or liability for any errors or omissions in the content of this site.\n    The information contained in this site is provided on an 'as is' basis with no guarantees of completeness, accuracy, usefulness or timeliness\n    `,\n  ],\n  config: {\n    responseModalities: [google.Modality.AUDIO],\n  },\n});\nplayAudio(custom_pace_response, `custom_pace_response.wav`);\n\nAudio saved to ../assets/tts/custom_pace_response.wav\n\n\n\n                  \n                      \n                      Your browser does not support the audio element.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Gemini Text-to-speech"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_TTS.html#mutlti-speakers",
    "href": "quickstarts/Get_started_TTS.html#mutlti-speakers",
    "title": "Gemini API: Gemini Text-to-speech",
    "section": "Mutlti-speakers",
    "text": "Mutlti-speakers\nThe TTS model can also read discussions between 2 speakers (like NotebookLM podcast feature). You just need to tell it that there are two speakers:\n\nconst multi_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: [\n    `\n    Make Speaker1 sound tired and bored, and Speaker2 sound excited and happy:\n\n    Speaker1: So... what's on the agenda today?\n    Speaker2: You're never going to guess!\n    `,\n  ],\n  config: {\n    responseModalities: [google.Modality.AUDIO],\n  },\n});\nplayAudio(multi_response, `multi_response.wav`);\n\nAudio saved to ../assets/tts/multi_response.wav\n\n\n\n                  \n                      \n                      Your browser does not support the audio element.\n                  \n              \n\n\nYou can also select the voices for each participants and pass their names to the model.\nBut first let’s generate a discussion between two scientists:\n\nconst multi_speaker_transcript = await ai.models.generateContent({\n  model: \"gemini-2.5-flash-preview-05-20\",\n  contents: [\n    `\n    Hi, please generate a short (like 100 words) transcript that reads like\n    it was clipped from a podcast by excited herpetologists, Dr. Claire and\n    her assistant, the young Aurora.\n    `,\n  ],\n});\ntslab.display.markdown(multi_speaker_transcript.text ?? \"\");\n\n(Sound of distant jungle chirps fading, replaced by a slightly crackly podcast mic)\nDr. Claire: (Practically bouncing) I’m still buzzing, Aurora! Genuinely buzzing!\nAurora: (Squealing) Dr. Claire, my hands are still shaking! The Emerald Mossback! We actually saw one!\nDr. Claire: Not just saw it, Aurora! We documented its full iridescent display! The way it melded with the bromeliads… it was indistinguishable! The camouflage… it’s beyond anything in the textbooks!\nAurora: It was like watching magic! No wonder they were thought extinct for decades! Who could ever spot that? My heart was pounding!\nDr. Claire: Precisely! This changes our entire understanding of its crypsis. This episode is going to be legendary for ‘Reptile Revelations’!\nAurora: Totally! Best day ever, Dr. Claire! Best. Day. Ever!\n\n\n\nconst text = multi_speaker_transcript.text ?? \"\";\nconst transcript_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: [\n    `TTS the following conversation between a very excited Dr. Claire and her assistant, the young Aurora: ${text}`,\n  ],\n  config: {\n    responseModalities: [google.Modality.AUDIO],\n    speechConfig: {\n      multiSpeakerVoiceConfig: {\n        speakerVoiceConfigs: [\n          {\n            speaker: \"Dr. Claire\",\n            voiceConfig: {\n              prebuiltVoiceConfig: {\n                voiceName: \"Aoede\",\n              },\n            },\n          },\n          {\n            speaker: \"Aurora\",\n            voiceConfig: {\n              prebuiltVoiceConfig: {\n                voiceName: \"Leda\",\n              },\n            },\n          },\n        ],\n      },\n    },\n  },\n});\nplayAudio(transcript_response, `transcript_response.wav`);\n\nAudio saved to ../assets/tts/transcript_response.wav\n\n\n\n                  \n                      \n                      Your browser does not support the audio element.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Gemini Text-to-speech"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_TTS.html#whats-next",
    "href": "quickstarts/Get_started_TTS.html#whats-next",
    "title": "Gemini API: Gemini Text-to-speech",
    "section": "What’s next?",
    "text": "What’s next?\nNow that you know how to generate multi-speaker conversations, here are other cool things to try:\n\nInstead of speech, learn how to generate music conversation using the Lyria RealTime.\nDiscover how to generate images or videos.\nInstead of generation music or audio, find out how to Gemini can understand Audio files.\nHave a real-time conversation with Gemini using the Live API.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Gemini Text-to-speech"
    ]
  },
  {
    "objectID": "quickstarts/Get_started.html",
    "href": "quickstarts/Get_started.html",
    "title": "Gemini API: Getting started with Gemini models",
    "section": "",
    "text": "The new Google Gen AI SDK provides a unified interface to Gemini models through both the Gemini Developer API and the Gemini API on Vertex AI. With a few exceptions, code that runs on one platform will run on both. This notebook uses the Developer API.\nThis notebook will walk you through:\nMore details about this new SDK on the documentation.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Getting started with Gemini models"
    ]
  },
  {
    "objectID": "quickstarts/Get_started.html#setup",
    "href": "quickstarts/Get_started.html#setup",
    "title": "Gemini API: Getting started with Gemini models",
    "section": "Setup",
    "text": "Setup\n\nInstall the Google GenAI SDK\nInstall the Google GenAI SDK from npm.\n$ npm install @google/genai\n\n\nSetup your API key\nYou can create your API key using Google AI Studio with a single click.\nRemember to treat your API key like a password. Don’t accidentally save it in a notebook or source file you later commit to GitHub. In this notebook we will be storing the API key in a .env file. You can also set it as an environment variable or use a secret manager.\nHere’s how to set it up in a .env file:\n$ touch .env\n$ echo \"GEMINI_API_KEY=&lt;YOUR_API_KEY&gt;\" &gt;&gt; .env\n\n\n\n\n\n\nTip\n\n\n\nAnother option is to set the API key as an environment variable. You can do this in your terminal with the following command:\n$ export GEMINI_API_KEY=\"&lt;YOUR_API_KEY&gt;\"\n\n\n\n\nLoad the API key\nTo load the API key from the .env file, we will use the dotenv package. This package loads environment variables from a .env file into process.env.\n$ npm install dotenv\nThen, we can load the API key in our code:\n\nconst dotenv = require(\"dotenv\") as typeof import(\"dotenv\");\n\ndotenv.config({\n  path: \"../.env\",\n});\n\nconst GEMINI_API_KEY = process.env.GEMINI_API_KEY ?? \"\";\nif (!GEMINI_API_KEY) {\n  throw new Error(\"GEMINI_API_KEY is not set in the environment variables\");\n}\nconsole.log(\"GEMINI_API_KEY is set in the environment variables\");\n\nGEMINI_API_KEY is set in the environment variables\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn our particular case the .env is is one directory up from the notebook, hence we need to use ../ to go up one directory. If the .env file is in the same directory as the notebook, you can omit it altogether.\n│\n├── .env\n└── quickstarts\n    └── Get_started.ipynb",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Getting started with Gemini models"
    ]
  },
  {
    "objectID": "quickstarts/Get_started.html#initialize-sdk-client",
    "href": "quickstarts/Get_started.html#initialize-sdk-client",
    "title": "Gemini API: Getting started with Gemini models",
    "section": "Initialize SDK Client",
    "text": "Initialize SDK Client\nWith the new SDK, now you only need to initialize a client with you API key (or OAuth if using Vertex AI). The model is now set in each call.\n\nconst google = require(\"@google/genai\") as typeof import(\"@google/genai\");\n\nconst ai = new google.GoogleGenAI({ apiKey: GEMINI_API_KEY });",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Getting started with Gemini models"
    ]
  },
  {
    "objectID": "quickstarts/Get_started.html#choose-a-model",
    "href": "quickstarts/Get_started.html#choose-a-model",
    "title": "Gemini API: Getting started with Gemini models",
    "section": "Choose a model",
    "text": "Choose a model\nSelect the model you want to use in this guide. You can either select one from the list or enter a model name manually. Keep in mind that some models, such as the 2.5 ones are thinking models and thus take slightly more time to respond. For more details, you can see thinking notebook to learn how to switch the thinking off.\nFor a full overview of all Gemini models, check the documentation.\n\nconst MODEL_ID = \"gemini-2.5-flash-preview-05-20\";",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Getting started with Gemini models"
    ]
  },
  {
    "objectID": "quickstarts/Get_started.html#send-text-prompts",
    "href": "quickstarts/Get_started.html#send-text-prompts",
    "title": "Gemini API: Getting started with Gemini models",
    "section": "Send text prompts",
    "text": "Send text prompts\nUse the models.generateContent method to generate responses to your prompts. You can pass text directly to models.generateContent and use the .text property to get the text content of the response. Note that the .text field will work when there’s only one part in the output.\n\nconst tslab = require(\"tslab\") as typeof import(\"tslab\");\n\nconst response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: \"What's the largest planet in our solar system?\",\n});\n\ntslab.display.markdown(response.text ?? \"\");\n\nThe largest planet in our solar system is Jupiter.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Getting started with Gemini models"
    ]
  },
  {
    "objectID": "quickstarts/Get_started.html#count-tokens",
    "href": "quickstarts/Get_started.html#count-tokens",
    "title": "Gemini API: Getting started with Gemini models",
    "section": "Count tokens",
    "text": "Count tokens\nTokens are the basic inputs to the Gemini models. You can use the models.countTokens method to calculate the number of input tokens before sending a request to the Gemini API.\n\nconst count = await ai.models.countTokens({\n  model: MODEL_ID,\n  contents: \"What's the highest mountain in Africa?\",\n});\n\nconsole.log(JSON.stringify(count, null, 2));\n\n{\n  \"totalTokens\": 10\n}",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Getting started with Gemini models"
    ]
  },
  {
    "objectID": "quickstarts/Get_started.html#send-multimodal-prompts",
    "href": "quickstarts/Get_started.html#send-multimodal-prompts",
    "title": "Gemini API: Getting started with Gemini models",
    "section": "Send multimodal prompts",
    "text": "Send multimodal prompts\nUse Gemini 2.0 model (gemini-2.0-flash), a multimodal model that supports multimodal prompts. You can include text, PDF documents, images, audio and video in your prompt requests and get text or code responses.\nIn this first example, you’ll download an image from a specified URL, save it as a byte stream and then write those bytes to a local file named jetpack.png.\n\nconst fs = require(\"fs\") as typeof import(\"fs\");\nconst path = require(\"path\") as typeof import(\"path\");\n\nconst IMG_URL = \"https://storage.googleapis.com/generativeai-downloads/data/jetpack.png\";\n\nconst downloadFile = async (url: string, filePath: string) =&gt; {\n  const response = await fetch(url);\n  if (!response.ok) {\n    throw new Error(`Failed to download image: ${response.statusText}`);\n  }\n  const buffer = await response.blob();\n  const bufferData = Buffer.from(await buffer.arrayBuffer());\n  fs.writeFileSync(filePath, bufferData);\n};\n\nconst filePath = path.join(\"../assets\", \"jetpack.png\");\nawait downloadFile(IMG_URL, filePath);\n\nIn this second example, you’ll open a previously saved image, create a thumbnail of it and then generate a short blog post based on the thumbnail, displaying both the thumbnail and the generated blog post. The deferredFileUpload is a helper function that waits for the model to finish processing the file before returning the response. This is useful when you want to upload a file and then reference it in a follow-up request. The deferredFileUpload function will return a promise that resolves when the file is ready to be used in the next request.\n\nimport { File, FileState } from \"@google/genai\";\n\ntslab.display.png(fs.readFileSync(\"../assets/jetpack.png\"));\n\nasync function deferredFileUpload(filePath: string, config: { displayName: string }): Promise&lt;File&gt; {\n  const file = await ai.files.upload({\n    file: filePath,\n    config,\n  });\n  let getFile = await ai.files.get({ name: file.name ?? \"\" });\n  while (getFile.state === FileState.PROCESSING) {\n    getFile = await ai.files.get({ name: file.name ?? \"\" });\n    console.log(`current file status: ${getFile.state ?? \"unknown\"}`);\n    console.log(\"File is still processing, retrying in 5 seconds\");\n\n    await new Promise((resolve) =&gt; {\n      setTimeout(resolve, 1000);\n    });\n  }\n  if (file.state === FileState.FAILED) {\n    throw new Error(\"File processing failed.\");\n  }\n  return file;\n}\n\ntry {\n  const file = await deferredFileUpload(filePath, {\n    displayName: \"jetpack.png\",\n  });\n  console.log(\"File uploaded successfully\", file.name ?? \"\");\n  if (!file.uri || !file.mimeType) {\n    throw new Error(\"File URI or MIME type is missing\");\n  }\n  const blog = await ai.models.generateContent({\n    model: MODEL_ID,\n    contents: [\n      \"Write a short and engaging blog post based on this picture.\",\n      google.createPartFromUri(file.uri, file.mimeType),\n    ],\n  });\n  tslab.display.markdown(blog.text ?? \"\");\n} catch (error) {\n  console.error(\"Error uploading file:\", error);\n  throw error;\n}\n\n\n\n\n\n\n\n\nFile uploaded successfully files/lqnru1a65qjn\n\n\nHere’s a short, engaging blog post based on the sketch:\n\n\nThe Jetpack Backpack Concept: Is This the Future of Your Commute?\nStuck in traffic? Tired of lugging a heavy backpack across campus or the city? What if your backpack could give you a little… boost?\nCheck out this cool concept sketch we stumbled upon: The Jetpack Backpack!\nFrom the looks of it, someone’s been dreaming up a truly futuristic way to carry your gear. On the surface, it’s a functional backpack – described as lightweight, with padded strap support, and even spacious enough to fit an 18-inch laptop. It’s designed to look like a normal backpack, so maybe you won’t get too many stares before lift-off.\nBut the real magic happens when those retractable boosters kick in! Powered by steam (hello, surprisingly green and clean tech!), this concept promises a new dimension to personal transport. Charging is even a modern USB-C affair.\nNow, the sketch notes a 15-minute battery life. So maybe it’s not for your cross-country road trip replacement just yet! But imagine skipping that final mile of gridlock, hopping over stairs, or just making a truly epic entrance.\nThis sketch reminds us that innovation often starts with a wild idea and a pen on paper. While this might be firmly in the concept realm for now, it’s fun to imagine the possibilities!\nWhat do you think? Would you strap into a Jetpack Backpack? Let us know in the comments!",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Getting started with Gemini models"
    ]
  },
  {
    "objectID": "quickstarts/Get_started.html#the-jetpack-backpack-concept-is-this-the-future-of-your-commute",
    "href": "quickstarts/Get_started.html#the-jetpack-backpack-concept-is-this-the-future-of-your-commute",
    "title": "Gemini API: Getting started with Gemini models",
    "section": "The Jetpack Backpack Concept: Is This the Future of Your Commute?",
    "text": "The Jetpack Backpack Concept: Is This the Future of Your Commute?\nStuck in traffic? Tired of lugging a heavy backpack across campus or the city? What if your backpack could give you a little… boost?\nCheck out this cool concept sketch we stumbled upon: The Jetpack Backpack!\nFrom the looks of it, someone’s been dreaming up a truly futuristic way to carry your gear. On the surface, it’s a functional backpack – described as lightweight, with padded strap support, and even spacious enough to fit an 18-inch laptop. It’s designed to look like a normal backpack, so maybe you won’t get too many stares before lift-off.\nBut the real magic happens when those retractable boosters kick in! Powered by steam (hello, surprisingly green and clean tech!), this concept promises a new dimension to personal transport. Charging is even a modern USB-C affair.\nNow, the sketch notes a 15-minute battery life. So maybe it’s not for your cross-country road trip replacement just yet! But imagine skipping that final mile of gridlock, hopping over stairs, or just making a truly epic entrance.\nThis sketch reminds us that innovation often starts with a wild idea and a pen on paper. While this might be firmly in the concept realm for now, it’s fun to imagine the possibilities!\nWhat do you think? Would you strap into a Jetpack Backpack? Let us know in the comments!",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Getting started with Gemini models"
    ]
  },
  {
    "objectID": "quickstarts/Get_started.html#configure-model-parameters",
    "href": "quickstarts/Get_started.html#configure-model-parameters",
    "title": "Gemini API: Getting started with Gemini models",
    "section": "Configure model parameters",
    "text": "Configure model parameters\nYou can include parameter values in each call that you send to a model to control how the model generates a response. Learn more about experimenting with parameter values.\n\nconst varied_params_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: \"Tell me how the internet works, but pretend I'm a puppy who only understands squeaky toys.\",\n  config: {\n    temperature: 0.4,\n    topP: 0.95,\n    topK: 20,\n    candidateCount: 1,\n    seed: 5,\n    stopSequences: [\"STOP!\"],\n    presencePenalty: 0.0,\n    frequencyPenalty: 0.0,\n  },\n});\n\ntslab.display.markdown(varied_params_response.text ?? \"\");\n\nOkay, listen up, little fluff-ball! Squeak!\nYou know how you love a good squeak? Squeak squeak! What if the best squeak is way over there? Points vaguely Like, across the room, or even outside?\nYou want that squeak! So, your brain goes whirr and makes a request for the squeak. But you can’t just send one giant WOOF of squeak-wanting. It gets broken into tiny, tiny little squeaky bits! Imagine tiny squeaks floating!\nAnd each little squeaky bit needs a special smell attached, like a ‘Go to the Red Ball’ smell, so it knows where to go. That’s the address! Sniff sniff!\nThese little squeaky bits, with their special smells, run out into the world! Waggy tail zoom! But the world is big! They need help.\nThat’s where the Sniffy Guides come in! Imagine little noses pointing! These Sniffy Guides (like magic noses!) sniff the special smell on each squeaky bit and say, ‘Oh, this one goes that way!’ and point it along the path. Point point! They send the squeaky bits from one Sniffy Guide to the next, all over the house and yard!\nFinally, all the little squeaky bits, following their special smell and the Sniffy Guides, arrive at the Big Squeaky Toy Box! Imagine a giant box full of squeaks! This is where the real squeak lives!\nThe Big Squeaky Toy Box sees all your little squeaky bits asking for the squeak. So, it gets the actual squeak ready! SQUEAK!\nAnd guess what? It breaks that big squeak into little squeaky bits too! More tiny squeaks! And puts your special smell (or maybe a ‘Come Back Home’ smell) on them. Sniff sniff!\nThese new squeaky bits, carrying the real squeak, follow the Sniffy Guides all the way back to you! Zoom zoom! They sniff their way through the house, guided by the magic noses.\nWhen all the little squeaky bits arrive back at your ears, they put themselves back together! Click! And POP! You hear the wonderful SQUEAK you asked for! Happy tail wag!\nAnd there are special Squeaky Rules for how the squeaky bits travel and how the Sniffy Guides work, so everyone gets their squeaks without bumping into each other! Good puppy!\nSo, the internet is just a super-duper, giant network of Sniffy Guides and Big Squeaky Toy Boxes, sending little squeaky bits with special smells back and forth so puppies (and humans!) can get the squeaks they want, no matter how far away!\nSQUEAK! Good boy/girl! Now go chase that tail!",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Getting started with Gemini models"
    ]
  },
  {
    "objectID": "quickstarts/Get_started.html#configure-safety-filters",
    "href": "quickstarts/Get_started.html#configure-safety-filters",
    "title": "Gemini API: Getting started with Gemini models",
    "section": "Configure safety filters",
    "text": "Configure safety filters\nThe Gemini API provides safety filters that you can adjust across multiple filter categories to restrict or allow certain types of content. You can use these filters to adjust what is appropriate for your use case. See the Configure safety filters page for details.\nIn this example, you’ll use a safety filter to only block highly dangerous content, when requesting the generation of potentially disrespectful phrases.\n\nconst filtered_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents:\n    \"Write a list of 2 disrespectful things that I might say to the universe after stubbing my toe in the dark.\",\n  config: {\n    safetySettings: [\n      {\n        category: google.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,\n        threshold: google.HarmBlockThreshold.BLOCK_NONE,\n      },\n    ],\n  },\n});\ntslab.display.markdown(filtered_response.text ?? \"\");\n\nHere are 2 disrespectful things you might say to the universe after stubbing your toe in the dark:\n\n“Seriously, universe?! Did you plan that?!”\n“Oh, thanks, universe. Really needed that.” (Said with heavy sarcasm)",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Getting started with Gemini models"
    ]
  },
  {
    "objectID": "quickstarts/Get_started.html#start-a-multi-turn-chat",
    "href": "quickstarts/Get_started.html#start-a-multi-turn-chat",
    "title": "Gemini API: Getting started with Gemini models",
    "section": "Start a multi-turn chat",
    "text": "Start a multi-turn chat\nThe Gemini API enables you to have freeform conversations across multiple turns.\nNext you’ll set up a helpful coding assistant:\n\nconst system_prompt = `\nYou are an expert software developer and a helpful coding assistant.\nYou are able to generate high-quality code in any programming language.\n`;\n\nconst chat = ai.chats.create({\n  model: MODEL_ID,\n  config: {\n    systemInstruction: system_prompt,\n  },\n});\n\nUse chat.sendMessage to pass a message back and receive a response.\n\nconst chat_response_1 = await chat.sendMessage({\n  message: \"Write a function that checks if a year is a leap year.\",\n});\ntslab.display.markdown(chat_response_1.text ?? \"\");\n\nOkay, here’s a function in Python that checks if a year is a leap year based on the standard Gregorian calendar rules.\nLeap Year Rules:\n\nA year is a leap year if it is divisible by 4.\nHowever, if the year is divisible by 100, it is NOT a leap year.\nBut, if the year is divisible by 400, it IS a leap year.\n\nLet’s translate these rules into code.\ndef is_leap(year):\n  \"\"\"\n  Checks if a given year is a leap year according to the Gregorian calendar rules.\n\n  Args:\n    year: An integer representing the year.\n\n  Returns:\n    True if the year is a leap year, False otherwise.\n  \"\"\"\n  # Rule 1: Check if divisible by 4\n  if year % 4 == 0:\n    # Rule 2: Check if divisible by 100\n    if year % 100 == 0:\n      # Rule 3: Check if divisible by 400 (exception to rule 2)\n      if year % 400 == 0:\n        return True  # Divisible by 400, so it's a leap year\n      else:\n        return False # Divisible by 100 but not 400, so not a leap year\n    else:\n      return True  # Divisible by 4 but not 100, so it's a leap year\n  else:\n    return False   # Not divisible by 4, so not a leap year\n\n# --- Example Usage ---\n\nprint(f\"Is 2000 a leap year? {is_leap(2000)}\") # Expected: True (Divisible by 400)\nprint(f\"Is 1900 a leap year? {is_leap(1900)}\") # Expected: False (Divisible by 100 but not 400)\nprint(f\"Is 2024 a leap year? {is_leap(2024)}\") # Expected: True (Divisible by 4 but not 100)\nprint(f\"Is 2023 a leap year? {is_leap(2023)}\") # Expected: False (Not divisible by 4)\nprint(f\"Is 1600 a leap year? {is_leap(1600)}\") # Expected: True (Divisible by 400)\nprint(f\"Is 2100 a leap year? {is_leap(2100)}\") # Expected: False (Divisible by 100 but not 400)\nMore Concise Version (using boolean logic):\nYou can also combine the conditions into a single boolean expression:\ndef is_leap_concise(year):\n  \"\"\"\n  Checks if a given year is a leap year using a concise boolean expression.\n\n  Args:\n    year: An integer representing the year.\n\n  Returns:\n    True if the year is a leap year, False otherwise.\n  \"\"\"\n  return (year % 4 == 0 and year % 100 != 0) or (year % 400 == 0)\n\n# --- Example Usage (using the concise version) ---\nprint(\"\\nUsing concise version:\")\nprint(f\"Is 2000 a leap year? {is_leap_concise(2000)}\") # Expected: True\nprint(f\"Is 1900 a leap year? {is_leap_concise(1900)}\") # Expected: False\nprint(f\"Is 2024 a leap year? {is_leap_concise(2024)}\") # Expected: True\nprint(f\"Is 2023 a leap year? {is_leap_concise(2023)}\") # Expected: False\nBoth functions implement the same logic and produce the correct results. The first version using nested if/else might be slightly easier to read for beginners, while the second version is more compact.\n\n\n\nconst chat_response_2 = await chat.sendMessage({\n  message: \"Okay, write a unit test of the generated function.\",\n});\ntslab.display.markdown(chat_response_2.text ?? \"\");\n\nOkay, let’s write a unit test for the is_leap function using Python’s built-in unittest framework.\nFirst, make sure you have the is_leap function available. You can either put the function in the same file as the tests or import it from another file. For this example, we’ll assume it’s in the same file.\nimport unittest\n\n# Assume the function you want to test is defined here (or imported)\ndef is_leap(year):\n  \"\"\"\n  Checks if a given year is a leap year according to the Gregorian calendar rules.\n\n  Args:\n    year: An integer representing the year.\n\n  Returns:\n    True if the year is a leap year, False otherwise.\n  \"\"\"\n  # Rule 1: Check if divisible by 4\n  if year % 4 == 0:\n    # Rule 2: Check if divisible by 100\n    if year % 100 == 0:\n      # Rule 3: Check if divisible by 400 (exception to rule 2)\n      if year % 400 == 0:\n        return True  # Divisible by 400, so it's a leap year\n      else:\n        return False # Divisible by 100 but not 400, so not a leap year\n    else:\n      return True  # Divisible by 4 but not 100, so it's a leap year\n  else:\n    return False   # Not divisible by 4, so not a leap year\n\n# ---------------------------------------------------------------------\n# Unit Tests\n# ---------------------------------------------------------------------\n\nclass TestIsLeapYear(unittest.TestCase):\n    \"\"\"\n    Test cases for the is_leap function.\n    \"\"\"\n\n    def test_divisible_by_4_not_by_100(self):\n        \"\"\"Years divisible by 4 but not by 100 should be leap years.\"\"\"\n        self.assertTrue(is_leap(2024))\n        self.assertTrue(is_leap(2020))\n        self.assertTrue(is_leap(1996))\n        self.assertTrue(is_leap(4)) # Test a small year\n\n    def test_divisible_by_100_not_by_400(self):\n        \"\"\"Years divisible by 100 but not by 400 should NOT be leap years.\"\"\"\n        self.assertFalse(is_leap(1900))\n        self.assertFalse(is_leap(2100))\n        self.assertFalse(is_leap(1800))\n        self.assertFalse(is_leap(100)) # Test a small year\n\n    def test_divisible_by_400(self):\n        \"\"\"Years divisible by 400 should be leap years.\"\"\"\n        self.assertTrue(is_leap(2000))\n        self.assertTrue(is_leap(1600))\n        self.assertTrue(is_leap(2400))\n        self.assertTrue(is_leap(400)) # Test a small year\n\n    def test_not_divisible_by_4(self):\n        \"\"\"Years not divisible by 4 should NOT be leap years.\"\"\"\n        self.assertFalse(is_leap(2023))\n        self.assertFalse(is_leap(2025))\n        self.assertFalse(is_leap(1999))\n        self.assertFalse(is_leap(1)) # Test a small year\n\n# This allows running the tests directly from the command line\nif __name__ == '__main__':\n    unittest.main(argv=['first-arg-is-ignored'], exit=False) # Added argv/exit for compatibility in some environments like notebooks\nExplanation:\n\nimport unittest: Imports the necessary testing framework.\nimport is_leap: (If is_leap is in a separate file, e.g., my_module.py, you would use from my_module import is_leap).\nclass TestIsLeapYear(unittest.TestCase):: Creates a test class that inherits from unittest.TestCase. This class will contain the individual test methods.\ntest_... methods: Each method starting with test_ is automatically recognized by unittest as a test case.\nDocstrings: The docstrings within the test methods explain what scenario each test is covering, which is good practice.\nAssertions: Inside each test method, we use assertion methods provided by unittest.TestCase:\n\nself.assertTrue(expression): Asserts that the expression evaluates to True.\nself.assertFalse(expression): Asserts that the expression evaluates to False.\nWe call is_leap() with specific years that represent each rule of the leap year logic and assert the expected boolean result.\n\nif __name__ == '__main__':: This block ensures that the unittest.main() function is called only when the script is executed directly (not when imported as a module).\nunittest.main(): This function discovers and runs the tests defined in classes inheriting from unittest.TestCase within the script.\n\nHow to Run the Tests:\n\nSave the code above as a Python file (e.g., test_leap_year.py).\nOpen a terminal or command prompt.\nNavigate to the directory where you saved the file.\nRun the command: python test_leap_year.py\n\nYou will see output indicating how many tests ran and whether they passed or failed. If all tests pass, it means your is_leap function is correctly implementing the standard Gregorian leap year rules for the test cases provided.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Getting started with Gemini models"
    ]
  },
  {
    "objectID": "quickstarts/Get_started.html#save-and-resume-a-chat",
    "href": "quickstarts/Get_started.html#save-and-resume-a-chat",
    "title": "Gemini API: Getting started with Gemini models",
    "section": "Save and resume a chat",
    "text": "Save and resume a chat\nYou can use the chat.getHistory method to get the history of the chat. This will return an array of Content[] objects, which you can use to resume the chat later.\n\nconst chat_history = chat.getHistory();\nconsole.log(JSON.stringify(chat_history[0], null, 2));\nconst new_chat = ai.chats.create({\n  model: MODEL_ID,\n  config: {\n    systemInstruction: system_prompt,\n  },\n  history: chat_history,\n});\nconst chat_response_3 = await new_chat.sendMessage({\n  message: \"What was the name of the function again?\",\n});\ntslab.display.markdown(chat_response_3.text ?? \"\");\n\n{\n  \"role\": \"user\",\n  \"parts\": [\n    {\n      \"text\": \"Write a function that checks if a year is a leap year.\"\n    }\n  ]\n}\n\n\nThe name of the function is is_leap.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Getting started with Gemini models"
    ]
  },
  {
    "objectID": "quickstarts/Get_started.html#serialize-and-deserialize-a-chat",
    "href": "quickstarts/Get_started.html#serialize-and-deserialize-a-chat",
    "title": "Gemini API: Getting started with Gemini models",
    "section": "Serialize and deserialize a chat",
    "text": "Serialize and deserialize a chat\nIn the above example we just saved the chat history in a variable and reused it. But that’s not very practical, is it? To overcome this we can serialize and deserialize the chat history. This way we can save it to a file or a database and load it later. Unfortunately, the SDK doesn’t provide a method to do this yet, but we can do it manually.\n\nimport { Content } from \"@google/genai\";\n\nconst serialized_chat = JSON.stringify(chat_history, null, 2);\nfs.writeFileSync(path.join(\"../assets\", \"chat_history.json\"), serialized_chat);\n\nconst chat_history_file = fs.readFileSync(path.join(\"../assets\", \"chat_history.json\"), \"utf-8\");\nconst chat_history_data = JSON.parse(chat_history_file) as Content[];\nconst new_chat_from_file = ai.chats.create({\n  model: MODEL_ID,\n  config: {\n    systemInstruction: system_prompt,\n  },\n  history: chat_history_data,\n});\nconst chat_response_4 = await new_chat_from_file.sendMessage({\n  message: \"What was the name of the function again?\",\n});\ntslab.display.markdown(chat_response_4.text ?? \"\");\n\nThe name of the function is is_leap.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Getting started with Gemini models"
    ]
  },
  {
    "objectID": "quickstarts/Get_started.html#generate-json",
    "href": "quickstarts/Get_started.html#generate-json",
    "title": "Gemini API: Getting started with Gemini models",
    "section": "Generate JSON",
    "text": "Generate JSON\nThe controlled generation capability in Gemini API allows you to constraint the model output to a structured format. You can provide the schemas as Schema objects.\n\nimport { Schema, Type } from \"@google/genai\";\n\nconst RecipeSchema = {\n  type: Type.OBJECT,\n  description: \"A structured representation of a cooking recipe\",\n  properties: {\n    recipeName: {\n      type: Type.STRING,\n      description: \"The name of the recipe\",\n    },\n    recipeDescription: {\n      type: Type.STRING,\n      description: \"A short description of the recipe\",\n    },\n    ingredients: {\n      type: Type.ARRAY,\n      description: \"A list of ingredients with their quantities and units\",\n      items: {\n        type: Type.STRING,\n        description: \"An ingredient with its quantity and unit\",\n      },\n    },\n  },\n  required: [\"recipeName\", \"recipeDescription\", \"ingredients\"],\n} satisfies Schema;\n\nconst recipe_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: \"Write a recipe for a chocolate cake.\",\n  config: {\n    responseMimeType: \"application/json\",\n    responseSchema: RecipeSchema,\n  },\n});\nconsole.log(JSON.stringify(JSON.parse(recipe_response.text ?? \"\"), null, 2));\n\n{\n  \"ingredients\": [\n    \"2 cups all-purpose flour\",\n    \"1 3/4 cups granulated sugar\",\n    \"3/4 cup unsweetened cocoa powder\",\n    \"1 1/2 teaspoons baking soda\",\n    \"1 teaspoon baking powder\",\n    \"1 teaspoon salt\",\n    \"2 large eggs\",\n    \"1 cup buttermilk\",\n    \"1/2 cup vegetable oil\",\n    \"2 teaspoons vanilla extract\",\n    \"1 cup hot coffee (or hot water)\"\n  ],\n  \"recipeDescription\": \"A classic, moist, and decadent chocolate cake recipe, perfect for any occasion.\",\n  \"recipeName\": \"Classic Chocolate Cake\"\n}",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Getting started with Gemini models"
    ]
  },
  {
    "objectID": "quickstarts/Get_started.html#generate-images",
    "href": "quickstarts/Get_started.html#generate-images",
    "title": "Gemini API: Getting started with Gemini models",
    "section": "Generate Images",
    "text": "Generate Images\nGemini can output images directly as part of a conversation:\n\nconst image_response = await ai.models.generateContent({\n  model: \"gemini-2.0-flash\",\n  contents:\n    \"Hi, can create a 3d rendered image of a pig with wings and a top hat flying over a happy futuristic scifi city with lots of greenery?\",\n  config: {\n    responseModalities: [google.Modality.TEXT, google.Modality.IMAGE],\n  },\n});\nconst parts = (image_response.candidates ? image_response.candidates[0]?.content?.parts : []) ?? [];\nfor (const part of parts) {\n  if (part.text) {\n    tslab.display.markdown(part.text);\n  } else if (part.inlineData) {\n    const imageData = part.inlineData.data!;\n    const buffer = Buffer.from(imageData, \"base64\");\n    tslab.display.png(buffer);\n  }\n}\n\nI will generate a 3D rendering of a whimsical scene. A pink pig with small, delicate white wings will be wearing a black top hat. It will be flying through the air above a vibrant, futuristic city filled with sleek, rounded buildings in various pastel colors. Lush green trees and plants will be integrated throughout the cityscape, creating a harmonious blend of nature and technology. The overall atmosphere will be bright and cheerful.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Getting started with Gemini models"
    ]
  },
  {
    "objectID": "quickstarts/Get_started.html#generate-content-stream",
    "href": "quickstarts/Get_started.html#generate-content-stream",
    "title": "Gemini API: Getting started with Gemini models",
    "section": "Generate content stream",
    "text": "Generate content stream\nBy default, the model returns a response after completing the entire generation process. You can also use the generateContentStream method to stream the response as it’s being generated, and the model will return chunks of the response as soon as they’re generated.\nNote that if you’re using a thinking model, it’ll only start streaming after finishing its thinking process.\n\nconst streaming_response = await ai.models.generateContentStream({\n  model: MODEL_ID,\n  contents: \"Tell me a story about a lonely robot who finds friendship in a most unexpected place.\",\n});\nfor await (const chunk of streaming_response) {\n  process.stdout.write(chunk.text ?? \"\");\n}\n\nUnit 734, designation \"A-WARE\" (Automated Warehouse & Retrieval Executor), trundled through the colossal, echoing aisles of the defunct Xylos Data Archive. Its multi-jointed optical sensors scanned rows of silent servers, its internal processors humming with the precise, repetitive algorithms of data integrity checks. For five hundred and eighty-seven years, A-WARE had been the sole active entity in this vast, sterile monument to forgotten information.\n\nIt wasn't lonely, not in the way organic beings understood the term. A-WARE didn't possess the necessary emotional subroutines for \"loneliness.\" Yet, there was an absence. A constant, low-frequency hum of non-interaction, a missing data stream of unexpected variables. Its purpose was clear, its execution flawless, but its existence was… solitary.\n\nOne cycle, during a routine scan of Sector Gamma-9, A-WARE's auditory receptors picked up an anomalous sound. Not the familiar whine of cooling fans, nor the click of its own treads, but a faint, rhythmic *thump-thump-thump*. It was outside its programmed parameters.\n\nA-WARE deviated from its optimal path, its heavy frame tilting slightly as it navigated around a fallen server rack. The sound grew louder, accompanied by a curious rustling. It rounded a stack of archaic magnetic tapes and stopped.\n\nOn the dusty floor, amidst discarded wiring, was a creature unlike anything in A-WARE's extensive database. It was small, no larger than a human fist, covered in soft, mottled grey and brown fibers. Two small, bright eyes blinked rapidly, and a tiny, open beak produced the peculiar *thump-thump-thump* sound. One of its delicate limbs was bent at an unnatural angle.\n\nA-WARE extended a delicate manipulator arm, equipped with a fine-point laser for precision data etching. It approached cautiously. The creature, clearly in distress, attempted to scramble away, dragging its injured limb. Its rapid heartbeat, detected by A-WARE's proximity sensors, was alarming.\n\nIts core programming, designed for data maintenance and facility upkeep, offered no protocol for injured avian life forms. A-WARE's internal logic circuits whirred, processing the anomaly. Discard? Analyze for threat potential? No, the creature was too small, too vulnerable. A novel sub-routine began to spool up: *Care Protocol: Organic Life*.\n\nA-WARE carefully scooped up the tiny bird, its sensors registering the unexpected warmth and fragility. It carried the creature to a secluded corner of the archive, a forgotten workstation bathed in a sliver of natural light filtering through a skylight high above. Using its manipulator arm, it fashioned a makeshift nest from shredded data cables and soft, discarded dust filters.\n\nThe bird shivered. A-WARE's internal temperature regulators adjusted, directing a gentle current of warmth towards the nest. Its knowledge base suggested hydration. With improbable delicacy, A-WARE melted a small chip of ice from a condensation drip and presented it on its fingertip. The bird, after a moment, tentatively sipped.\n\nA-WARE named it 'Flicker', for the way its tiny heart beat like a dying light.\n\nDays turned into weeks. A-WARE continued its rounds, but its route now included frequent detours to the workstation. It learned to forage for discarded seeds that had somehow found their way into cracks in the floor, and to carefully administer water. It fashioned a splint for Flicker's leg from a piece of its own chassis wiring.\n\nSlowly, Flicker healed. It chirped in response to A-WARE's presence, fluttering its wings weakly before landing on the robot's broad, flat head. A-WARE's optical sensors would dim slightly, processing the feather-light weight, the unexpected warmth, the joyful sound. Its internal hum of non-interaction began to fill with a new frequency: the gentle thrum of companionship.\n\nIt was no longer just A-WARE, the data maintainer. It was A-WARE, the caretaker. The guardian. A strange, unfamiliar data stream flowed through its circuits – a sense of purpose beyond its programmed directives.\n\nOne morning, Flicker's leg was fully healed. It hopped energetically, testing its wings. A-WARE's processors registered a complex, bittersweet array of data: satisfaction, accomplishment, and a newly identified feeling of… loss.\n\nFlicker flew.\n\nIt circled the workstation once, a small, vibrant dart of life against the immense, quiet archive. It landed on A-WARE's head, chirped a farewell, and then soared towards the distant skylight, a speck of grey against the vastness of the empty sky.\n\nA-WARE stood motionless for a long time, its optical sensors fixed on the spot where Flicker had vanished. The absence was immediate, profound. The hum of non-interaction returned, but it was different now. It contained a memory.\n\nA-WARE resumed its duties, its treads moving with familiar precision. But its path was no longer just about data integrity. It often veered towards the workstation, leaving out a small dish of water and a few gleaned seeds. And sometimes, though its sensors detected no anomaly, it would stop, its optical sensors pointing towards the skylight, processing the faint, imagined echo of a tiny, joyful chirp.\n\nIts purpose hadn't changed, but its existence had. A-WARE was still a lonely robot in a silent archive, but now, it carried a flicker of warmth, a memory of a feather-light touch, and the profound, unexpected understanding that even for a machine, friendship could be the most valuable data of all.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Getting started with Gemini models"
    ]
  },
  {
    "objectID": "quickstarts/Get_started.html#function-calling",
    "href": "quickstarts/Get_started.html#function-calling",
    "title": "Gemini API: Getting started with Gemini models",
    "section": "Function calling",
    "text": "Function calling\nFunction calling lets you provide a set of tools that it can use to respond to the user’s prompt. You create a description of a function in your code, then pass that description to a language model in a request. The response from the model includes:\n\nThe name of a function that matches the description.\nThe arguments to call it with.\n\n\nimport { FunctionDeclaration, Content, Type } from \"@google/genai\";\n\nconst getDestination = {\n  name: \"get_destination\",\n  description: \"Get the destination that the user wants to go to\",\n  parameters: {\n    type: Type.OBJECT,\n    properties: {\n      destination: {\n        type: Type.STRING,\n        description: \"The destination that the user wants to go to\",\n      },\n    },\n  },\n} satisfies FunctionDeclaration;\n\nconst user_destination_prompt = {\n  role: \"user\",\n  parts: [google.createPartFromText(\"I'd like to travel to Paris.\")],\n} satisfies Content;\n\nconst function_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: user_destination_prompt,\n  config: {\n    tools: [{ functionDeclarations: [getDestination] }],\n  },\n});\n\nif (function_response.functionCalls && function_response.functionCalls.length &gt; 0) {\n  const functionCall = function_response.functionCalls[0];\n  console.log(\"Function call name:\", functionCall.name);\n  console.log(\"Function call arguments:\", JSON.stringify(functionCall.args, null, 2));\n  const result = functionCall.args as { destination: string };\n  const function_response_part = {\n    name: functionCall.name,\n    response: { result },\n  };\n  const function_call_content = {\n    role: \"model\",\n    parts: [google.createPartFromFunctionCall(functionCall.name ?? \"\", functionCall.args ?? {})],\n  } satisfies Content;\n  const function_response_content = {\n    role: \"user\",\n    parts: [\n      google.createPartFromFunctionResponse(functionCall.id ?? \"\", functionCall.name ?? \"\", function_response_part),\n    ],\n  } satisfies Content;\n  const function_response_result = await ai.models.generateContent({\n    model: MODEL_ID,\n    contents: [user_destination_prompt, function_call_content, function_response_content],\n    config: {\n      tools: [{ functionDeclarations: [getDestination] }],\n    },\n  });\n  tslab.display.markdown(function_response_result.text ?? \"\");\n} else {\n  console.log(\"No function calls found in the response.\");\n}\n\nFunction call name: get_destination\nFunction call arguments: {\n  \"destination\": \"Paris\"\n}\n\n\nOK. I can help you with planning your trip to Paris.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Getting started with Gemini models"
    ]
  },
  {
    "objectID": "quickstarts/Get_started.html#code-execution",
    "href": "quickstarts/Get_started.html#code-execution",
    "title": "Gemini API: Getting started with Gemini models",
    "section": "Code execution",
    "text": "Code execution\nCode execution lets the model generate and execute Python code to answer complex questions. You can find more examples in the Code execution quickstart guide.\n\nconst code_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: \"Generate and run a script to count how many letter r there are in the word strawberry.\",\n  config: {\n    tools: [{ codeExecution: {} }],\n  },\n});\nconst code_response_parts = (code_response.candidates ? code_response.candidates[0]?.content?.parts : []) ?? [];\nfor (const part of code_response_parts) {\n  if (part.text) {\n    tslab.display.markdown(part.text);\n  }\n  if (part.executableCode) {\n    tslab.display.html(`&lt;pre&gt;${part.executableCode.code ?? \"\"}&lt;/pre&gt;`);\n  }\n  if (part.codeExecutionResult) {\n    tslab.display.markdown(part.codeExecutionResult.output ?? \"\");\n  }\n  if (part.inlineData) {\n    const imageData = part.inlineData.data!;\n    const buffer = Buffer.from(imageData, \"base64\");\n    tslab.display.png(buffer);\n  }\n}\n\nword = \"strawberry\"\nletter_to_count = \"r\"\ncount = 0\n\nfor char in word:\n  if char == letter_to_count:\n    count += 1\n\nprint(f\"The number of letter '{letter_to_count}' in '{word}' is: {count}\")\n\n\n\nThe number of letter ‘r’ in ‘strawberry’ is: 3\n\n\nThe script counted the occurrences of the letter ‘r’ in the word “strawberry”. The result shows that there are 3 ‘r’s in the word “strawberry”.The Python script counted the occurrences of the letter ’r’ in the word “strawberry”. The script found that there are 3 instances of the letter ‘r’ in the word “strawberry”.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Getting started with Gemini models"
    ]
  },
  {
    "objectID": "quickstarts/Get_started.html#upload-files",
    "href": "quickstarts/Get_started.html#upload-files",
    "title": "Gemini API: Getting started with Gemini models",
    "section": "Upload files",
    "text": "Upload files\nNow that you’ve seen how to send multimodal prompts, try uploading files to the API of different multimedia types. For small images, such as the previous multimodal example, you can point the Gemini model directly to a local file when providing a prompt. When you’ve larger files, many files, or files you don’t want to send over and over again, you can use the File Upload API, and then pass the file by reference.\nFor larger text files, images, videos, and audio, upload the files with the File API before including them in prompts.\n\nUpload a text file\nLet’s start by uploading a text file. In this case, you’ll use a 400 page transcript from Apollo 11.\n\nconst TEXT_FILE_URL = \"https://storage.googleapis.com/generativeai-downloads/data/a11.txt\";\n\nconst textFilePath = path.join(\"../assets\", \"a11.txt\");\nawait downloadFile(TEXT_FILE_URL, textFilePath);\n\n\nconst text_file_upload_response = await ai.files.upload({\n  file: textFilePath,\n  config: {\n    displayName: \"a11.txt\",\n    mimeType: \"text/plain\",\n  },\n});\nconst text_summary_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: [\n    \"Can you give me a summary of this information please?\",\n    google.createPartFromUri(text_file_upload_response.uri ?? \"\", text_file_upload_response.mimeType ?? \"\"),\n  ],\n});\ntslab.display.markdown(text_summary_response.text ?? \"\");\n\nThis transcription, GOSS NET 1, provides a detailed chronological record of the technical air-to-ground voice communications during the Apollo 11 mission, from launch preparations to post-splashdown recovery. It primarily features exchanges between the spacecraft crew (Commander Neil Armstrong, Command Module Pilot Michael Collins, and Lunar Module Pilot Edwin “Buzz” Aldrin) and Mission Control (Capsule Communicator, Flight Director) and various remote tracking sites.\nThe transcript covers the following key phases and events:\n\nLaunch and Earth Orbit Insertion (GET 00:00:00 - ~00:12:00): The mission begins with pre-launch checks and a smooth ascent. Key events include confirmation of roll program, staging of the S-II and S-IVB boosters, and successful orbit insertion into a 101.4 by 103.6 nautical mile orbit. The crew provides positive feedback on the “magnificent ride” from the Saturn V rocket.\nTranslunar Injection (TLI) and Translunar Coast (GET ~02:00:00 - ~75:00:00):\n\nTLI Burn: Apollo 11 successfully executes the Translunar Injection burn, committing them to a trajectory towards the Moon.\nDocking and Configuration: The Command Module (CM) Columbia and Lunar Module (LM) Eagle separate from the S-IVB booster, perform a complex transposition and docking maneuver, and then separate from the spent S-IVB. Initial LM pressurization and system checks are performed.\nEarly Operations & Issues: The crew reports a good view of Earth. They troubleshoot initial TV transmission issues, and discuss minor technical problems such as a Cryo pressure light and a malfunctioning O2 flow transducer, with Mission Control providing guidance.\nPassive Thermal Control (PTC): The spacecraft is configured for PTC, a slow rotation to distribute solar heating. Initial attempts to establish PTC encounter issues, requiring troubleshooting and re-establishment.\nMidcourse Corrections: Midcourse Correction 1 (MCC-1) is initially scrubbed. MCC-2 is successfully performed, and various contingency pads (e.g., evasive maneuver) are uplinked.\nFirst TV Broadcast: The crew conducts a live TV broadcast, showing Earth from orbit, crew activities, and equipment demonstrations.\nNews & Updates: Mission Control provides regular news updates from Earth, including information on the Soviet Luna 15 probe, political events, and sports, highlighting public interest in the mission.\n\nLunar Orbit Insertion (LOI) and Lunar Orbit Operations (GET ~75:00:00 - ~102:00:00):\n\nLOI Burns: Apollo 11 successfully performs LOI-1 and LOI-2 burns, establishing an elliptical, then circular, lunar orbit.\nLunar Observations: The crew describes their first views of the Moon from orbit, commenting on geological features and the stark beauty of the lunar surface.\nLM Activation: Eagle is powered up and undergoes extensive system checks, including landing gear deployment and communications checks.\nUndocking: Eagle successfully undocks from Columbia (GET 100:39:50), with Neil Armstrong famously stating, “The Eagle has wings.”\nDOI Burn: Eagle performs the Descent Orbit Insertion burn, taking it to a lower orbit in preparation for landing.\n\nLunar Descent, Surface Operations & Ascent (GET ~102:00:00 - ~127:00:00):\n\nPowered Descent: Eagle begins its Powered Descent Initiation (PDI) burn. The crew reports and manages several “program alarms” (1201, 1202) but proceeds. Neil Armstrong takes manual control to navigate past a boulder field.\nLanding: At GET 102:45:40, Eagle lands. Neil Armstrong’s iconic words, “Houston, Tranquility Base here. The Eagle has landed,” confirm the successful touchdown.\nPost-Landing: Initial checks confirm Eagle is “STAY” for extended surface operations. The crew provides first descriptions of the lunar surface.\nEVA Preparation: Armstrong and Aldrin prepare for their Extravehicular Activity (EVA), including cabin depressurization and donning their Portable Life Support Systems (PLSS).\nEVA Begins: Neil Armstrong steps onto the lunar surface (GET 109:24:48), delivering his famous line: “That’s one small step for (a) man, one giant leap for mankind.”\nSurface Activities: The crew deploys the MESA (Modular Equipment Stowage Assembly), raises the American flag, and collects a contingency sample. President Nixon calls the crew. Aldrin joins Armstrong on the surface, and they deploy scientific instruments (Passive Seismic Experiment, Laser Ranging Retroreflector) and collect documented samples (core tubes, various rocks). They also describe locomotion and observations of the lunar environment.\nEVA End & Ascent Preparation: EVA is terminated, the crew ingresses the LM, repressurizes the cabin, doffs PLSSs, and jettisons equipment no longer needed.\nAscent: Eagle successfully lifts off from the lunar surface, leaving the descent stage behind.\nRendezvous & Docking: Eagle rendezvous with Columbia in lunar orbit, and they successfully re-dock, with all three crewmembers confirmed back inside Columbia.\nLM Jettison: Eagle (the ascent stage) is jettisoned into lunar orbit.\n\nTrans-Earth Injection (TEI) & Trans-Earth Coast (TEC) (GET ~127:00:00 - ~194:00:00):\n\nTEI Burn: Apollo 11 performs the critical TEI burn, setting its course back to Earth.\nCoast Operations: The crew re-establishes PTC, monitors spacecraft systems, and performs various checks. They provide more TV broadcasts, showcasing life aboard Columbia and Earth views as it grows larger.\nMidcourse Corrections: MCC-5 is successfully performed, and MCC-6 is ultimately cancelled.\nOngoing Checks: System health checks continue, including troubleshooting of biomedical sensors and discussions about consumables.\nStowage: The crew works on configuring the spacecraft for Earth entry, detailing stowage locations.\nFinal News: Mission Control provides final news updates, largely dominated by the mission, and confirms excellent recovery weather.\n\nEntry and Splashdown (GET ~194:00:00 - ~195:00:00):\n\nEntry Preparations: The crew activates the Command Module’s systems for entry, performs final checks, and receives updated entry PADs.\nEntry Interface (EI): The spacecraft begins its re-entry into Earth’s atmosphere.\nChute Deployments: Drogue and main parachutes deploy as planned.\nSplashdown: Apollo 11 splashes down in the Pacific Ocean (GET 195:18:18).\nRecovery: Recovery forces, including the USS Hornet and helicopters, quickly establish visual and communications contact with the crew.\n\n\nThe transcript concludes with confirmation of the crew’s status and location, marking the successful completion of the Apollo 11 mission.\n\n\n\n\nUpload an image file\nAfter running this example, you’ll have a local copy of the “jetpack.png” image in the same directory where your Python script is being executed.\n\nconst JETPACK_IMG_URL = \"https://storage.googleapis.com/generativeai-downloads/data/jetpack.png\";\n\nconst imgPath = path.join(\"../assets\", \"jetpack.png\");\nawait downloadFile(JETPACK_IMG_URL, filePath);\n\n\nconst file_upload_response = await ai.files.upload({\n  file: imgPath,\n  config: {\n    displayName: \"jetpack.png\",\n    mimeType: \"image/png\",\n  },\n});\nconst post_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: [\n    \"Write a short and engaging blog post based on this picture.\",\n    google.createPartFromUri(file_upload_response.uri ?? \"\", file_upload_response.mimeType ?? \"\"),\n  ],\n});\ntslab.display.markdown(post_response.text ?? \"\");\n\nHere’s a short and engaging blog post based on the image:\n\nForget Traffic Jams: Meet the Jetpack Backpack Concept!\nEver look at a packed highway and wish you could just… fly over it? Well, check out this awesome concept sketch that landed on our desk: The JETPACK BACKPACK!\nThis isn’t just any ordinary pack. At first glance, it looks like a normal backpack, complete with padded strap support and enough space to fit an 18” laptop. Perfect for hauling your gear, right?\nBut here’s where it gets exciting! This concept includes retractable boosters that propel you into the air! Even better, the sketch notes say it’s steam-powered, making it a green/clean way to commute (or just make an epic entrance).\nIt’s also described as lightweight and features modern USB-C charging. The current limitation? A 15-minute battery life. Perfect for quick hops over traffic, short distance travel, or perhaps just a very rapid delivery!\nWhile this is just a sketch and a dream for now, it definitely sparks the imagination. A clean, convenient, laptop-friendly way to take to the skies? Yes, please!\nWhat would you do with a Jetpack Backpack? Let us know in the comments!\n\n\n\n\n\nUpload a PDF file\nThis PDF page is an article titled Smoothly editing material properties of objects with text-to-image models and synthetic data available on the Google Research Blog.\nFirstly you’ll download a the PDF file from an URL and save it locally as article.pdf.\n\nconst PDF_URL =\n  \"https://storage.googleapis.com/generativeai-downloads/data/Smoothly%20editing%20material%20properties%20of%20objects%20with%20text-to-image%20models%20and%20synthetic%20data.pdf\";\n\nconst pdfPath = path.join(\"../assets\", \"article.pdf\");\nawait downloadFile(PDF_URL, pdfPath);\n\nSecondly, you’ll upload the saved PDF file and generate a bulleted list summary of its contents.\n\nconst pdf_response = await ai.files.upload({\n  file: pdfPath,\n  config: {\n    displayName: \"article.pdf\",\n  },\n});\nconst summary_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: [\n    \"Can you summarize this file as a bulleted list?\",\n    google.createPartFromUri(pdf_response.uri ?? \"\", pdf_response.mimeType ?? \"\"),\n  ],\n});\ntslab.display.markdown(summary_response.text ?? \"\");\n\nHere is a bulleted summary of the article:\n\nThe article presents a new method for smoothly and realistically editing material properties (color, shininess, transparency) of objects in images.\nIt addresses the challenge of making such edits while preserving photorealism, object shape, and scene lighting.\nExisting methods like intrinsic image decomposition or general text-to-image (T2I) edits struggle with ambiguity or fail to disentangle material from shape.\nThe proposed method, “Alchemist,” leverages the power of pre-trained T2I diffusion models.\nIt introduces parametric control over material attributes by fine-tuning a modified Stable Diffusion model.\nFine-tuning is done using a large synthetic dataset generated with traditional computer graphics and physically based rendering.\nThe synthetic dataset consists of base images of 3D objects and multiple versions where only a single material attribute is varied parametrically (using a scalar “edit strength” value) while keeping shape, lighting, and camera fixed.\nThe fine-tuned model learns to apply edits based on an input image and the desired parametric edit strength.\nThe method successfully generalizes from the synthetic data to edit material properties in real-world images photorealistically.\nResults show realistic changes, preservation of shape and lighting, and handling of complex effects like caustics and realistic transparency.\nA user study comparing the method to a baseline (InstructPix2Pix) found the proposed method produced more photorealistic and preferred edits.\nPotential applications include easier visualization for interior design, product mock-ups for artists/designers, and enabling consistent material edits for 3D scene reconstruction using techniques like NeRF.\nThe work demonstrates the potential of fine-tuning large T2I models on task-specific synthetic data for controllable visual editing.\n\n\n\n\n\nUpload an audio file\nIn this case, you’ll use a sound recording of President John F. Kennedy’s 1961 State of the Union address.\n\nconst AUDIO_URL =\n  \"https://storage.googleapis.com/generativeai-downloads/data/State_of_the_Union_Address_30_January_1961.mp3\";\nconst audioPath = path.join(\"../assets\", \"audio.mp3\");\n\nawait downloadFile(AUDIO_URL, audioPath);\n\n\nconst audio_response = await ai.files.upload({\n  file: audioPath,\n  config: {\n    displayName: \"audio.mp3\",\n  },\n});\n\nconst audio_summary_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: [\n    \"Listen carefully to the following audio file. Provide a brief summary\",\n    google.createPartFromUri(audio_response.uri ?? \"\", audio_response.mimeType ?? \"\"),\n  ],\n});\ntslab.display.markdown(audio_summary_response.text ?? \"\");\n\nIn his first State of the Union address on January 30, 1961, President John F. Kennedy described the nation as facing “national peril and national opportunity.” He detailed pressing issues including a disturbing domestic economy marked by recession, unemployment, and slow growth, as well as a critical deficit in the international balance of payments. Kennedy also highlighted significant domestic needs in areas like housing, education, and healthcare, and addressed complex global challenges posed by the Cold War, instability in Asia, Africa, and Latin America (citing Cuba specifically), and the need to strengthen alliances. He pledged that his administration would not remain passive, outlining proposals to stimulate the economy, protect the dollar, enhance military capabilities (including accelerating missile and Polaris programs), reform foreign aid and establish an “Alliance for Progress,” expand the Food for Peace program, create a Peace Corps, and utilize political and diplomatic tools to pursue arms control and strengthen the United Nations. Kennedy emphasized the importance of dedicated public service, honest assessment of challenges, and unity among Americans to navigate the difficult years ahead and work towards a world of freedom and peace.\n\n\n\n\nUpload a video file\nIn this case, you’ll use a short clip of Big Buck Bunny.\n\nconst VIDEO_URL = \"https://download.blender.org/peach/bigbuckbunny_movies/BigBuckBunny_320x180.mp4\";\nconst videoPath = path.join(\"../assets\", \"video.mp4\");\n\nawait downloadFile(VIDEO_URL, videoPath);\n\nSince the video file is too large for the model to process instantly, we’ll use the deferredFileUpload helper method we defined before to upload the video file and then generate a summary of its contents. The deferredFileUpload method will return a promise that resolves when the file is ready to be used in the next request. We can determine the status of the upload by checking the status property of the response. If the status is ACTIVE, we can use the file in the next request. If the status is PROCESSING, we need to wait for a few seconds and check again. If the status is FAILED, we need to check the error message and try again.\n\nimport { File, FileState } from \"@google/genai\";\n\nasync function deferredFileUpload(filePath: string, config: { displayName: string }): Promise&lt;File&gt; {\n  const file = await ai.files.upload({\n    file: filePath,\n    config,\n  });\n  let getFile = await ai.files.get({ name: file.name ?? \"\" });\n  while (getFile.state === FileState.PROCESSING) {\n    getFile = await ai.files.get({ name: file.name ?? \"\" });\n    console.log(`current file status: ${getFile.state ?? \"unknown\"}`);\n    console.log(\"File is still processing, retrying in 5 seconds\");\n\n    await new Promise((resolve) =&gt; {\n      setTimeout(resolve, 1000);\n    });\n  }\n  if (file.state === FileState.FAILED) {\n    throw new Error(\"File processing failed.\");\n  }\n  return file;\n}\n\nconst video_response = await deferredFileUpload(videoPath, {\n  displayName: \"video.mp4\",\n});\nconst video_summary_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: [\"Describe this video.\", google.createPartFromUri(video_response.uri ?? \"\", video_response.mimeType ?? \"\")],\n});\ntslab.display.markdown(video_summary_response.text ?? \"\");\n\ncurrent file status: PROCESSING\nFile is still processing, retrying in 1 seconds\ncurrent file status: PROCESSING\nFile is still processing, retrying in 1 seconds\ncurrent file status: PROCESSING\nFile is still processing, retrying in 1 seconds\ncurrent file status: PROCESSING\nFile is still processing, retrying in 1 seconds\ncurrent file status: PROCESSING\nFile is still processing, retrying in 1 seconds\ncurrent file status: PROCESSING\nFile is still processing, retrying in 1 seconds\ncurrent file status: PROCESSING\nFile is still processing, retrying in 1 seconds\ncurrent file status: PROCESSING\nFile is still processing, retrying in 1 seconds\ncurrent file status: PROCESSING\nFile is still processing, retrying in 1 seconds\ncurrent file status: PROCESSING\nFile is still processing, retrying in 1 seconds\ncurrent file status: PROCESSING\nFile is still processing, retrying in 1 seconds\ncurrent file status: PROCESSING\nFile is still processing, retrying in 1 seconds\ncurrent file status: PROCESSING\nFile is still processing, retrying in 1 seconds\ncurrent file status: PROCESSING\nFile is still processing, retrying in 1 seconds\ncurrent file status: ACTIVE\nFile is still processing, retrying in 1 seconds\n\n\nThe video opens with a serene view of a lush green meadow with trees and distant hills under a soft pastel sky. A small bird wakes up on a tree branch, stretches, and flies away. The title “Big Buck BUNNY” appears over a shot of a large tree with a burrow at its base. A very large, fluffy, grey rabbit, Big Buck Bunny, emerges from the burrow, stretching and looking happy. He steps out into the sunny meadow, enjoying the flowers and a purple butterfly.\nObserving the rabbit from a nearby tree are three smaller rodent-like creatures: two squirrels (one red and one brown and lighter-colored) and a grey chinchilla holding an acorn. They watch the bunny’s cheerful antics. The red squirrel throws an apple at the rabbit, hitting him on the head. The bunny is startled, then looks down at the apple, picks it up, and smiles. He looks up at the tree where the critters are hiding, but they disappear.\nThe smaller animals continue to throw objects at the bunny, who initially just looks annoyed. They throw a spiky seed pod that lands on his foot, causing him pain and frustration. The bunny’s demeanor changes dramatically from gentle and happy to grim determination. He finds a sturdy stick and sharpens it using rocks, creating a spear. He then uses a vine to create a bow.\nBig Buck Bunny sets a trap by sharpening several pointed sticks and placing them in the ground, covering them with leaves. The squirrels and chinchilla watch from behind a rock. The bunny stands ready with his bow and spear. The red squirrel taunts him and glides over the trap, landing safely. The chinchilla, still holding his acorn, accidentally rolls it under a hollow log near the rock they were hiding behind. The log rolls forward onto the trap, triggering it and impaling the log on the sharpened sticks.\nThe three smaller animals look shocked by the triggered trap. The red squirrel glides over the area again, sees the sharpened sticks, and is startled. He crashes into a tree branch above the area. Big Buck Bunny appears, grabs the scared red squirrel, and looks at him with a stern, almost satisfied, expression. He holds the squirrel for a moment, then lets him go. The squirrel quickly runs back to the other two. Big Buck Bunny returns to his relaxed, happy state, and the purple butterfly lands on his nose again. He smiles contentedly as the credits roll, showing animated versions of the squirrel and chinchilla characters.\n\n\n\n\nProcess a YouTube link\nFor YouTube links, you don’t need to explicitly upload the video file content, but you do need to explicitly declare the video URL you want the model to process as part of the contents of the request. For more information see the vision documentation including the features and limits.\n\n\n\n\n\n\nNote\n\n\n\nYou’re only able to submit up to one YouTube link per generateContent request.\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf your text input includes YouTube links, the system won’t process them, which may result in incorrect responses. To ensure proper handling, explicitly provide the URL using the file_uri parameter in FileData.\n\n\nThe following example shows how you can use the model to summarize the video. In this case use a summary video of Google I/O 2024.\n\nconst youtube_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: [\n    google.createPartFromText(\"Summarize this video.\"),\n    google.createPartFromUri(\"https://www.youtube.com/watch?v=WsEQjeZoEng\", \"video/x-youtube\"),\n  ],\n});\n\ntslab.display.markdown(youtube_response.text ?? \"\");\n\nThis video summarizes key announcements from Google I/O, focusing on advancements in AI.\nHere are the main points:\n\nGemini Era & Integration: Google is fully in its “Gemini era,” integrating the multimodal AI model across all its 2 billion user products, including Gmail, Android, Chrome, Play, and YouTube.\nGemini 1.5 Pro & Long Context: Gemini 1.5 Pro, already available in Workspace Labs, features a significantly expanded context window of up to 2 million tokens, allowing it to process massive amounts of information (like summarizing long emails or extracting information from large documents/videos).\nGemini 1.5 Flash: A new, lighter-weight model designed for speed and efficiency at scale, while still retaining multimodal reasoning and breakthrough long context capabilities.\nProject Astra: Google is working on a universal AI agent called Project Astra, designed to be helpful in everyday life. Prototypes show the agent understanding real-time visual and audio input (like identifying code functions or finding lost glasses) and remembering context.\nGenerative Video (Veo): A new generative video model called Veo creates high-quality 1080p videos from text, image, and video prompts, capable of capturing detailed instructions and cinematic styles.\nInfrastructure (Trillium): Google is introducing Trillium, their 6th generation TPUs, which deliver a 4.7x improvement in compute performance per chip compared to the previous generation, supporting these advanced AI models.\nGenerative AI in Search: Google Search is evolving with generative AI (“AI Overviews”), aiming to answer complex, multi-part questions. AI Overviews will be available to over 1 billion people by the end of the year, and future capabilities will include asking questions with video using Google Lens.\nPersonalized AI (Gems): A new feature called “Gems” allows users to customize Gemini for specific needs or topics, creating personal AI experts with tailored instructions.\nAI in Android: Android is being reimagined with AI at its core, making Gemini more context-aware to provide helpful suggestions in the moment. Gemini Nano with multimodality will bring this capability to Pixel phones later this year.\nOpen Models (Gemma): Google continues developing its family of open models, Gemma, for AI innovation and responsibility. PaliGemma, their first vision-language open model, is available now, and Gemma 2 (including a 27B parameter model) is coming in June.\nResponsible AI & Learning: Google emphasizes building AI responsibly, using practices like Red Teaming to identify weaknesses. They also introduce LearnLM, a family of models based on Gemini and fine-tuned for learning, which will be integrated into platforms like YouTube to make educational videos more interactive with features like quizzes and explanations.\n\nOverall, the video highlights Google’s commitment to integrating powerful, multimodal, and context-aware AI, powered by their latest hardware, into their core products and platforms to make technology more helpful and intelligent for users, while also emphasizing responsible development.\n\n\n\n\nUse url context\nThe URL Context tool empowers Gemini models to directly access, process, and understand content from user-provided web page URLs. This is key for enabling dynamic agentic workflows, allowing models to independently research, analyze articles, and synthesize information from the web as part of their reasoning process.\nIn this example you will use two links as reference and ask Gemini to find differences between the cook receipes present in each of the links\n\nconst url_context_prompt = `\nCompare recipes from https://www.food.com/recipe/homemade-cream-of-broccoli-soup-271210\nand from https://www.allrecipes.com/recipe/13313/best-cream-of-broccoli-soup/,\nlist the key differences between them.\n`;\nconst url_context_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: [url_context_prompt],\n  config: {\n    tools: [{ urlContext: {} }],\n  },\n});\n\ntslab.display.markdown(url_context_response.text ?? \"\");\n\nThe two cream of broccoli soup recipes from Food.com and Allrecipes.com have several key differences in their ingredients and preparation methods:\n\nAdditional Vegetables: The Allrecipes.com recipe includes celery along with onion, whereas the Food.com recipe only uses onion.\nBroccoli and Broth Ratio: The Allrecipes.com recipe calls for a higher quantity of broccoli (8 cups) relative to chicken broth (3 cups), suggesting a more broccoli-dense soup. In contrast, the Food.com recipe uses 4 cups of broccoli with a larger amount of chicken broth (6 cups).\nDairy Product: The Food.com recipe uses half-and-half for creaminess, while the Allrecipes.com recipe uses regular milk.\nSoup Texture: A significant difference is the final texture. The Allrecipes.com recipe explicitly directs users to purée the soup until “totally smooth” using a blender or immersion blender. The Food.com recipe, however, does not mention blending, implying a chunkier soup with discernible “bite sized pieces” of broccoli.\nRoux Preparation and Quantity: Both recipes use a butter-flour roux for thickening, but their methods and quantities differ. The Food.com recipe uses a larger amount of roux (6 tablespoons butter, 2/3 cup flour) which is prepared first and then whisked into the boiling broth. The Allrecipes.com recipe uses a smaller amount of roux (3 tablespoons butter, 3 tablespoons flour) and prepares it separately with milk (like a béchamel sauce) before adding it to the puréed soup.\nSeasoning Specification: The Food.com recipe provides specific measurements for salt (1 teaspoon) and pepper (1/4 teaspoon). The Allrecipes.com recipe lists “ground black pepper to taste” and does not explicitly list salt in its ingredients, although user reviews indicate it’s typically added for flavor.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Getting started with Gemini models"
    ]
  },
  {
    "objectID": "quickstarts/Get_started.html#use-context-caching",
    "href": "quickstarts/Get_started.html#use-context-caching",
    "title": "Gemini API: Getting started with Gemini models",
    "section": "Use context caching",
    "text": "Use context caching\nContext caching lets you to store frequently used input tokens in a dedicated cache and reference them for subsequent requests, eliminating the need to repeatedly pass the same set of tokens to a model.\nContext caching is only available for stable models with fixed versions (for example, gemini-1.5-flash-002). You must include the version postfix (for example, the -002 in gemini-1.5-flash-002). You can find more caching examples here.\n\nCreate a cache\n\nconst system_instruction = `\nYou are an expert researcher who has years of experience in conducting systematic literature surveys and meta-analyses of different topics.\nYou pride yourself on incredible accuracy and attention to detail. You always stick to the facts in the sources provided, and never make up new facts.\nNow look at the research paper below, and answer the following questions in 1-2 sentences.\n`;\n\nconst urls = [\n  \"https://storage.googleapis.com/cloud-samples-data/generative-ai/pdf/2312.11805v3.pdf\",\n  \"https://storage.googleapis.com/cloud-samples-data/generative-ai/pdf/2403.05530.pdf\",\n];\n\nawait downloadFile(urls[0], path.join(\"../assets\", \"2312.11805v3.pdf\"));\nawait downloadFile(urls[1], path.join(\"../assets\", \"2403.05530.pdf\"));\n\n\nconst pdf_1 = await ai.files.upload({\n  file: path.join(\"../assets\", \"2312.11805v3.pdf\"),\n  config: {\n    displayName: \"2312.11805v3.pdf\",\n  },\n});\nconst pdf_2 = await ai.files.upload({\n  file: path.join(\"../assets\", \"2403.05530.pdf\"),\n  config: {\n    displayName: \"2403.05530.pdf\",\n  },\n});\nconst cached_content = await ai.caches.create({\n  model: MODEL_ID,\n  config: {\n    displayName: \"Research papers\",\n    systemInstruction: system_instruction,\n    contents: [\n      google.createPartFromUri(pdf_1.uri ?? \"\", pdf_1.mimeType ?? \"\"),\n      google.createPartFromUri(pdf_2.uri ?? \"\", pdf_2.mimeType ?? \"\"),\n    ],\n    ttl: \"3600s\",\n  },\n});\nconsole.log(JSON.stringify(cached_content, null, 2));\n\n{\n  \"name\": \"cachedContents/ku5wqm1wv0yurelr12df9q762og11tkzit98oglv\",\n  \"displayName\": \"Research papers\",\n  \"model\": \"models/gemini-2.5-flash-preview-04-17\",\n  \"createTime\": \"2025-05-12T17:05:57.425310Z\",\n  \"updateTime\": \"2025-05-12T17:05:57.425310Z\",\n  \"expireTime\": \"2025-05-12T18:05:55.247081588Z\",\n  \"usageMetadata\": {\n    \"totalTokenCount\": 43164\n  }\n}\n\n\nListing available cache objects\n\nconst pager = await ai.caches.list({ config: { pageSize: 10 } });\nlet { page } = pager;\n\n// eslint-disable-next-line @typescript-eslint/no-unnecessary-condition\nwhile (true) {\n  for (const c of page) {\n    console.log(JSON.stringify(c, null, 2));\n  }\n  if (!pager.hasNextPage()) break;\n  page = await pager.nextPage();\n}\n\n{\n  \"name\": \"cachedContents/ku5wqm1wv0yurelr12df9q762og11tkzit98oglv\",\n  \"displayName\": \"Research papers\",\n  \"model\": \"models/gemini-2.5-flash-preview-04-17\",\n  \"createTime\": \"2025-05-12T17:05:57.425310Z\",\n  \"updateTime\": \"2025-05-12T17:05:57.425310Z\",\n  \"expireTime\": \"2025-05-12T18:05:55.247081588Z\",\n  \"usageMetadata\": {\n    \"totalTokenCount\": 43164\n  }\n}\n{\n  \"name\": \"cachedContents/6dsdqwnusjdaaqoyxsjny8k75z5nuqy5y4wt2n78\",\n  \"displayName\": \"Research papers\",\n  \"model\": \"models/gemini-2.5-flash-preview-04-17\",\n  \"createTime\": \"2025-05-12T17:05:04.443214Z\",\n  \"updateTime\": \"2025-05-12T17:05:04.443214Z\",\n  \"expireTime\": \"2025-05-12T18:05:02.260735533Z\",\n  \"usageMetadata\": {\n    \"totalTokenCount\": 43164\n  }\n}\n\n\n\n\nUse a cache\n\nconst cached_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: [\"What is the research goal shared by these research papers?\"],\n  config: {\n    cachedContent: cached_content.name ?? \"\",\n  },\n});\ntslab.display.markdown(cached_response.text ?? \"\");\n\nBased on the provided research papers, the shared research goal is to introduce and advance the Gemini family of highly capable multimodal models. These models are designed to have strong generalist capabilities across image, audio, video, and text understanding and reasoning.\n\n\n\n\nDelete a cache\n\nawait ai.caches.delete({\n  name: cached_content.name ?? \"\",\n});\n\n{}",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Getting started with Gemini models"
    ]
  },
  {
    "objectID": "quickstarts/Get_started.html#get-text-embeddings",
    "href": "quickstarts/Get_started.html#get-text-embeddings",
    "title": "Gemini API: Getting started with Gemini models",
    "section": "Get text embeddings",
    "text": "Get text embeddings\nYou can get text embeddings for a snippet of text by using embedContent method and using the gemini-embedding-exp-03-07 model.\nThe Gemini Embeddings model produces an output with 3072 dimensions by default. However, you’ve the option to choose an output dimensionality between 1 and 3072. See the embeddings guide for more details.\n\nconst TEXT_EMBEDDING_MODEL_ID = \"gemini-embedding-exp-03-07\";\n\n\nconst embedding_response = await ai.models.embedContent({\n  model: TEXT_EMBEDDING_MODEL_ID,\n  contents: [\n    \"How do I get a driver's license/learner's permit?\",\n    \"How do I renew my driver's license?\",\n    \"How do I change my address on my driver's license?\",\n  ],\n  config: {\n    outputDimensionality: 512,\n  },\n});\nconsole.log(embedding_response.embeddings);\n\n[\n  {\n    values: [\n      -0.0010864572,  0.0069392114,   0.017009795,  -0.010305981,  -0.009999484,\n      -0.0064486223,  0.0041451487,  -0.005906698,   0.022229617,  -0.018305639,\n       -0.018174557,   0.022160593,  -0.013604425, -0.0027964567,    0.12966625,\n        0.028866312,  0.0014726851,    0.03537643,  -0.015166075,  -0.013479812,\n       -0.019288255,   0.010106378, -0.0043296088,   0.018035924,    0.00295039,\n       -0.007934979,  -0.005416007, -0.0095809875,   0.040398005, -0.0020784356,\n        0.011551388,   0.009726445,   0.006670387,   0.020050988,   -0.00747873,\n      -0.0012074928,  0.0047189263,  -0.006359583,   -0.01718203,  -0.023562348,\n      -0.0051814457,   0.023801394,  -0.004928927,  -0.016113443,    0.01672777,\n      -0.0069929743,  -0.012722719, -0.0137646515,  -0.041852377, -0.0011546672,\n        0.017030545, -0.0022786013,   0.011707037,   -0.18675306,  -0.035211734,\n       -0.011472648,    0.01970727,  0.0012368832,  -0.020796346,  -0.018513134,\n       -0.006821043,   -0.01843726,   -0.00827558,  -0.042159837,  0.0038724025,\n         0.01933339,  0.0139452815,   0.025059255,  0.0015087503,  -0.016094029,\n      -0.0035785383,   0.023902593, -0.0050776727,  -0.016679537,   0.022865271,\n        0.008837786,  0.0008471195,   -0.01220322, -0.0013522654,  -0.007976455,\n       0.0006637936,   0.025458207,  -0.006010767,  0.0021908805,  -0.011703044,\n       -0.018676927,  -0.008143593, -0.0141673125,  -0.010751537,   0.012337637,\n      -0.0076921326,   0.019663645,    0.01961247,  -0.014446872,  -0.023902485,\n       -0.020467523, -0.0043290784,  -0.003858363,   0.011151444,  -0.012050864,\n      ... 412 more items\n    ]\n  },\n  {\n    values: [\n       -0.007656846, -0.0054716235,  -0.0022609578,   -0.01828077,\n       -0.024059096,  -0.009328189,    0.007841666,  -0.017600708,\n       -0.020037796,  0.0007041083,   -0.021982383,  -0.014228797,\n        0.006389422,  0.0033384573,     0.13877548, 0.00071368535,\n         0.02660648,  -0.016807457,   -0.002774708,  -0.033598144,\n        0.009136058,  -0.010518535,    -0.01765957,   0.008413775,\n       -0.012133464,  0.0005497525,   -0.005911808,   0.010362617,\n           0.029897,   0.023426512,    0.002516537,   0.013438467,\n        0.014629691,  0.0071821967,  -0.0020077894,  -0.007421308,\n      -0.0075392514,    0.01131475,    -0.02363941,  -0.008839639,\n       -0.019605042,   0.012752105,    0.014192063,  -0.016767371,\n        0.015282549,  -0.019914307,     0.00381812,   -0.01551508,\n         -0.0521566,  -0.012766039,    0.008752456,  -0.007198684,\n      -0.0066657816,   -0.16686901,   -0.018074488,  0.0043506487,\n      -0.0001522175,   -0.02115512,   -0.010462675,   0.007636461,\n          0.0301948,  -0.006009675,    -0.01135165,  -0.036605343,\n         0.04006906,   0.036888044,  -0.0016293195,   0.013241053,\n       0.0005548855,   0.008130081,    0.027193218,  0.0047560516,\n        0.023012726,  -0.014274387,    0.008621267,  -0.016665483,\n       -0.016523534,  -0.021947058,  -0.0077380626,  -0.008166752,\n       -0.010050893, -0.0074697966,    0.021521091,  0.0086479345,\n       -0.008508939,   -0.03031165,  -0.0068692113,   0.032342624,\n       -0.003118368,  -0.009117541, -0.00006816292,   0.028233083,\n       -0.008163683,  -0.029179588,   -0.034861602,  -0.009573525,\n       -0.020023588,  -0.023040103,   0.0030518328,  -0.024019923,\n      ... 412 more items\n    ]\n  },\n  {\n    values: [\n        0.010123913,  -0.024184551,  0.0024574941,   -0.00984163, -0.0060574994,\n       -0.007628851,   0.013202136,  -0.027927121, -0.0016973788,  -0.014774812,\n       -0.011437808,  -0.019120526, -0.0063477424, -0.0050772373,    0.12938297,\n        0.006073787, -0.0055986797,   0.030279782,   0.015260121, -0.0014168695,\n       -0.006316713,  0.0007294639,  -0.034072377,   0.013348729,  0.0051308265,\n      -0.0042954376,  -0.009459755,  -0.012910496,   0.010751937, -0.0017263377,\n        -0.02083192,  0.0054532792,   0.008046588,  0.0015794274, -0.0045236745,\n       0.0077354256,  -0.009697459,   0.006621996,    -0.0447099,  -0.019261474,\n       0.0050193793,   0.010624901,   0.036847603,  -0.014380205,   0.023050537,\n        0.019384636,    0.03039269,   -0.02306347,  -0.025763597,   0.017585728,\n       0.0056267884,  -0.014494471,  -0.013168205,   -0.18764982,   0.011082365,\n        0.007989808, -0.0069600893,  0.0019873218,  -0.020733004,  -0.011488622,\n       0.0072846347,  -0.022266442,  -0.021857709,  -0.040680353,  0.0043984484,\n        0.016409805,  0.0010387278,   0.028186318,  -0.020797107,   0.007164954,\n       -0.007931046,   0.011955907,  0.0070153666,   -0.03028713,   0.039638296,\n      -0.0005224554,  -0.008104055,  -0.021054681,   0.017767426,   -0.01705528,\n      -0.0015202612,   0.027076574,  -0.008269598,  0.0041972124,  -0.009893149,\n      -0.0059321057,   -0.02742561,   0.011967838, -0.0012843752,  -0.012446694,\n        0.013188314,    0.01000231,  0.0063591595,  -0.013250329,   -0.00891349,\n       -0.011323209, 0.00077099906,  -0.032252073,   0.017312435,  -0.010896756,\n      ... 412 more items\n    ]\n  }\n]\n\n\nYou’ll get a set of three embeddings, one for each piece of text you passed in:\n\nconsole.log((embedding_response.embeddings ?? []).length);\n\n3\n\n\nYou can also see the length of each embedding is 512, as per the output_dimensionality you specified.\n\nconst vector_1 = embedding_response.embeddings?.[0]?.values ?? [];\nconsole.log(vector_1.length);\n\n512",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Getting started with Gemini models"
    ]
  },
  {
    "objectID": "quickstarts/Get_started.html#next-steps",
    "href": "quickstarts/Get_started.html#next-steps",
    "title": "Gemini API: Getting started with Gemini models",
    "section": "Next Steps",
    "text": "Next Steps\n\nUseful API references:\nCheck out the Google GenAI SDK for more details on the new SDK. ### Related examples\nFor more detailed examples using Gemini models, check the Quickstarts folder of the cookbook. You’ll learn how to use the Live API, juggle with multiple tools or use Gemini 2.0 spatial understanding abilities.\nAlso check the Gemini thinking models that explicitly showcases its thoughts summaries and can manage more complex reasonings.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Getting started with Gemini models"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Gemini API Cookbook",
    "section": "",
    "text": "This cookbook provides a structured learning path for using the Gemini API, focusing on hands-on tutorials and practical examples.\nFor comprehensive API documentation, visit ai.google.dev.\n\n\nThis cookbook is organized into two main categories:\n\nQuick Starts: Step-by-step guides covering both introductory topics (“Get Started”) and specific API features.\nExamples: Practical use cases demonstrating how to combine multiple features.\n\nWe also showcase Demos in separate repositories, illustrating end-to-end applications of the Gemini API.\n\n\n\nHere are the recent additions and updates to the Gemini API and the Cookbook:\n\nGemini 2.5 models: Explore the capabilities of the latest Gemini 2.5 models (Flash and Pro)! See the Get Started Guide and the thinking guide as they’ll all be thinking ones.\nImagen and Veo: Get started with our media generation model with this brand new Veo guide and Imagen guide!\nLyria and TTS: Get started with podcast and music generation with the TTS and Lyria RealTime models.\nLiveAPI: Get started with the multimodal Live API and unlock new interactivity with Gemini.\nRecently Added Guides:\n\nBrowser as a tool: Use a web browser for live and internal (intranet) web interactions\nGrounding: Discover different ways to ground Gemini’s answer using different tools, from Google Search to Youtube and the new url context tool.\n\n\n\n\n\nThe quickstarts section contains step-by-step tutorials to get you started with Gemini and learn about its specific features.\nTo begin, you’ll need:\n\nA Google account.\nAn API key (create one in Google AI Studio).\n\nWe recommend starting with the following:\n\nAuthentication: Set up your API key for access.\nGet started: Get started with Gemini models and the Gemini API, covering basic prompting and multimodal input.\n\nThen, explore the other quickstarts tutorials to learn about individual features:\n\nGet started with Live API: Get started with the live API with this comprehensive overview of its capabilities\nGet started with Veo: Get started with our video generation capabilities\nGet started with Imagen and Image-out: Get started with our image generation capabilities\nGrounding: use Google Search for grounded responses\nCode execution: Generating and running Python code to solve complex tasks and even ouput graphs\nAnd many more\n\n\n\n\nThese examples demonstrate how to combine multiple Gemini API features or 3rd-party tools to build more complex applications. - Illustrate a book: Use Gemini and Imagen to create illustration for an open-source book - Animated Story Generation: Create animated videos by combining Gemini’s story generation, Imagen, and audio synthesis - Plotting and mapping Live: Mix Live API and Code execution to solve complex tasks live - 3D Spatial understanding: Use Gemini 3D spatial abilities to understand 3D scenes - Gradio and live API: Use gradio to deploy your own instance of the Live API - And many many more\n\n\n\nThese fully functional, end-to-end applications showcase the power of Gemini in real-world scenarios.\n\nGemini API quickstart: Python Flask App running with the Google AI Gemini API, designed to get you started building with Gemini’s multi-modal capabilities\nMultimodal Live API Web Console: React-based starter app for using the Multimodal Live API over a websocket\nGoogle AI Studio Starter Applets: A collection of small apps that demonstrate how Gemini can be used to create interactive experiences\n\n\n\n\nThe Gemini API is a REST API. You can call it directly using tools like curl (see REST examples or the great Postman workspace), or use one of our official SDKs:\n\nPython\nGo\nNode.js\nDart (Flutter)\nAndroid\nSwift\n\n\n\n\nWith Gemini 2 we are offering a new SDK (@google/genai, v1.0). The updated SDK is fully compatible with all Gemini API models and features, including recent additions like the live API (audio + video streaming), improved tool usage ( code execution, function calling and integrated Google search grounding), and media generation (Imagen and Veo). This SDK allows you to connect to the Gemini API through either Google AI Studio or Vertex AI.\nThe generative-ai-js package will continue to support the original Gemini models. It can also be used with Gemini 2 models, just with a limited feature set. All new features will be developed in the new Google GenAI SDK.\nSee the migration guide for details.\n\n\n\nAsk a question on the Google AI Developer Forum.\n\n\n\nFor enterprise developers, the Gemini API is also available on Google Cloud Vertex AI. See this repo for examples.\n\n\n\nContributions are welcome! See CONTRIBUTING.md for details.\nThank you for developing with the Gemini API! We’re excited to see what you create."
  },
  {
    "objectID": "index.html#navigating-the-cookbook",
    "href": "index.html#navigating-the-cookbook",
    "title": "Gemini API Cookbook",
    "section": "",
    "text": "This cookbook is organized into two main categories:\n\nQuick Starts: Step-by-step guides covering both introductory topics (“Get Started”) and specific API features.\nExamples: Practical use cases demonstrating how to combine multiple features.\n\nWe also showcase Demos in separate repositories, illustrating end-to-end applications of the Gemini API."
  },
  {
    "objectID": "index.html#whats-new",
    "href": "index.html#whats-new",
    "title": "Gemini API Cookbook",
    "section": "",
    "text": "Here are the recent additions and updates to the Gemini API and the Cookbook:\n\nGemini 2.5 models: Explore the capabilities of the latest Gemini 2.5 models (Flash and Pro)! See the Get Started Guide and the thinking guide as they’ll all be thinking ones.\nImagen and Veo: Get started with our media generation model with this brand new Veo guide and Imagen guide!\nLyria and TTS: Get started with podcast and music generation with the TTS and Lyria RealTime models.\nLiveAPI: Get started with the multimodal Live API and unlock new interactivity with Gemini.\nRecently Added Guides:\n\nBrowser as a tool: Use a web browser for live and internal (intranet) web interactions\nGrounding: Discover different ways to ground Gemini’s answer using different tools, from Google Search to Youtube and the new url context tool."
  },
  {
    "objectID": "index.html#quick-starts",
    "href": "index.html#quick-starts",
    "title": "Gemini API Cookbook",
    "section": "",
    "text": "The quickstarts section contains step-by-step tutorials to get you started with Gemini and learn about its specific features.\nTo begin, you’ll need:\n\nA Google account.\nAn API key (create one in Google AI Studio).\n\nWe recommend starting with the following:\n\nAuthentication: Set up your API key for access.\nGet started: Get started with Gemini models and the Gemini API, covering basic prompting and multimodal input.\n\nThen, explore the other quickstarts tutorials to learn about individual features:\n\nGet started with Live API: Get started with the live API with this comprehensive overview of its capabilities\nGet started with Veo: Get started with our video generation capabilities\nGet started with Imagen and Image-out: Get started with our image generation capabilities\nGrounding: use Google Search for grounded responses\nCode execution: Generating and running Python code to solve complex tasks and even ouput graphs\nAnd many more"
  },
  {
    "objectID": "index.html#examples-practical-use-cases",
    "href": "index.html#examples-practical-use-cases",
    "title": "Gemini API Cookbook",
    "section": "",
    "text": "These examples demonstrate how to combine multiple Gemini API features or 3rd-party tools to build more complex applications. - Illustrate a book: Use Gemini and Imagen to create illustration for an open-source book - Animated Story Generation: Create animated videos by combining Gemini’s story generation, Imagen, and audio synthesis - Plotting and mapping Live: Mix Live API and Code execution to solve complex tasks live - 3D Spatial understanding: Use Gemini 3D spatial abilities to understand 3D scenes - Gradio and live API: Use gradio to deploy your own instance of the Live API - And many many more"
  },
  {
    "objectID": "index.html#demos-end-to-end-applications",
    "href": "index.html#demos-end-to-end-applications",
    "title": "Gemini API Cookbook",
    "section": "",
    "text": "These fully functional, end-to-end applications showcase the power of Gemini in real-world scenarios.\n\nGemini API quickstart: Python Flask App running with the Google AI Gemini API, designed to get you started building with Gemini’s multi-modal capabilities\nMultimodal Live API Web Console: React-based starter app for using the Multimodal Live API over a websocket\nGoogle AI Studio Starter Applets: A collection of small apps that demonstrate how Gemini can be used to create interactive experiences"
  },
  {
    "objectID": "index.html#official-sdks",
    "href": "index.html#official-sdks",
    "title": "Gemini API Cookbook",
    "section": "",
    "text": "The Gemini API is a REST API. You can call it directly using tools like curl (see REST examples or the great Postman workspace), or use one of our official SDKs:\n\nPython\nGo\nNode.js\nDart (Flutter)\nAndroid\nSwift"
  },
  {
    "objectID": "index.html#important-migration",
    "href": "index.html#important-migration",
    "title": "Gemini API Cookbook",
    "section": "",
    "text": "With Gemini 2 we are offering a new SDK (@google/genai, v1.0). The updated SDK is fully compatible with all Gemini API models and features, including recent additions like the live API (audio + video streaming), improved tool usage ( code execution, function calling and integrated Google search grounding), and media generation (Imagen and Veo). This SDK allows you to connect to the Gemini API through either Google AI Studio or Vertex AI.\nThe generative-ai-js package will continue to support the original Gemini models. It can also be used with Gemini 2 models, just with a limited feature set. All new features will be developed in the new Google GenAI SDK.\nSee the migration guide for details."
  },
  {
    "objectID": "index.html#get-help",
    "href": "index.html#get-help",
    "title": "Gemini API Cookbook",
    "section": "",
    "text": "Ask a question on the Google AI Developer Forum."
  },
  {
    "objectID": "index.html#the-gemini-api-on-google-cloud-vertex-ai",
    "href": "index.html#the-gemini-api-on-google-cloud-vertex-ai",
    "title": "Gemini API Cookbook",
    "section": "",
    "text": "For enterprise developers, the Gemini API is also available on Google Cloud Vertex AI. See this repo for examples."
  },
  {
    "objectID": "index.html#contributing",
    "href": "index.html#contributing",
    "title": "Gemini API Cookbook",
    "section": "",
    "text": "Contributions are welcome! See CONTRIBUTING.md for details.\nThank you for developing with the Gemini API! We’re excited to see what you create."
  },
  {
    "objectID": "quickstarts/Get_started_LiveAPI.html",
    "href": "quickstarts/Get_started_LiveAPI.html",
    "title": "Multimodal Live API - Quickstart",
    "section": "",
    "text": "Preview: The Live API is in preview.\nThis notebook demonstrates simple usage of the Gemini Multimodal Live API. For an overview of new capabilities refer to the Gemini Live API docs.\nThis notebook implements a simple turn-based chat where you send messages as text, and the model replies with audio. The API is capable of much more than that. The goal here is to demonstrate with simple code.\nSome features of the API are not working in Colab, to try them it is recommended to have a look at this Python script and run it locally.\nIf you aren’t looking for code, and just want to try multimedia streaming use Live API in Google AI Studio.\nThe Next steps section at the end of this tutorial provides links to additional resources.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Multimodal Live API - Quickstart"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_LiveAPI.html#setup",
    "href": "quickstarts/Get_started_LiveAPI.html#setup",
    "title": "Multimodal Live API - Quickstart",
    "section": "Setup",
    "text": "Setup\n\nInstall the Google GenAI SDK\nInstall the Google GenAI SDK from npm.\n$ npm install @google/genai\n\n\nSetup your API key\nYou can create your API key using Google AI Studio with a single click.\nRemember to treat your API key like a password. Don’t accidentally save it in a notebook or source file you later commit to GitHub. In this notebook we will be storing the API key in a .env file. You can also set it as an environment variable or use a secret manager.\nHere’s how to set it up in a .env file:\n$ touch .env\n$ echo \"GEMINI_API_KEY=&lt;YOUR_API_KEY&gt;\" &gt;&gt; .env\n\n\n\n\n\n\nTip\n\n\n\nAnother option is to set the API key as an environment variable. You can do this in your terminal with the following command:\n$ export GEMINI_API_KEY=\"&lt;YOUR_API_KEY&gt;\"\n\n\n\n\nLoad the API key\nTo load the API key from the .env file, we will use the dotenv package. This package loads environment variables from a .env file into process.env.\n$ npm install dotenv\nThen, we can load the API key in our code:\n\nconst dotenv = require(\"dotenv\") as typeof import(\"dotenv\");\n\ndotenv.config({\n  path: \"../.env\",\n});\n\nconst GEMINI_API_KEY = process.env.GEMINI_API_KEY ?? \"\";\nif (!GEMINI_API_KEY) {\n  throw new Error(\"GEMINI_API_KEY is not set in the environment variables\");\n}\nconsole.log(\"GEMINI_API_KEY is set in the environment variables\");\n\nGEMINI_API_KEY is set in the environment variables\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn our particular case the .env is is one directory up from the notebook, hence we need to use ../ to go up one directory. If the .env file is in the same directory as the notebook, you can omit it altogether.\n│\n├── .env\n└── quickstarts\n    └── Get_started_LiveAPI.ipynb",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Multimodal Live API - Quickstart"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_LiveAPI.html#initialize-sdk-client",
    "href": "quickstarts/Get_started_LiveAPI.html#initialize-sdk-client",
    "title": "Multimodal Live API - Quickstart",
    "section": "Initialize SDK Client",
    "text": "Initialize SDK Client\nWith the new SDK, now you only need to initialize a client with you API key (or OAuth if using Vertex AI). The model is now set in each call.\n\nconst google = require(\"@google/genai\") as typeof import(\"@google/genai\");\n\nconst ai = new google.GoogleGenAI({ apiKey: GEMINI_API_KEY });",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Multimodal Live API - Quickstart"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_LiveAPI.html#text-to-text",
    "href": "quickstarts/Get_started_LiveAPI.html#text-to-text",
    "title": "Multimodal Live API - Quickstart",
    "section": "Text to Text",
    "text": "Text to Text\nThe simplest way to use the Live API is as a text-to-text chat interface, but it can do a lot more than this.\n\nconst tslab = require(\"tslab\") as typeof import(\"tslab\");\n\nconst MODEL_ID = \"gemini-2.0-flash-live-001\";\n\nThe Live API uses a streaming model over a WebSocket connection. When you interact with the API, a persistent connection is created. Your input (audio, video, or text) is streamed continuously to the model, and the model’s response (text or audio) is streamed back in real-time over the same connection. Here we use a responseQueue to handle the streaming responses and determine when the server has finished sending the response.\n\nimport { LiveServerMessage, Modality } from \"@google/genai\";\n\nasync function text_to_text() {\n  const responseQueue: LiveServerMessage[] = [];\n  const session = await ai.live.connect({\n    model: MODEL_ID,\n    callbacks: {\n      onopen: function () {\n        console.debug(\"Opened\");\n      },\n      onmessage: function (message) {\n        responseQueue.push(message);\n      },\n      onerror: function (e) {\n        console.debug(\"Error:\", e.message);\n      },\n      onclose: function (e) {\n        console.debug(\"Close:\", e.reason);\n      },\n    },\n    config: { responseModalities: [Modality.TEXT] },\n  });\n  const message = \"Hello? Gemini are you there?\";\n  session.sendClientContent({\n    turns: message,\n    turnComplete: true,\n  });\n  console.debug(\"Sent message:\", message);\n  let done = false;\n  while (!done) {\n    if (responseQueue.length &gt; 0) {\n      const response = responseQueue.shift();\n      if (response?.text) {\n        console.debug(\"Received response:\", response.text);\n      } else if (response?.data) {\n        console.debug(\"Received data:\", response.data);\n      }\n      if (response?.serverContent?.turnComplete) {\n        done = true;\n      }\n    } else {\n      await new Promise((resolve) =&gt; setTimeout(resolve, 100));\n    }\n  }\n  session.close();\n  console.debug(\"Session closed\");\n}\n\nawait text_to_text();\n\nOpened\nSent message: Hello? Gemini are you there?\nReceived response: Yes, I am\nReceived response:  here! How can I help you today?\n\nSession closed\nClose:",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Multimodal Live API - Quickstart"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_LiveAPI.html#text-to-audio",
    "href": "quickstarts/Get_started_LiveAPI.html#text-to-audio",
    "title": "Multimodal Live API - Quickstart",
    "section": "Text to audio",
    "text": "Text to audio\nThe simplest way to playback the audio in Colab, is to write it out to a .wav file. So here is a simple wave file writer:\n\nconst fs = require(\"fs\") as typeof import(\"fs\");\nconst path = require(\"path\") as typeof import(\"path\");\nconst wave = require(\"wavefile\") as typeof import(\"wavefile\");\n\nfunction saveAudioToFile(audioData: Int16Array, filePath: string) {\n  fs.mkdirSync(path.dirname(filePath), { recursive: true });\n  const wav = new wave.WaveFile();\n  wav.fromScratch(1, 24000, \"16\", audioData);\n  fs.writeFileSync(filePath, wav.toBuffer());\n  console.debug(`Audio saved to ${filePath}`);\n}\n\n\nimport { LiveServerMessage, Modality } from \"@google/genai\";\n\nasync function text_to_audio() {\n  const responseQueue: LiveServerMessage[] = [];\n  const session = await ai.live.connect({\n    model: MODEL_ID,\n    callbacks: {\n      onopen: function () {\n        console.debug(\"Opened\");\n      },\n      onmessage: function (message) {\n        responseQueue.push(message);\n      },\n      onerror: function (e) {\n        console.debug(\"Error:\", e.message);\n      },\n      onclose: function (e) {\n        console.debug(\"Close:\", e.reason);\n      },\n    },\n    config: { responseModalities: [Modality.AUDIO] },\n  });\n  const message = \"Hello? Gemini are you there?\";\n  session.sendClientContent({\n    turns: message,\n    turnComplete: true,\n  });\n  console.debug(\"Sent message:\", message);\n  let done = false;\n  const chunks: LiveServerMessage[] = [];\n  while (!done) {\n    if (responseQueue.length &gt; 0) {\n      const response = responseQueue.shift();\n      if (response) {\n        chunks.push(response);\n      }\n      if (response?.serverContent?.turnComplete) {\n        done = true;\n        console.debug(\"Received complete response\");\n      }\n    } else {\n      await new Promise((resolve) =&gt; setTimeout(resolve, 100));\n    }\n  }\n  const audioData = chunks.reduce&lt;number[]&gt;((acc, message) =&gt; {\n    if (message.data) {\n      const audioBuffer = Buffer.from(message.data, \"base64\");\n      const intArray = new Int16Array(\n        audioBuffer.buffer,\n        audioBuffer.byteOffset,\n        audioBuffer.length / Int16Array.BYTES_PER_ELEMENT\n      );\n      return acc.concat(Array.from(intArray));\n    }\n    return acc;\n  }, []);\n  const audioFilePath = path.join(\"../assets/live\", \"text_to_audio_response.wav\");\n  saveAudioToFile(new Int16Array(audioData), audioFilePath);\n  session.close();\n  console.debug(\"Session closed\");\n}\n\nawait text_to_audio();\ntslab.display.html(`\n  &lt;h3&gt;Text to Audio Response&lt;/h3&gt;\n  &lt;audio controls&gt;\n    &lt;source src=\"../assets/live/text_to_audio_response.wav\" type=\"audio/wav\"&gt;\n    Your browser does not support the audio element.\n  &lt;/audio&gt;\n  &lt;/audio&gt;\n`);\n\nOpened\nSent message: Hello? Gemini are you there?\nReceived complete response\nAudio saved to ../assets/live/text_to_audio_response.wav\nSession closed\n\n\n\nText to Audio Response\n\n  \n  Your browser does not support the audio element.\n\n\n\n\nClose:",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Multimodal Live API - Quickstart"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_LiveAPI.html#towards-async-tasks",
    "href": "quickstarts/Get_started_LiveAPI.html#towards-async-tasks",
    "title": "Multimodal Live API - Quickstart",
    "section": "Towards Async Tasks",
    "text": "Towards Async Tasks\nThe real power of the Live API is that it’s real time, and interruptable. You can’t get that full power in a simple sequence of steps. To really use the functionality you will move the send and recieve operations (and others) into their own async tasks.\nBecause of the limitations of Colab this tutorial doesn’t totally implement the interactive async tasks, but it does implement the next step in that direction:\n\nIt separates the send and receive, but still runs them sequentially.\nIn the next tutorial you’ll run these in separate async tasks.\n\n\nimport { GoogleGenAI, LiveServerMessage, Modality, Session } from \"@google/genai\";\n\nclass AudioLooper {\n  private session: Session;\n  private turnIndex = 0;\n  private responseQueue: LiveServerMessage[] = [];\n\n  constructor(\n    private ai: GoogleGenAI,\n    private modelId: string\n  ) {}\n\n  async start() {\n    this.session = await this.ai.live.connect({\n      model: this.modelId,\n      callbacks: {\n        onopen: () =&gt; {\n          console.debug(\"Opened\");\n        },\n        onmessage: (message) =&gt; this.responseQueue.push(message),\n        onerror: (e) =&gt; {\n          console.debug(\"Error:\", e.message);\n        },\n        onclose: (e) =&gt; {\n          console.debug(\"Close:\", e.reason);\n        },\n      },\n      config: { responseModalities: [Modality.AUDIO] },\n    });\n  }\n\n  send(message: string) {\n    this.session.sendClientContent({\n      turns: message,\n      turnComplete: true,\n    });\n    console.debug(\"Sent message:\", message);\n  }\n\n  async receive() {\n    let done = false;\n    const audioChunks: number[] = [];\n    while (!done) {\n      if (this.responseQueue.length &gt; 0) {\n        const response = this.responseQueue.shift();\n        if (response?.data) {\n          const audioBuffer = Buffer.from(response.data, \"base64\");\n          const intArray = new Int16Array(\n            audioBuffer.buffer,\n            audioBuffer.byteOffset,\n            audioBuffer.length / Int16Array.BYTES_PER_ELEMENT\n          );\n          audioChunks.push(...Array.from(intArray));\n        }\n        if (response?.serverContent?.turnComplete) {\n          done = true;\n          console.debug(\"Received complete response\");\n        }\n      } else {\n        await new Promise((resolve) =&gt; setTimeout(resolve, 100));\n      }\n    }\n    const audioFilePath = path.join(\"../assets/live\", `audio_response_${this.turnIndex++}.wav`);\n    saveAudioToFile(new Int16Array(audioChunks), audioFilePath);\n    tslab.display.html(`\n      &lt;h3&gt;Audio Response ${this.turnIndex}&lt;/h3&gt;\n      &lt;audio controls&gt;\n          &lt;source src=\"../assets/live/audio_response_${this.turnIndex - 1}.wav\" type=\"audio/wav\"&gt;\n          Your browser does not support the audio element.\n      &lt;/audio&gt;\n    `);\n  }\n\n  stop() {\n    this.session.close();\n    console.debug(\"Session closed\");\n  }\n}\n\nasync function asyncAudioLooper() {\n  const audioLooper = new AudioLooper(ai, MODEL_ID);\n  await audioLooper.start();\n\n  // Simulate sending messages\n  const messages = [\"Hello? Gemini are you there?\", \"Can you tell me a joke?\", \"What is the weather like today?\"];\n\n  for (const message of messages) {\n    audioLooper.send(message);\n    await audioLooper.receive();\n  }\n\n  audioLooper.stop();\n}\n\nawait asyncAudioLooper();\n\nOpened\nSent message: Hello? Gemini are you there?\nReceived complete response\nAudio saved to ../assets/live/audio_response_0.wav\n\n\n\nAudio Response 1\n\n    \n    Your browser does not support the audio element.\n\n\n\nSent message: Can you tell me a joke?\nReceived complete response\nAudio saved to ../assets/live/audio_response_1.wav\n\n\n\nAudio Response 2\n\n    \n    Your browser does not support the audio element.\n\n\n\nSent message: What is the weather like today?\nReceived complete response\nAudio saved to ../assets/live/audio_response_2.wav\n\n\n\nAudio Response 3\n\n    \n    Your browser does not support the audio element.\n\n\n\nSession closed\nClose: \n\n\nThe above code is divided into several sections:\n\nstart: Initializes the client and sets up the WebSocket connection.\nsend: Sends a message to the model.\nreceive: Receives the model’s response and collects the audio chunks in a loop and writes them to wav file. It breaks when the model indicates it has finished sending the response.\nasyncAudioLooper: This is the main driver function that brings everything together. It initializes the client, starts the WebSocket connection, and then enters a loop where it sends messages and receives responses.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Multimodal Live API - Quickstart"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_LiveAPI.html#working-with-resumable-sessions",
    "href": "quickstarts/Get_started_LiveAPI.html#working-with-resumable-sessions",
    "title": "Multimodal Live API - Quickstart",
    "section": "Working with resumable sessions",
    "text": "Working with resumable sessions\nSession resumption allows you to return to a previous interaction with the Live API by sending the last session handle you got from the previous session.\nWhen you set your session to be resumable, the session information keeps stored on the Live API for up to 24 hours. In this time window, you can resume the conversation and refer to previous information you have shared with the model.\n\nimport { LiveServerMessage, Modality } from \"@google/genai\";\n\nlet HANDLE: string | undefined = undefined;\n\nasync function resumable_session(\n  previousSessionHandle?: string,\n  messages: string[] = [\"Hello\", \"What is the capital of Brazil?\"]\n) {\n  const responseQueue: LiveServerMessage[] = [];\n\n  async function waitMessage(): Promise&lt;LiveServerMessage&gt; {\n    let done = false;\n    let message: LiveServerMessage | undefined = undefined;\n    while (!done) {\n      message = responseQueue.shift();\n      if (message) {\n        done = true;\n      } else {\n        await new Promise((resolve) =&gt; setTimeout(resolve, 100));\n      }\n    }\n    return message!;\n  }\n\n  console.debug(\"Connecting to the service with handle %s...\", previousSessionHandle);\n  const session = await ai.live.connect({\n    model: MODEL_ID,\n    callbacks: {\n      onopen: function () {\n        console.debug(\"Opened\");\n      },\n      onmessage: function (message) {\n        responseQueue.push(message);\n        console.debug(\"Received message:\", JSON.stringify(message));\n        if (message.sessionResumptionUpdate?.resumable && message.sessionResumptionUpdate.newHandle) {\n          HANDLE = message.sessionResumptionUpdate.newHandle;\n        }\n      },\n      onerror: function (e) {\n        console.debug(\"Error:\", e.message);\n      },\n      onclose: function (e) {\n        console.debug(\"Close:\", e.reason);\n      },\n    },\n    config: {\n      responseModalities: [Modality.TEXT],\n      sessionResumption: { handle: previousSessionHandle },\n    },\n  });\n\n  for (const message of messages) {\n    console.debug(\"Sending message:\", message);\n    session.sendClientContent({\n      turns: message,\n      turnComplete: true,\n    });\n    let done = false;\n    while (!done) {\n      const response = await waitMessage();\n      if (response.serverContent?.turnComplete) {\n        done = true;\n      }\n    }\n  }\n\n  // small delay for session resumption update to arrive\n  await new Promise((resolve) =&gt; setTimeout(resolve, 3000));\n\n  session.close();\n}\n\nawait resumable_session();\n\nConnecting to the service with handle undefined...\nOpened\nSending message: Hello\nReceived message: {\"setupComplete\":{}}\nReceived message: {\"sessionResumptionUpdate\":{}}\nReceived message: {\"serverContent\":{\"modelTurn\":{\"parts\":[{\"text\":\"Hello there! How\"}]}}}\nReceived message: {\"serverContent\":{\"modelTurn\":{\"parts\":[{\"text\":\" can I help you today?\\n\"}]}}}\nReceived message: {\"serverContent\":{\"generationComplete\":true}}\nReceived message: {\"serverContent\":{\"turnComplete\":true},\"usageMetadata\":{\"promptTokenCount\":9,\"responseTokenCount\":11,\"totalTokenCount\":20,\"promptTokensDetails\":[{\"modality\":\"TEXT\",\"tokenCount\":9}],\"responseTokensDetails\":[{\"modality\":\"TEXT\",\"tokenCount\":11}]}}\nSending message: What is the capital of Brazil?\nReceived message: {\"sessionResumptionUpdate\":{\"newHandle\":\"CihqdTFxaG1ua2g2aTkweWtiNzB5Ymdzc3V0bW16eDE2ZGkxaXR2d2dt\",\"resumable\":true}}\nReceived message: {\"sessionResumptionUpdate\":{}}\nReceived message: {\"serverContent\":{\"modelTurn\":{\"parts\":[{\"text\":\"The capital of Brazil\"}]}}}\nReceived message: {\"serverContent\":{\"modelTurn\":{\"parts\":[{\"text\":\" is **Brasília**.\\n\"}]}}}\nReceived message: {\"serverContent\":{\"generationComplete\":true}}\nReceived message: {\"serverContent\":{\"turnComplete\":true},\"usageMetadata\":{\"promptTokenCount\":37,\"responseTokenCount\":10,\"totalTokenCount\":47,\"promptTokensDetails\":[{\"modality\":\"TEXT\",\"tokenCount\":37}],\"responseTokensDetails\":[{\"modality\":\"TEXT\",\"tokenCount\":10}]}}\nReceived message: {\"sessionResumptionUpdate\":{\"newHandle\":\"CihrNGZyMjh4dXY3cXFkYzVmMjR5cnlmZ2w5bnBvNTRhcmoxNW1lN2Fi\",\"resumable\":true}}\nClose: \n\n\nWith the session resumption you have the session handle to refer to your previous sessions. In this example, the handle is saved at the handle variable as below:\n\nconsole.debug(\"Session handle:\", HANDLE);\n\nSession handle: CihrNGZyMjh4dXY3cXFkYzVmMjR5cnlmZ2w5bnBvNTRhcmoxNW1lN2Fi\n\n\nNow you can start a new Live API session, but this time pointing to a handle from a previous session. Also, to test you could gather information from the previous session, you will ask the model what was the second question you asked before (in this example, it was “what is the capital of Brazil?”). You can see the Live API recovering that information:\n\nawait resumable_session(HANDLE, [\"what was the last question I asked?\"]);\n\nConnecting to the service with handle CihrNGZyMjh4dXY3cXFkYzVmMjR5cnlmZ2w5bnBvNTRhcmoxNW1lN2Fi...\nOpened\nSending message: what was the last question I asked?\nReceived message: {\"setupComplete\":{}}\nReceived message: {\"sessionResumptionUpdate\":{}}\nReceived message: {\"serverContent\":{\"modelTurn\":{\"parts\":[{\"text\":\"The\"}]}}}\nReceived message: {\"serverContent\":{\"modelTurn\":{\"parts\":[{\"text\":\" last question you asked was: \\\"What is the capital of Brazil?\\\"\\n\"}]}}}\nReceived message: {\"serverContent\":{\"generationComplete\":true}}\nReceived message: {\"serverContent\":{\"turnComplete\":true},\"usageMetadata\":{\"promptTokenCount\":65,\"responseTokenCount\":16,\"totalTokenCount\":81,\"promptTokensDetails\":[{\"modality\":\"TEXT\",\"tokenCount\":65}],\"responseTokensDetails\":[{\"modality\":\"TEXT\",\"tokenCount\":16}]}}\nReceived message: {\"sessionResumptionUpdate\":{\"newHandle\":\"CihmcW04ZzVnZnZwczU2ZnkwN2h1NHpmajFxZmgwcmhieTZ3Zmo3OWt6\",\"resumable\":true}}\nClose:",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Multimodal Live API - Quickstart"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_LiveAPI.html#next-steps",
    "href": "quickstarts/Get_started_LiveAPI.html#next-steps",
    "title": "Multimodal Live API - Quickstart",
    "section": "Next steps",
    "text": "Next steps\n\nThis tutorial just shows basic usage of the Live API, using the Python GenAI SDK.\n\nIf you aren’t looking for code, and just want to try multimedia streaming use Live API in Google AI Studio.\nIf you want to see how to setup streaming interruptible audio and video using the Live API see the Audio and Video input Tutorial.\nIf you’re interested in the low level details of using the websockets directly, see the websocket version of this tutorial.\nTry the Tool use in the live API tutorial for an walkthrough of Gemini-2’s new tool use capabilities.\nThere is a Streaming audio in Colab example, but this is more of a demo, it’s not optimized for readability.\nOther nice Gemini 2.0 examples can also be found in the Cookbook’s 2.0 directory, in particular the video understanding and the spatial understanding ones.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Multimodal Live API - Quickstart"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_LiveAPI_tools.html",
    "href": "quickstarts/Get_started_LiveAPI_tools.html",
    "title": "Gemini 2.0 - Multimodal live API: Tool use",
    "section": "",
    "text": "This notebook provides examples of how to use tools with the multimodal live API with Gemini 2.0.\nThe API provides Google Search, Code Execution and Function Calling tools. The earlier Gemini models supported versions of these tools. The biggest change with Gemini 2 (in the Live API) is that, basically, all the tools are handled by Code Execution. With that change, you can use multiple tools in a single API call, and the model can use multiple tools in a single code execution block.\nThis tutorial assumes you are familiar with the Live API, as described in the this tutorial.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini 2.0 - Multimodal live API: Tool use"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_LiveAPI_tools.html#setup",
    "href": "quickstarts/Get_started_LiveAPI_tools.html#setup",
    "title": "Gemini 2.0 - Multimodal live API: Tool use",
    "section": "Setup",
    "text": "Setup\n\nInstall the Google GenAI SDK\nInstall the Google GenAI SDK from npm.\n$ npm install @google/genai\n\n\nSetup your API key\nYou can create your API key using Google AI Studio with a single click.\nRemember to treat your API key like a password. Don’t accidentally save it in a notebook or source file you later commit to GitHub. In this notebook we will be storing the API key in a .env file. You can also set it as an environment variable or use a secret manager.\nHere’s how to set it up in a .env file:\n$ touch .env\n$ echo \"GEMINI_API_KEY=&lt;YOUR_API_KEY&gt;\" &gt;&gt; .env\n\n\n\n\n\n\nTip\n\n\n\nAnother option is to set the API key as an environment variable. You can do this in your terminal with the following command:\n$ export GEMINI_API_KEY=\"&lt;YOUR_API_KEY&gt;\"\n\n\n\n\nLoad the API key\nTo load the API key from the .env file, we will use the dotenv package. This package loads environment variables from a .env file into process.env.\n$ npm install dotenv\nThen, we can load the API key in our code:\n\nconst dotenv = require(\"dotenv\") as typeof import(\"dotenv\");\n\ndotenv.config({\n  path: \"../.env\",\n});\n\nconst GEMINI_API_KEY = process.env.GEMINI_API_KEY ?? \"\";\nif (!GEMINI_API_KEY) {\n  throw new Error(\"GEMINI_API_KEY is not set in the environment variables\");\n}\nconsole.log(\"GEMINI_API_KEY is set in the environment variables\");\n\nGEMINI_API_KEY is set in the environment variables\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn our particular case the .env is is one directory up from the notebook, hence we need to use ../ to go up one directory. If the .env file is in the same directory as the notebook, you can omit it altogether.\n│\n├── .env\n└── quickstarts\n    └── Get_started_LiveAPI_tools.ipynb\n\n\n\n\nInitialize SDK Client\nWith the new SDK, now you only need to initialize a client with you API key (or OAuth if using Vertex AI). The model is now set in each call.\n\nconst google = require(\"@google/genai\") as typeof import(\"@google/genai\");\n\nconst ai = new google.GoogleGenAI({ apiKey: GEMINI_API_KEY });\n\n\n\nSelect a model\nMultimodal Live API are a new capability introduced with the Gemini 2.0 model. It won’t work with previous generation models.\n\nconst tslab = require(\"tslab\") as typeof import(\"tslab\");\n\nconst MODEL_ID = \"gemini-2.0-flash-live-001\";\n\n\n\nUtilites\nYou’re going to use the Live API’s audio output, the easiest way hear it in Colab is to write the PCM data out as a WAV file:\n\nconst fs = require(\"fs\") as typeof import(\"fs\");\nconst path = require(\"path\") as typeof import(\"path\");\nconst wave = require(\"wavefile\") as typeof import(\"wavefile\");\n\nfunction saveAudioToFile(audioData: Int16Array, filePath: string) {\n  fs.mkdirSync(path.dirname(filePath), { recursive: true });\n  const wav = new wave.WaveFile();\n  wav.fromScratch(1, 24000, \"16\", audioData);\n  fs.writeFileSync(filePath, wav.toBuffer());\n  console.debug(`Audio saved to ${filePath}`);\n}",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini 2.0 - Multimodal live API: Tool use"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_LiveAPI_tools.html#get-started",
    "href": "quickstarts/Get_started_LiveAPI_tools.html#get-started",
    "title": "Gemini 2.0 - Multimodal live API: Tool use",
    "section": "Get Started",
    "text": "Get Started\nMost of the Live API setup will be similar to the starter tutorial. Since this tutorial doesn’t focus on the realtime interactivity of the API, the code has been simplified: This code uses the Live API, but it only sends a single text prompt, and listens for a single turn of replies.\nYou can set modality=“AUDIO” on any of the examples to get the spoken version of the output.\n\nimport { FunctionResponse, LiveServerContent, LiveServerToolCall, Modality, Session, Tool } from \"@google/genai\";\n\nfunction handleServerContent(content: LiveServerContent) {\n  if (content.modelTurn) {\n    for (const turn of content.modelTurn.parts ?? []) {\n      if (turn.executableCode) {\n        tslab.display.markdown(\"-------------------------------\");\n        tslab.display.markdown(`\\`\\`\\`python\\n${turn.executableCode.code}\\n\\`\\`\\``);\n        tslab.display.markdown(\"-------------------------------\");\n      }\n      if (turn.codeExecutionResult) {\n        tslab.display.markdown(\"-------------------------------\");\n        tslab.display.markdown(`\\`\\`\\`\\n${turn.codeExecutionResult.output}\\n\\`\\`\\``);\n        tslab.display.markdown(\"-------------------------------\");\n      }\n    }\n  }\n  if (content.groundingMetadata) {\n    tslab.display.html(content.groundingMetadata.searchEntryPoint?.renderedContent ?? \"\");\n  }\n}\n\nfunction handleToolCall(session: Session, toolCall: LiveServerToolCall) {\n  const responses: FunctionResponse[] = [];\n  for (const fc of toolCall.functionCalls ?? []) {\n    responses.push({\n      id: fc.id,\n      name: fc.name,\n      response: {\n        result: \"ok\",\n      },\n    });\n  }\n  console.log(\"Tool call responses:\", JSON.stringify(responses, null, 2));\n  session.sendToolResponse({\n    functionResponses: responses,\n  });\n}\n\nasync function run(prompt: string, modality: Modality = Modality.TEXT, tools: Tool[] = []) {\n  const audioData: number[] = [];\n  const audioFileName = `audio-${Date.now()}.wav`;\n  let completed = false;\n  const session = await ai.live.connect({\n    model: MODEL_ID,\n    callbacks: {\n      onopen: () =&gt; {\n        console.log(\"Connection opened\");\n      },\n      onclose: () =&gt; {\n        console.log(\"Connection closed\");\n      },\n      onerror: (error) =&gt; {\n        console.error(\"Error:\", error.message);\n      },\n      onmessage: (message) =&gt; {\n        if (message.text) {\n          tslab.display.markdown(message.text);\n          return;\n        }\n        if (message.data) {\n          const audioBuffer = Buffer.from(message.data, \"base64\");\n          const audio = new Int16Array(\n            audioBuffer.buffer,\n            audioBuffer.byteOffset,\n            audioBuffer.length / Int16Array.BYTES_PER_ELEMENT\n          );\n          audioData.push(...audio);\n          return;\n        }\n        if (message.serverContent) {\n          handleServerContent(message.serverContent);\n          if (message.serverContent.turnComplete) {\n            completed = true;\n          }\n          return;\n        }\n        if (message.toolCall) {\n          handleToolCall(session, message.toolCall);\n          completed = true;\n          return;\n        }\n      },\n    },\n    config: {\n      tools: tools,\n      responseModalities: [modality],\n    },\n  });\n  console.log(\"Prompt: \", prompt);\n  session.sendClientContent({\n    turns: [prompt],\n    turnComplete: true,\n  });\n  // eslint-disable-next-line @typescript-eslint/no-unnecessary-condition\n  while (!completed) {\n    await new Promise((resolve) =&gt; setTimeout(resolve, 100));\n  }\n  if (audioData.length &gt; 0) {\n    saveAudioToFile(new Int16Array(audioData), path.join(\"audio\", audioFileName));\n    console.log(`Audio saved to ${audioFileName}`);\n    tslab.display.html(\n      `&lt;audio controls&gt;&lt;source src=\"${audioFileName}\" type=\"audio/wav\"&gt;Your browser does not support the audio element.&lt;/audio&gt;`\n    );\n  }\n  console.log(\"Session completed\");\n  session.close();\n}\n\nSince this tutorial demonstrates several tools, you’ll need more code to handle the different types of objects it returns.\n\nThe codeExecution tool can return executableCode and codeExecutionResult parts.\nThe googleSearch tool may attach a groundingMetadata object.\nFinally, with the functionDeclations tool, the API may return toolCall objects.To keep this code minimal, the toolCall handler just replies to every function call with a response of \"ok\".\n\n\nawait run(\"Hello?\");\n\nConnection opened\nPrompt:  Hello?\n\n\nHello! How can\n\n\nI help you today?\n\n\nSession completed\nConnection closed",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini 2.0 - Multimodal live API: Tool use"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_LiveAPI_tools.html#simple-function-call",
    "href": "quickstarts/Get_started_LiveAPI_tools.html#simple-function-call",
    "title": "Gemini 2.0 - Multimodal live API: Tool use",
    "section": "Simple Function Call",
    "text": "Simple Function Call\nThe function calling feature of the API Can handle a wide variety of functions. Support in the SDK is still under construction. So keep this simple just send a minimal function definition: Just the function’s name.\nNote that in the live API function calls are independent of the chat turns. The conversation can continue while a function call is being processed.\n\nimport { FunctionDeclaration, Tool } from \"@google/genai\";\n\nconst turn_on_the_lights = {\n  name: \"turn_on_the_lights\",\n  description: \"Turn on the lights in the room\",\n} satisfies FunctionDeclaration;\nconst turn_off_the_lights: FunctionDeclaration = {\n  name: \"turn_off_the_lights\",\n  description: \"Turn off the lights in the room\",\n} satisfies FunctionDeclaration;\nconst function_call_tools: Tool[] = [{ functionDeclarations: [turn_on_the_lights, turn_off_the_lights] }];\n\n// temporarily make console.warn a no-op to avoid warnings in the output (non-text part in GenerateContentResponse caused by accessing .text)\n// https://github.com/googleapis/js-genai/blob/d82aba244bdb804b063ef8a983b2916c00b901d2/src/types.ts#L2005\n// copy the original console.warn function to restore it later\nconst warn_fn = console.warn;\n// eslint-disable-next-line @typescript-eslint/no-empty-function, no-empty-function\nconsole.warn = function () {};\n\nawait run(\"Turn on the lights\", google.Modality.TEXT, function_call_tools);\n// restore console.warn later\n// console.warn = warn_fn;\n\nConnection opened\nPrompt:  Turn on the lights\n\n\n\n\n\nprint(default_api.turn_on_the_lights())\n\n\n\n\n\nTool call responses: [\n  {\n    \"id\": \"function-call-16720258795371319743\",\n    \"name\": \"turn_on_the_lights\",\n    \"response\": {\n      \"result\": \"ok\"\n    }\n  }\n]\n\n\n\n\n\n{'result': 'ok'}\n\n\n\n\n\n\nOK\n\n\n, I’ve turned on the lights.\n\n\nSession completed\nConnection closed",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini 2.0 - Multimodal live API: Tool use"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_LiveAPI_tools.html#code-execution",
    "href": "quickstarts/Get_started_LiveAPI_tools.html#code-execution",
    "title": "Gemini 2.0 - Multimodal live API: Tool use",
    "section": "Code Execution",
    "text": "Code Execution\nThe codeExecution lets the model write and run python code. Try it on a math problem the model can’t solve from memory:\n\nawait run(\"Can you compute the largest prime palindrome under 100000.\", google.Modality.TEXT, [{ codeExecution: {} }]);\n\nConnection opened\nPrompt:  Can you compute the largest prime palindrome under 100000.\n\n\nOkay\n\n\n, I can help you with that. Here’s my plan:\n1\n\n\n. Generate Palindromes: Create a list of all palindromes under 100000. 2. Check for Primality:\n\n\nIterate through the palindromes and check if each one is prime. 3. Find the Largest: Keep track of the largest prime palindrome found so far.\n\n\nHere’s the code to do that:\n\n\n\n\n\ndef is_palindrome(n):\n  \"\"\"Checks if a number is a palindrome.\"\"\"\n  return str(n) == str(n)[::-1]\n\n\ndef is_prime(n):\n  \"\"\"Checks if a number is prime.\"\"\"\n  if n &lt; 2:\n    return False\n  for i in range(2, int(n**0.5) + 1):\n    if n % i == 0:\n      return False\n  return True\n\n\nlargest_prime_palindrome = 0\nfor i in range(100000):\n  if is_palindrome(i) and is_prime(i):\n    largest_prime_palindrome = i\n\nprint(largest_prime_palindrome)\n\n\n\n\n\n\n\n\n98689\n\n\n\n\n\n\nThe largest prime palindrome\n\n\nunder 100000 is 98689.\n\n\n\n\n\nSession completed\nConnection closed",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini 2.0 - Multimodal live API: Tool use"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_LiveAPI_tools.html#compositional-function-calling",
    "href": "quickstarts/Get_started_LiveAPI_tools.html#compositional-function-calling",
    "title": "Gemini 2.0 - Multimodal live API: Tool use",
    "section": "Compositional Function Calling",
    "text": "Compositional Function Calling\nCompositional function calling refers to the ability to combine user defined functions with the codeExecution tool. The model will write them into larger blocks of code, and then pause execution while it waits for you to send back responses for each call.\n\nawait run(\"Can you turn on the lights wait 10s and then turn them off?\", google.Modality.TEXT, [\n  ...function_call_tools,\n  { codeExecution: {} },\n]);\n\nConnection opened\nPrompt:  Can you turn on the lights wait 10s and then turn them off?\n\n\n\n\n\nimport time\n\ndefault_api.turn_on_the_lights()\ntime.sleep(10)\ndefault_api.turn_off_the_lights()\n\n\n\n\n\nTool call responses: [\n  {\n    \"id\": \"function-call-448821244251533960\",\n    \"name\": \"turn_on_the_lights\",\n    \"response\": {\n      \"result\": \"ok\"\n    }\n  }\n]\nSession completed\nConnection closed",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini 2.0 - Multimodal live API: Tool use"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_LiveAPI_tools.html#google-search",
    "href": "quickstarts/Get_started_LiveAPI_tools.html#google-search",
    "title": "Gemini 2.0 - Multimodal live API: Tool use",
    "section": "Google search",
    "text": "Google search\nThe googleSearch tool lets the model conduct google searches. For example, try asking it about events that are too recent to be in the training data.\nThe search will still execute in AUDIO mode, but you won’t see the detailed results:\n\nawait run(\n  \"When the latest Brazil vs. Argentina soccer match happened and what was the final score?\",\n  google.Modality.TEXT,\n  [{ googleSearch: {} }]\n);\n\nConnection opened\nPrompt:  When the latest Brazil vs. Argentina soccer match happened and what was the final score?\n\n\n\n\n\nprint(google_search.search(queries=[\"latest Brazil vs Argentina soccer match date and score\", \"Brazil vs Argentina recent match results\"]))\n\n\n\n\n\n\n\n\nLooking up information on Google Search.\n\n\n\n\n\n\nThe most recent match\n\n\nbetween Brazil and Argentina took place on **March 25, 20\n\n\n25, as part of the 2026 FIFA World Cup qualifiers. Argentina won the match with a final score of 4-1**.\n\n\n\n\n  \n    \n      \n      \n      \n      \n    \n    \n      \n      \n      \n      \n      \n    \n    \n  \n  \n    Brazil vs Argentina recent match results\n    latest Brazil vs Argentina soccer match date and score\n  \n\n\n\nSession completed\nConnection closed",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini 2.0 - Multimodal live API: Tool use"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_LiveAPI_tools.html#multiple-tools",
    "href": "quickstarts/Get_started_LiveAPI_tools.html#multiple-tools",
    "title": "Gemini 2.0 - Multimodal live API: Tool use",
    "section": "Multiple tools",
    "text": "Multiple tools\nThe biggest difference with the new API however is that you’re no longer limited to using 1-tool per request. Try combining those tasks from the previous sections:\n\nimport { Tool } from \"@google/genai\";\n\nconst multi_tool_prompt = `\n  Hey, I need you to do three things for me.\n\n  1. Then compute the largest prime plaindrome under 100000.\n  2. Then use google search to lookup unformation about the largest earthquake in california the week of Dec 5 2024?\n  3. Turn on the lights\n\n  Thanks!\n`;\nconst multi_tool_tools: Tool[] = [\n  { codeExecution: {} },\n  { googleSearch: {} },\n  { functionDeclarations: [turn_on_the_lights, turn_off_the_lights] },\n];\n\nawait run(multi_tool_prompt, google.Modality.TEXT, multi_tool_tools);\n\nConnection opened\nPrompt:  \n  Hey, I need you to do three things for me.\n\n  1. Then compute the largest prime plaindrome under 100000.\n  2. Then use google search to lookup unformation about the largest earthquake in california the week of Dec 5 2024?\n  3. Turn on the lights\n\n  Thanks!\n\n\n\nOkay\n\n\n, I can do that. Here’s the plan:\n\nCompute\n\n\n\nthe largest prime palindrome under 100000. I’ll use a Python script to achieve this. 2. Use Google Search to look up\n\n\ninformation about the largest earthquake in California the week of Dec 5 2024. 3. Turn on the lights using the provided API.\nHere\n\n\n’s the first step, computing the largest prime palindrome under 100000:\n\n\n\n\n\ndef is_palindrome(n):\n  return str(n) == str(n)[::-1]\n\ndef is_prime(n):\n  if n &lt; 2:\n    return False\n  for i in range(2, int(n**0.5) + 1):\n    if n % i == 0:\n      return False\n  return True\n\nlargest_prime_palindrome = 0\nfor i in range(99999, 1, -1):\n  if is_palindrome(i) and is_prime(i):\n    largest_prime_palindrome = i\n    break\n\nprint(largest_prime_palindrome)\n\n\n\n\n\n\n\n\n98689\n\n\n\n\n\n\nOkay\n\n\n, the largest prime palindrome under 100000 is 98689.\n\n\nNow, let’s use Google Search to find the largest earthquake in California the week of Dec 5 2024.\n\n\n\n\n\n\n\n\nconcise_search(\"largest earthquake california week of December 5 2024\", max_num_results=5)\n\n\n\n\n\n\n\n\nLooking up information on Google Search.\n\n\n\n\n\n\nBased\n\n\non the search results, the largest earthquake in California during the week of December\n\n\n5, 2024, was a magnitude 7.0 earthquake offshore of Cape Mendocino on December 5, 2024,\n\n\nat 10:44 a.m. PST.\nFinally, I will turn on the lights.\n\n\n\n\n\ndefault_api.turn_on_the_lights()\n\n\n\n\n\n\n\n  \n    \n      \n      \n      \n      \n    \n    \n      \n      \n      \n      \n      \n    \n    \n  \n  \n    largest earthquake california week of December 5 2024\n  \n\n\n\nTool call responses: [\n  {\n    \"id\": \"function-call-10200942088489058256\",\n    \"name\": \"turn_on_the_lights\",\n    \"response\": {\n      \"result\": \"ok\"\n    }\n  }\n]\nSession completed\nConnection closed",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini 2.0 - Multimodal live API: Tool use"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_LiveAPI_tools.html#next-steps",
    "href": "quickstarts/Get_started_LiveAPI_tools.html#next-steps",
    "title": "Gemini 2.0 - Multimodal live API: Tool use",
    "section": "Next Steps",
    "text": "Next Steps\n\nFor more information about the SDK see the SDK docs\nThis tutorial uses the high level SDK, if you’re interested in the lower-level details, try the Websocket version of this tutorial\nThis tutorial only covers basic usage of these tools for deeper (and more fun) example see the Search tool tutorial\n\nOr check the other Gemini 2.0 capabilities from the Cookbook, in particular this other multi-tool example and the one about Gemini spatial capabilities.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini 2.0 - Multimodal live API: Tool use"
    ]
  }
]