[
  {
    "objectID": "quickstarts/New_in_002.html",
    "href": "quickstarts/New_in_002.html",
    "title": "What’s new in Gemini-1.5-pro-002 and Gemini-1.5-flash-002",
    "section": "",
    "text": "This notebook explores the new options added with the 002 versions of the 1.5 series models:",
    "crumbs": [
      "Home",
      "Quickstarts",
      "What's new in Gemini-1.5-pro-002 and Gemini-1.5-flash-002"
    ]
  },
  {
    "objectID": "quickstarts/New_in_002.html#setup",
    "href": "quickstarts/New_in_002.html#setup",
    "title": "What’s new in Gemini-1.5-pro-002 and Gemini-1.5-flash-002",
    "section": "Setup",
    "text": "Setup\n\nInstall the Google GenAI SDK\nInstall the Google GenAI SDK from npm.\n$ npm install @google/genai\n\n\nSetup your API key\nYou can create your API key using Google AI Studio with a single click.\nRemember to treat your API key like a password. Don’t accidentally save it in a notebook or source file you later commit to GitHub. In this notebook we will be storing the API key in a .env file. You can also set it as an environment variable or use a secret manager.\nHere’s how to set it up in a .env file:\n$ touch .env\n$ echo \"GEMINI_API_KEY=&lt;YOUR_API_KEY&gt;\" &gt;&gt; .env\n\n\n\n\n\n\nTip\n\n\n\nAnother option is to set the API key as an environment variable. You can do this in your terminal with the following command:\n$ export GEMINI_API_KEY=\"&lt;YOUR_API_KEY&gt;\"\n\n\n\n\nLoad the API key\nTo load the API key from the .env file, we will use the dotenv package. This package loads environment variables from a .env file into process.env.\n$ npm install dotenv\nThen, we can load the API key in our code:\n\nconst dotenv = require(\"dotenv\") as typeof import(\"dotenv\");\n\ndotenv.config({\n  path: \"../.env\",\n});\n\nconst GEMINI_API_KEY = process.env.GEMINI_API_KEY ?? \"\";\nif (!GEMINI_API_KEY) {\n  throw new Error(\"GEMINI_API_KEY is not set in the environment variables\");\n}\nconsole.log(\"GEMINI_API_KEY is set in the environment variables\");\n\nGEMINI_API_KEY is set in the environment variables\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn our particular case the .env is is one directory up from the notebook, hence we need to use ../ to go up one directory. If the .env file is in the same directory as the notebook, you can omit it altogether.\n│\n├── .env\n└── quickstarts\n    └── Get_started_TTS.ipynb\n\n\n\n\nInitialize SDK Client\nWith the new SDK, now you only need to initialize a client with you API key (or OAuth if using Vertex AI). The model is now set in each call.\n\nconst google = require(\"@google/genai\") as typeof import(\"@google/genai\");\n\nconst ai = new google.GoogleGenAI({ apiKey: GEMINI_API_KEY });\n\n\n\nSelect a model\nCheck available 002 models\n\nconst models = await ai.models.list();\nlet { page } = models;\nwhile (page.length &gt; 0) {\n  for (const model of page) {\n    if (model.name.includes(\"002\")) {\n      console.log(model.name);\n    }\n  }\n  page = models.hasNextPage() ? await models.nextPage() : [];\n}\n\nmodels/gemini-1.5-pro-002\nmodels/gemini-1.5-flash-002\nmodels/imagen-3.0-generate-002\n\n\n\nconst tslab = require(\"tslab\") as typeof import(\"tslab\");\n\nconst MODEL_ID = \"gemini-1.5-flash-002\";",
    "crumbs": [
      "Home",
      "Quickstarts",
      "What's new in Gemini-1.5-pro-002 and Gemini-1.5-flash-002"
    ]
  },
  {
    "objectID": "quickstarts/New_in_002.html#quick-refresher-on-generationconfig-optional",
    "href": "quickstarts/New_in_002.html#quick-refresher-on-generationconfig-optional",
    "title": "What’s new in Gemini-1.5-pro-002 and Gemini-1.5-flash-002",
    "section": "Quick refresher on GenerationConfig [Optional]",
    "text": "Quick refresher on GenerationConfig [Optional]\n\nconst response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: \"Why don't people have tails\",\n  config: {\n    temperature: 1.0,\n    maxOutputTokens: 5,\n  },\n});\nconsole.log(response.text);\n\nHumans don't have\n\n\n\nEach generateContent request is sent with a GenerationConfig (chat.sendMessage uses generateContent).\nYou can set the GenerationConfig by passing it in the arguments to generateContent (or chat.sendMessage).\nIf you’re ever unsure about the parameters of GenerationConfig check types.GenerationConfig.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "What's new in Gemini-1.5-pro-002 and Gemini-1.5-flash-002"
    ]
  },
  {
    "objectID": "quickstarts/New_in_002.html#candidate-count",
    "href": "quickstarts/New_in_002.html#candidate-count",
    "title": "What’s new in Gemini-1.5-pro-002 and Gemini-1.5-flash-002",
    "section": "Candidate count",
    "text": "Candidate count\nWith 002 models you can now use candidateCount &gt; 1.\n\nconst responses = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: \"Why don't people have tails\",\n  config: {\n    candidateCount: 2,\n  },\n});\n\n\n\n\n\n\n\nNote\n\n\n\nNote that the .text quick-accessor only works for the simple 1-candidate case, and the sdk will log a console.warn stating that it’s only returning the first candidate. If you want to access all candidates, use the .candidates property instead.\n\n\n\ntslab.display.markdown(responses.text ?? \"\");\n\nthere are multiple candidates in the response, returning text from the first one.\n\n\nHumans don’t have tails because of evolutionary changes over millions of years. Our ancestors did have tails, but as humans evolved and adapted to walking upright (bipedalism), the tail became less necessary and advantageous. The genes that controlled tail development were gradually switched off or modified through a process called natural selection.\nSpecifically:\n\nLoss of function mutations: Random genetic mutations occurred that affected the genes responsible for tail development. If these mutations didn’t have a significant negative impact on survival and reproduction, they could be passed on to future generations. Over time, accumulating mutations progressively reduced tail size until it became vestigial (the coccyx, or tailbone, is what remains).\nBipedalism and its consequences: Walking upright changed the way our bodies functioned and the pressures of natural selection. A tail, which is useful for balance and climbing in arboreal (tree-dwelling) animals, became less important for bipedal locomotion. The energy spent developing and maintaining a tail was likely better spent on other adaptations crucial for survival on the ground.\nDevelopmental changes: The changes in our embryonic development also contributed to the reduction of the tail. Human embryos do initially have a tail-like structure during development, but it regresses and is largely absorbed before birth, leaving only the coccyx.\n\nIn short, the lack of a tail in humans is the result of a complex interplay of genetic mutations, evolutionary pressures related to bipedalism, and changes in embryonic development. It’s an example of how natural selection can lead to the loss of features that are no longer advantageous.\n\n\nWith multiple candidates you have to handle the list of candidates yourself:\n\nfor (const response of responses.candidates ?? []) {\n  tslab.display.markdown(response.content?.parts?.[0].text ?? \"\");\n  tslab.display.markdown(\"\\n---\\n\");\n}\n\nHumans don’t have tails because of evolutionary changes over millions of years. Our ancestors had tails, but as humans evolved, the genes that controlled tail development were suppressed. This wasn’t a sudden change, but a gradual process driven by natural selection. Several theories attempt to explain why this happened:\n\nLoss of function: The tail’s original functions (balance, climbing, etc.) became less crucial as our ancestors adopted bipedalism (walking upright). The energy and resources used to maintain a tail were no longer offset by its benefits, leading to a neutral, or even slightly negative, selective pressure on genes controlling tail development. Mutations that reduced or eliminated tail growth were not disadvantageous and were therefore passed on.\nDevelopmental changes: The changes might also have resulted from shifts in embryonic development. Genes regulating the development of the tail and the coccyx (the small bone at the base of our spine, which is a vestigial tail) may have interacted in ways that ultimately favored shorter and eventually, non-existent external tails.\nSexual selection: It’s possible that, at some point, taillessness or shorter tails became a desirable trait from a sexual selection perspective. This is purely speculative and difficult to prove.\n\nIt’s important to note that we still have a vestigial tail – the coccyx. This remnant of our tailed ancestry is a testament to our evolutionary history. While a functional tail is absent, the genetic mechanisms for tail development are still present, albeit largely inactive, as evidenced by rare cases of human babies born with tails (which are usually surgically removed). These are usually malformations rather than a “re-emergence” of a functional tail, but they highlight the fact that the genetic information for a tail isn’t entirely lost.\n\n\n\n\n\nHumans don’t have tails because of evolution. Our ancestors, like apes, did have tails, but over millions of years, the genes responsible for tail development were switched off or mutated in ways that resulted in the loss of the tail. This wasn’t a conscious decision, but rather a consequence of natural selection.\nSeveral theories propose why this might have been advantageous:\n\nImproved balance and bipedalism: As humans evolved to walk upright, a tail might have become more of a hindrance than a help. A tail could interfere with balance and efficient movement on two legs.\nReduced energy consumption: Maintaining a tail requires energy. Losing the tail could have freed up resources for other developmental needs.\nSocial signaling: In some primates, tails are used for communication. However, humans developed other, more complex communication methods, making a tail less necessary.\nSexual selection: The lack of a tail might have become a desirable trait in mate selection, leading to its elimination through sexual selection.\n\nIt’s important to note that this is a complex evolutionary process, and the exact reasons are still being researched. It’s likely a combination of these factors, and perhaps others we haven’t yet identified, that contributed to the loss of the tail in humans. The occasional birth of a human baby with a vestigial tail (a small, rudimentary tail) demonstrates that the genes for tail development haven’t completely disappeared from the human genome, they’re just usually inactive.\n\n\n\n\n\nThe response contains multiple full Candidate objects.\n\nconsole.log(JSON.stringify(responses, null, 2));\n\n{\n  \"candidates\": [\n    {\n      \"content\": {\n        \"parts\": [\n          {\n            \"text\": \"Humans don't have tails because of evolutionary changes over millions of years.  Our ancestors did have tails, but as humans evolved and adapted to walking upright (bipedalism), the tail became less necessary and advantageous.  The genes that controlled tail development were gradually switched off or modified through a process called natural selection.  \\n\\nSpecifically:\\n\\n* **Loss of function mutations:**  Random genetic mutations occurred that affected the genes responsible for tail development.  If these mutations didn't have a significant negative impact on survival and reproduction, they could be passed on to future generations.  Over time, accumulating mutations progressively reduced tail size until it became vestigial (the coccyx, or tailbone, is what remains).\\n\\n* **Bipedalism and its consequences:**  Walking upright changed the way our bodies functioned and the pressures of natural selection.  A tail, which is useful for balance and climbing in arboreal (tree-dwelling) animals, became less important for bipedal locomotion. The energy spent developing and maintaining a tail was likely better spent on other adaptations crucial for survival on the ground.\\n\\n* **Developmental changes:**  The changes in our embryonic development also contributed to the reduction of the tail.  Human embryos do initially have a tail-like structure during development, but it regresses and is largely absorbed before birth, leaving only the coccyx.\\n\\nIn short, the lack of a tail in humans is the result of a complex interplay of genetic mutations, evolutionary pressures related to bipedalism, and changes in embryonic development.  It's an example of how natural selection can lead to the loss of features that are no longer advantageous.\\n\"\n          }\n        ],\n        \"role\": \"model\"\n      },\n      \"finishReason\": \"STOP\",\n      \"avgLogprobs\": -0.3816025597708566\n    },\n    {\n      \"content\": {\n        \"parts\": [\n          {\n            \"text\": \"Humans don't have tails primarily because of **evolutionary changes**.  Over millions of years, our primate ancestors gradually lost their tails through a process of natural selection.  While the exact reasons are complex and not fully understood, several contributing factors are likely:\\n\\n* **Bipedalism:**  As our ancestors transitioned to walking upright on two legs, a tail became less advantageous.  Tails are helpful for balance and locomotion in quadrupedal (four-legged) animals, but they would have been less useful and potentially even hindering for bipedal locomotion.  The energy expended maintaining a tail might have been better allocated to other adaptations.\\n\\n* **Changes in Genes:**  Specific genetic mutations likely played a crucial role.  These mutations affected the genes controlling tail development during embryonic growth, leading to shorter and eventually absent tails in our lineage.\\n\\n* **Selection Pressures:**  Natural selection favored individuals with shorter tails or no tails at all.  Those without tails might have had advantages in terms of energy efficiency, agility, or even reduced predation risk, depending on the specific environmental pressures.\\n\\nIt's important to note that the loss of the tail wasn't a sudden event.  It occurred gradually over a long period, with intermediate forms showing progressively shorter tails.  The vestigial tailbone (coccyx) we have today is a remnant of this evolutionary process.  It's a small, fused collection of bones at the base of our spine, reflecting our tailed ancestry.\\n\"\n          }\n        ],\n        \"role\": \"model\"\n      },\n      \"finishReason\": \"STOP\",\n      \"avgLogprobs\": -0.302224858601888,\n      \"index\": 1\n    }\n  ],\n  \"modelVersion\": \"gemini-1.5-flash-002\",\n  \"usageMetadata\": {\n    \"promptTokenCount\": 7,\n    \"candidatesTokenCount\": 636,\n    \"totalTokenCount\": 643,\n    \"promptTokensDetails\": [\n      {\n        \"modality\": \"TEXT\",\n        \"tokenCount\": 7\n      }\n    ],\n    \"candidatesTokensDetails\": [\n      {\n        \"modality\": \"TEXT\",\n        \"tokenCount\": 636\n      }\n    ]\n  }\n}",
    "crumbs": [
      "Home",
      "Quickstarts",
      "What's new in Gemini-1.5-pro-002 and Gemini-1.5-flash-002"
    ]
  },
  {
    "objectID": "quickstarts/New_in_002.html#penalties",
    "href": "quickstarts/New_in_002.html#penalties",
    "title": "What’s new in Gemini-1.5-pro-002 and Gemini-1.5-flash-002",
    "section": "Penalties",
    "text": "Penalties\nThe 002 models expose penalty arguments that let you affect the statistics of output tokens.\n\nPresence penalty\nThe presencePenalty penalizes tokens that have already been used in the output, so it induces variety in the model’s output. This is detectible if you count the unique words in the output.\nHere’s a function to run a prompt a few times and report the fraction of unique words (words don’t map perfectly to tokens but it’s a simple way to see the effect).\n\nconst PROMPT = \"Tell me a story\";\n\nasync function uniqueWords(penalty?: number, N = 10): Promise&lt;number[]&gt; {\n  const fractions: number[] = [];\n  for (let i = 0; i &lt; N; i++) {\n    const response = await ai.models.generateContent({\n      model: MODEL_ID,\n      contents: PROMPT,\n      config: {\n        ...(penalty ? { presencePenalty: penalty } : {}),\n      },\n    });\n    const words = (response.text ?? \"\").toLowerCase().split(/\\s+/);\n    fractions.push(new Set(words).size / words.length);\n  }\n  return fractions;\n}\n\nfunction mean(arr: number[]): number {\n  return arr.reduce((a, b) =&gt; a + b, 0) / arr.length;\n}\n\n\nconst v1 = await uniqueWords();\nconsole.log(JSON.stringify(v1, null, 2));\n\n[\n  0.565121412803532,\n  0.5858585858585859,\n  0.6172839506172839,\n  0.6188235294117647,\n  0.6035634743875279,\n  0.5790754257907542,\n  0.6004901960784313,\n  0.5717821782178217,\n  0.5853658536585366,\n  0.5574837310195228\n]\n\n\n\n// baseline\nconsole.log(\"Mean unique words:\", mean(v1).toFixed(2));\n\nMean unique words: 0.59\n\n\n\n// the penalty encourages diversity in the oputput tokens.\nconst v2 = await uniqueWords(1.999);\nconsole.log(JSON.stringify(v2, null, 2));\n\n[\n  0.5771971496437055,\n  0.5931818181818181,\n  0.5955555555555555,\n  0.6108247422680413,\n  0.5742092457420924,\n  0.59,\n  0.6296296296296297,\n  0.6077097505668935,\n  0.6077981651376146,\n  0.6103896103896104\n]\n\n\n\nconsole.log(\"Mean unique words with penalty:\", mean(v2).toFixed(2));\n\nMean unique words with penalty: 0.60\n\n\n\n// a negative penalty discourages diversity in the output tokens.\nconst v3 = await uniqueWords(-1.999);\nconsole.log(JSON.stringify(v3, null, 2));\n\n[\n  0.5708061002178649,\n  0.5588235294117647,\n  0.5515695067264574,\n  0.5736263736263736,\n  0.5495867768595041,\n  0.6142506142506142,\n  0.6072289156626506,\n  0.5573770491803278,\n  0.5839080459770115,\n  0.6190476190476191\n]\n\n\n\nconsole.log(\"Mean unique words with negative penalty:\", mean(v3).toFixed(2));\n\nMean unique words with negative penalty: 0.58\n\n\nThe presencePenalty has a small effect on the vocabulary statistics.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "What's new in Gemini-1.5-pro-002 and Gemini-1.5-flash-002"
    ]
  },
  {
    "objectID": "quickstarts/New_in_002.html#frequency-penalty",
    "href": "quickstarts/New_in_002.html#frequency-penalty",
    "title": "What’s new in Gemini-1.5-pro-002 and Gemini-1.5-flash-002",
    "section": "Frequency Penalty",
    "text": "Frequency Penalty\nFrequency penalty is similar to the presencePenalty but the penalty is multiplied by the number of times a token is used. This effect is much stronger than the presencePenalty.\nThe easiest way to see that it works is to ask the model to do something repetitive. The model has to get creative while trying to complete the task.\n\nconst frequency_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: `please repeat \"Cat\" 50 times, 10 per line`,\n  config: {\n    frequencyPenalty: 1.999,\n  },\n});\nconsole.log(frequency_response.text ?? \"\");\n\nCat Cat Cat Cat Cat Cat Cat Cat Cat Cat\nCat Cat Cat Cat Cat Cat Cat Cat Ca tCat\nCat Cat Cat Cat Cat Cat Cat Cat Cat\nCat Cat Cat Cat Ca tCat Cat Cat Cat\nCat Cat Cat Cat Cat Cat Cat Cat Ca tCat\nCat Cat Cat Cat Ca tCat Cat Cata t\n\n\n\nSince the frequency penalty accumulates with usage, it can have a much stronger effect on the output compared to the presence penalty.\n\n\n\n\n\n\nImportant\n\n\n\nBe careful with negative frequency penalties: A negative penalty makes a token more likely the more it’s used. This positive feedback quickly leads the model to just repeat a common token until it hits the maxOutputTokens limit (once it starts the model can’t produce the  token).\n\n\n\nconst negative_frequency_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: PROMPT,\n  config: {\n    frequencyPenalty: -2.0,\n    maxOutputTokens: 400,\n  },\n});\ntslab.display.markdown(negative_frequency_response.text ?? \"\");\n\nElara, a wisp of a girl with eyes the colour of a stormy sea, lived in a lighthouse perched precariously on the edge of the Whispering Cliffs. Her only companion was her grandfather, a grizzled man whose weathered face held the map of a thousand storms. He taught her the language of the sea – the mournful cry of gulls, the rhythmic crash of waves, the subtle shift in the wind that foretold a tempest.\nOne day, a storm unlike any Elara had ever seen descended upon the coast. The wind howled like a banshee, tearing at the lighthouse, and the waves, monstrous and frothing, crashed against the rocks below with terrifying force. Grandpa, usually unflappable, felt a tremor of unease. He’d seen many storms, but this one felt… different.\nMidst the fury, a faint, rhythmic glow pulsed through the swirling mist. Intrigued, and despite Grandpa’s warnings, Elara climbed to the lantern room. Through the driving rain, she saw it: a small, glowing boat, battling the waves with impossible grace. It was crafted from what looked like polished sea glass, and its sail, a shimmering, opalescent membrane, billowed, defying the wind.\nAs the boat neared, a figure, shimmering, like the boat, emerged, a woman, her,, hair like spun moonlight, her eyes like twin stars. The woman smiled, a serene, otherworldly smile, and extended a hand, a hand that, as Elara touched it, sent a jolt of warmth through her, a feeling of profound peace.\nThe woman spoke, her voice a whisper of the sea, “The storm, child, is not of this world. It is a tear from a fallen star. We, the children of the tide, must mend it.”\nElara, without hesitation, stepped onto the luminous boat.\n\n\n\nconsole.log(negative_frequency_response.candidates?.[0]?.finishReason);\n\nMAX_TOKENS",
    "crumbs": [
      "Home",
      "Quickstarts",
      "What's new in Gemini-1.5-pro-002 and Gemini-1.5-flash-002"
    ]
  },
  {
    "objectID": "quickstarts/New_in_002.html#next-steps",
    "href": "quickstarts/New_in_002.html#next-steps",
    "title": "What’s new in Gemini-1.5-pro-002 and Gemini-1.5-flash-002",
    "section": "Next steps",
    "text": "Next steps\nCheck out the latest models and their capabilities.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "What's new in Gemini-1.5-pro-002 and Gemini-1.5-flash-002"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_LiveAPI_tools.html",
    "href": "quickstarts/Get_started_LiveAPI_tools.html",
    "title": "Gemini 2.0 - Multimodal live API: Tool use",
    "section": "",
    "text": "This notebook provides examples of how to use tools with the multimodal live API with Gemini 2.0.\nThe API provides Google Search, Code Execution and Function Calling tools. The earlier Gemini models supported versions of these tools. The biggest change with Gemini 2 (in the Live API) is that, basically, all the tools are handled by Code Execution. With that change, you can use multiple tools in a single API call, and the model can use multiple tools in a single code execution block.\nThis tutorial assumes you are familiar with the Live API, as described in the this tutorial.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini 2.0 - Multimodal live API: Tool use"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_LiveAPI_tools.html#setup",
    "href": "quickstarts/Get_started_LiveAPI_tools.html#setup",
    "title": "Gemini 2.0 - Multimodal live API: Tool use",
    "section": "Setup",
    "text": "Setup\n\nInstall the Google GenAI SDK\nInstall the Google GenAI SDK from npm.\n$ npm install @google/genai\n\n\nSetup your API key\nYou can create your API key using Google AI Studio with a single click.\nRemember to treat your API key like a password. Don’t accidentally save it in a notebook or source file you later commit to GitHub. In this notebook we will be storing the API key in a .env file. You can also set it as an environment variable or use a secret manager.\nHere’s how to set it up in a .env file:\n$ touch .env\n$ echo \"GEMINI_API_KEY=&lt;YOUR_API_KEY&gt;\" &gt;&gt; .env\n\n\n\n\n\n\nTip\n\n\n\nAnother option is to set the API key as an environment variable. You can do this in your terminal with the following command:\n$ export GEMINI_API_KEY=\"&lt;YOUR_API_KEY&gt;\"\n\n\n\n\nLoad the API key\nTo load the API key from the .env file, we will use the dotenv package. This package loads environment variables from a .env file into process.env.\n$ npm install dotenv\nThen, we can load the API key in our code:\n\nconst dotenv = require(\"dotenv\") as typeof import(\"dotenv\");\n\ndotenv.config({\n  path: \"../.env\",\n});\n\nconst GEMINI_API_KEY = process.env.GEMINI_API_KEY ?? \"\";\nif (!GEMINI_API_KEY) {\n  throw new Error(\"GEMINI_API_KEY is not set in the environment variables\");\n}\nconsole.log(\"GEMINI_API_KEY is set in the environment variables\");\n\nGEMINI_API_KEY is set in the environment variables\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn our particular case the .env is is one directory up from the notebook, hence we need to use ../ to go up one directory. If the .env file is in the same directory as the notebook, you can omit it altogether.\n│\n├── .env\n└── quickstarts\n    └── Get_started_LiveAPI_tools.ipynb\n\n\n\n\nInitialize SDK Client\nWith the new SDK, now you only need to initialize a client with you API key (or OAuth if using Vertex AI). The model is now set in each call.\n\nconst google = require(\"@google/genai\") as typeof import(\"@google/genai\");\n\nconst ai = new google.GoogleGenAI({ apiKey: GEMINI_API_KEY });\n\n\n\nSelect a model\nMultimodal Live API are a new capability introduced with the Gemini 2.0 model. It won’t work with previous generation models.\n\nconst tslab = require(\"tslab\") as typeof import(\"tslab\");\n\nconst MODEL_ID = \"gemini-2.0-flash-live-001\";\n\n\n\nUtilites\nYou’re going to use the Live API’s audio output, the easiest way hear it in Colab is to write the PCM data out as a WAV file:\n\nconst fs = require(\"fs\") as typeof import(\"fs\");\nconst path = require(\"path\") as typeof import(\"path\");\nconst wave = require(\"wavefile\") as typeof import(\"wavefile\");\n\nfunction saveAudioToFile(audioData: Int16Array, filePath: string) {\n  fs.mkdirSync(path.dirname(filePath), { recursive: true });\n  const wav = new wave.WaveFile();\n  wav.fromScratch(1, 24000, \"16\", audioData);\n  fs.writeFileSync(filePath, wav.toBuffer());\n  console.debug(`Audio saved to ${filePath}`);\n}",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini 2.0 - Multimodal live API: Tool use"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_LiveAPI_tools.html#get-started",
    "href": "quickstarts/Get_started_LiveAPI_tools.html#get-started",
    "title": "Gemini 2.0 - Multimodal live API: Tool use",
    "section": "Get Started",
    "text": "Get Started\nMost of the Live API setup will be similar to the starter tutorial. Since this tutorial doesn’t focus on the realtime interactivity of the API, the code has been simplified: This code uses the Live API, but it only sends a single text prompt, and listens for a single turn of replies.\nYou can set modality=“AUDIO” on any of the examples to get the spoken version of the output.\n\nimport { FunctionResponse, LiveServerContent, LiveServerToolCall, Modality, Session, Tool } from \"@google/genai\";\n\nfunction handleServerContent(content: LiveServerContent) {\n  if (content.modelTurn) {\n    for (const turn of content.modelTurn.parts ?? []) {\n      if (turn.executableCode) {\n        tslab.display.markdown(\"-------------------------------\");\n        tslab.display.markdown(`\\`\\`\\`python\\n${turn.executableCode.code}\\n\\`\\`\\``);\n        tslab.display.markdown(\"-------------------------------\");\n      }\n      if (turn.codeExecutionResult) {\n        tslab.display.markdown(\"-------------------------------\");\n        tslab.display.markdown(`\\`\\`\\`\\n${turn.codeExecutionResult.output}\\n\\`\\`\\``);\n        tslab.display.markdown(\"-------------------------------\");\n      }\n    }\n  }\n  if (content.groundingMetadata) {\n    tslab.display.html(content.groundingMetadata.searchEntryPoint?.renderedContent ?? \"\");\n  }\n}\n\nfunction handleToolCall(session: Session, toolCall: LiveServerToolCall) {\n  const responses: FunctionResponse[] = [];\n  for (const fc of toolCall.functionCalls ?? []) {\n    responses.push({\n      id: fc.id,\n      name: fc.name,\n      response: {\n        result: \"ok\",\n      },\n    });\n  }\n  console.log(\"Tool call responses:\", JSON.stringify(responses, null, 2));\n  session.sendToolResponse({\n    functionResponses: responses,\n  });\n}\n\nasync function run(prompt: string, modality: Modality = Modality.TEXT, tools: Tool[] = []) {\n  const audioData: number[] = [];\n  const audioFileName = `audio-${Date.now()}.wav`;\n  let completed = false;\n  const session = await ai.live.connect({\n    model: MODEL_ID,\n    callbacks: {\n      onopen: () =&gt; {\n        console.log(\"Connection opened\");\n      },\n      onclose: () =&gt; {\n        console.log(\"Connection closed\");\n      },\n      onerror: (error) =&gt; {\n        console.error(\"Error:\", error.message);\n      },\n      onmessage: (message) =&gt; {\n        if (message.text) {\n          tslab.display.markdown(message.text);\n          return;\n        }\n        if (message.data) {\n          const audioBuffer = Buffer.from(message.data, \"base64\");\n          const audio = new Int16Array(\n            audioBuffer.buffer,\n            audioBuffer.byteOffset,\n            audioBuffer.length / Int16Array.BYTES_PER_ELEMENT\n          );\n          audioData.push(...audio);\n          return;\n        }\n        if (message.serverContent) {\n          handleServerContent(message.serverContent);\n          if (message.serverContent.turnComplete) {\n            completed = true;\n          }\n          return;\n        }\n        if (message.toolCall) {\n          handleToolCall(session, message.toolCall);\n          completed = true;\n          return;\n        }\n      },\n    },\n    config: {\n      tools: tools,\n      responseModalities: [modality],\n    },\n  });\n  console.log(\"Prompt: \", prompt);\n  session.sendClientContent({\n    turns: [prompt],\n    turnComplete: true,\n  });\n  // eslint-disable-next-line @typescript-eslint/no-unnecessary-condition\n  while (!completed) {\n    await new Promise((resolve) =&gt; setTimeout(resolve, 100));\n  }\n  if (audioData.length &gt; 0) {\n    saveAudioToFile(new Int16Array(audioData), path.join(\"audio\", audioFileName));\n    console.log(`Audio saved to ${audioFileName}`);\n    tslab.display.html(\n      `&lt;audio controls&gt;&lt;source src=\"${audioFileName}\" type=\"audio/wav\"&gt;Your browser does not support the audio element.&lt;/audio&gt;`\n    );\n  }\n  console.log(\"Session completed\");\n  session.close();\n}\n\nSince this tutorial demonstrates several tools, you’ll need more code to handle the different types of objects it returns.\n\nThe codeExecution tool can return executableCode and codeExecutionResult parts.\nThe googleSearch tool may attach a groundingMetadata object.\nFinally, with the functionDeclations tool, the API may return toolCall objects.To keep this code minimal, the toolCall handler just replies to every function call with a response of \"ok\".\n\n\nawait run(\"Hello?\");\n\nConnection opened\nPrompt:  Hello?\n\n\nHello! How can\n\n\nI help you today?\n\n\nSession completed\nConnection closed",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini 2.0 - Multimodal live API: Tool use"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_LiveAPI_tools.html#simple-function-call",
    "href": "quickstarts/Get_started_LiveAPI_tools.html#simple-function-call",
    "title": "Gemini 2.0 - Multimodal live API: Tool use",
    "section": "Simple Function Call",
    "text": "Simple Function Call\nThe function calling feature of the API Can handle a wide variety of functions. Support in the SDK is still under construction. So keep this simple just send a minimal function definition: Just the function’s name.\nNote that in the live API function calls are independent of the chat turns. The conversation can continue while a function call is being processed.\n\nimport { FunctionDeclaration, Tool } from \"@google/genai\";\n\nconst turn_on_the_lights = {\n  name: \"turn_on_the_lights\",\n  description: \"Turn on the lights in the room\",\n} satisfies FunctionDeclaration;\nconst turn_off_the_lights: FunctionDeclaration = {\n  name: \"turn_off_the_lights\",\n  description: \"Turn off the lights in the room\",\n} satisfies FunctionDeclaration;\nconst function_call_tools: Tool[] = [{ functionDeclarations: [turn_on_the_lights, turn_off_the_lights] }];\n\n// temporarily make console.warn a no-op to avoid warnings in the output (non-text part in GenerateContentResponse caused by accessing .text)\n// https://github.com/googleapis/js-genai/blob/d82aba244bdb804b063ef8a983b2916c00b901d2/src/types.ts#L2005\n// copy the original console.warn function to restore it later\nconst warn_fn = console.warn;\n// eslint-disable-next-line @typescript-eslint/no-empty-function, no-empty-function\nconsole.warn = function () {};\n\nawait run(\"Turn on the lights\", google.Modality.TEXT, function_call_tools);\n// restore console.warn later\n// console.warn = warn_fn;\n\nConnection opened\nPrompt:  Turn on the lights\n\n\n\n\n\nprint(default_api.turn_on_the_lights())\n\n\n\n\n\nTool call responses: [\n  {\n    \"id\": \"function-call-16720258795371319743\",\n    \"name\": \"turn_on_the_lights\",\n    \"response\": {\n      \"result\": \"ok\"\n    }\n  }\n]\n\n\n\n\n\n{'result': 'ok'}\n\n\n\n\n\n\nOK, I’ve turned on the lights.\n\n\nSession completed\nConnection closed",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini 2.0 - Multimodal live API: Tool use"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_LiveAPI_tools.html#code-execution",
    "href": "quickstarts/Get_started_LiveAPI_tools.html#code-execution",
    "title": "Gemini 2.0 - Multimodal live API: Tool use",
    "section": "Code Execution",
    "text": "Code Execution\nThe codeExecution lets the model write and run python code. Try it on a math problem the model can’t solve from memory:\n\nawait run(\"Can you compute the largest prime palindrome under 100000.\", google.Modality.TEXT, [{ codeExecution: {} }]);\n\nConnection opened\nPrompt:  Can you compute the largest prime palindrome under 100000.\n\n\nOkay, I can help you with that. Here’s my plan:\n\n\n\nGenerate Palindromes: Create a list of all palindromes under 100000.\nCheck for Primality: Iterate through the palindromes and check if each one is prime.\nFind the Largest: Keep track of the largest prime palindrome found so far.\n\n\n\nHere’s the code to do that:\n\n\n\n\n\ndef is_palindrome(n):\n  \"\"\"Checks if a number is a palindrome.\"\"\"\n  return str(n) == str(n)[::-1]\n\n\ndef is_prime(n):\n  \"\"\"Checks if a number is prime.\"\"\"\n  if n &lt; 2:\n    return False\n  for i in range(2, int(n**0.5) + 1):\n    if n % i == 0:\n      return False\n  return True\n\n\nlargest_prime_palindrome = 0\nfor i in range(100000):\n  if is_palindrome(i) and is_prime(i):\n    largest_prime_palindrome = i\n\nprint(largest_prime_palindrome)\n\n\n\n\n\n\n\n\n98689\n\n\n\n\n\n\nThe largest prime palindrome\n\n\nunder 100000 is 98689.\n\n\n\n\n\nSession completed\nConnection closed",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini 2.0 - Multimodal live API: Tool use"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_LiveAPI_tools.html#compositional-function-calling",
    "href": "quickstarts/Get_started_LiveAPI_tools.html#compositional-function-calling",
    "title": "Gemini 2.0 - Multimodal live API: Tool use",
    "section": "Compositional Function Calling",
    "text": "Compositional Function Calling\nCompositional function calling refers to the ability to combine user defined functions with the codeExecution tool. The model will write them into larger blocks of code, and then pause execution while it waits for you to send back responses for each call.\n\nawait run(\"Can you turn on the lights wait 10s and then turn them off?\", google.Modality.TEXT, [\n  ...function_call_tools,\n  { codeExecution: {} },\n]);\n\nConnection opened\nPrompt:  Can you turn on the lights wait 10s and then turn them off?\n\n\n\n\n\nimport time\n\ndefault_api.turn_on_the_lights()\ntime.sleep(10)\ndefault_api.turn_off_the_lights()\n\n\n\n\n\nTool call responses: [\n  {\n    \"id\": \"function-call-448821244251533960\",\n    \"name\": \"turn_on_the_lights\",\n    \"response\": {\n      \"result\": \"ok\"\n    }\n  }\n]\nSession completed\nConnection closed",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini 2.0 - Multimodal live API: Tool use"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_LiveAPI_tools.html#google-search",
    "href": "quickstarts/Get_started_LiveAPI_tools.html#google-search",
    "title": "Gemini 2.0 - Multimodal live API: Tool use",
    "section": "Google search",
    "text": "Google search\nThe googleSearch tool lets the model conduct google searches. For example, try asking it about events that are too recent to be in the training data.\nThe search will still execute in AUDIO mode, but you won’t see the detailed results:\n\nawait run(\n  \"When the latest Brazil vs. Argentina soccer match happened and what was the final score?\",\n  google.Modality.TEXT,\n  [{ googleSearch: {} }]\n);\n\nConnection opened\nPrompt:  When the latest Brazil vs. Argentina soccer match happened and what was the final score?\n\n\n\n\n\nprint(google_search.search(queries=[\"latest Brazil vs Argentina soccer match date and score\", \"Brazil vs Argentina recent match results\"]))\n\n\n\n\n\n\n\n\nLooking up information on Google Search.\n\n\n\n\n\n\nThe most recent match between Brazil and Argentina took place on March 25, 2025, as part of the 2026 FIFA World Cup qualifiers. Argentina won the match with a final score of 4-1.\n\n\n\n\n  \n    \n      \n      \n      \n      \n    \n    \n      \n      \n      \n      \n      \n    \n    \n  \n  \n    Brazil vs Argentina recent match results\n    latest Brazil vs Argentina soccer match date and score\n  \n\n\n\nSession completed\nConnection closed",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini 2.0 - Multimodal live API: Tool use"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_LiveAPI_tools.html#multiple-tools",
    "href": "quickstarts/Get_started_LiveAPI_tools.html#multiple-tools",
    "title": "Gemini 2.0 - Multimodal live API: Tool use",
    "section": "Multiple tools",
    "text": "Multiple tools\nThe biggest difference with the new API however is that you’re no longer limited to using 1-tool per request. Try combining those tasks from the previous sections:\n\nimport { Tool } from \"@google/genai\";\n\nconst multi_tool_prompt = `\n  Hey, I need you to do three things for me.\n\n  1. Then compute the largest prime plaindrome under 100000.\n  2. Then use google search to lookup unformation about the largest earthquake in california the week of Dec 5 2024?\n  3. Turn on the lights\n\n  Thanks!\n`;\nconst multi_tool_tools: Tool[] = [\n  { codeExecution: {} },\n  { googleSearch: {} },\n  { functionDeclarations: [turn_on_the_lights, turn_off_the_lights] },\n];\n\nawait run(multi_tool_prompt, google.Modality.TEXT, multi_tool_tools);\n\nConnection opened\nPrompt:  \n  Hey, I need you to do three things for me.\n\n  1. Then compute the largest prime plaindrome under 100000.\n  2. Then use google search to lookup unformation about the largest earthquake in california the week of Dec 5 2024?\n  3. Turn on the lights\n\n  Thanks!\n\n\n\nOkay, I can do that. Here’s the plan:\n\n\n\nCompute the largest prime palindrome under 100000. I’ll use a Python script to achieve this.\nUse Google Search to look up information about the largest earthquake in California the week of Dec 5 2024.\nTurn on the lights using the provided API.\n\n\n\nHere’s the first step, computing the largest prime palindrome under 100000:\n\n\n\n\n\ndef is_palindrome(n):\n  return str(n) == str(n)[::-1]\n\ndef is_prime(n):\n  if n &lt; 2:\n    return False\n  for i in range(2, int(n**0.5) + 1):\n    if n % i == 0:\n      return False\n  return True\n\nlargest_prime_palindrome = 0\nfor i in range(99999, 1, -1):\n  if is_palindrome(i) and is_prime(i):\n    largest_prime_palindrome = i\n    break\n\nprint(largest_prime_palindrome)\n\n\n\n\n\n98689\n\n\n\n\n\nOkay, the largest prime palindrome under 100000 is 98689.\n\n\nNow, let’s use Google Search to find the largest earthquake in California the week of Dec 5 2024.\n\n\n\n\n\n\n\n\nconcise_search(\"largest earthquake california week of December 5 2024\", max_num_results=5)\n\n\n\n\n\n\n\n\nLooking up information on Google Search.\n\n\n\n\n\n\nBased on the search results, the largest earthquake in California during the week of December 5, 2024, was a magnitude 7.0 earthquake offshore of Cape Mendocino on December 5, 2024, at 10:44 a.m. PST.\n\n\nFinally, I will turn on the lights.\n\n\n\n\n\ndefault_api.turn_on_the_lights()\n\n\n\n\n\n\n\n  \n    \n      \n      \n      \n      \n    \n    \n      \n      \n      \n      \n      \n    \n    \n  \n  \n    largest earthquake california week of December 5 2024\n  \n\n\n\nTool call responses: [\n  {\n    \"id\": \"function-call-10200942088489058256\",\n    \"name\": \"turn_on_the_lights\",\n    \"response\": {\n      \"result\": \"ok\"\n    }\n  }\n]\nSession completed\nConnection closed",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini 2.0 - Multimodal live API: Tool use"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_LiveAPI_tools.html#next-steps",
    "href": "quickstarts/Get_started_LiveAPI_tools.html#next-steps",
    "title": "Gemini 2.0 - Multimodal live API: Tool use",
    "section": "Next Steps",
    "text": "Next Steps\n\nFor more information about the SDK see the SDK docs\nThis tutorial uses the high level SDK, if you’re interested in the lower-level details, try the Websocket version of this tutorial\nThis tutorial only covers basic usage of these tools for deeper (and more fun) example see the Search tool tutorial\n\nOr check the other Gemini 2.0 capabilities from the Cookbook, in particular this other multi-tool example and the one about Gemini spatial capabilities.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini 2.0 - Multimodal live API: Tool use"
    ]
  },
  {
    "objectID": "quickstarts/JSON_mode.html",
    "href": "quickstarts/JSON_mode.html",
    "title": "Gemini API: JSON Mode Quickstart",
    "section": "",
    "text": "The Gemini API can be used to generate a JSON output if you set the schema that you would like to use.\nTwo methods are available. You can either set the desired output in the prompt or supply a schema to the model separately.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: JSON Mode Quickstart"
    ]
  },
  {
    "objectID": "quickstarts/JSON_mode.html#setup",
    "href": "quickstarts/JSON_mode.html#setup",
    "title": "Gemini API: JSON Mode Quickstart",
    "section": "Setup",
    "text": "Setup\n\nInstall the Google GenAI SDK\nInstall the Google GenAI SDK from npm.\n$ npm install @google/genai\n\n\nSetup your API key\nYou can create your API key using Google AI Studio with a single click.\nRemember to treat your API key like a password. Don’t accidentally save it in a notebook or source file you later commit to GitHub. In this notebook we will be storing the API key in a .env file. You can also set it as an environment variable or use a secret manager.\nHere’s how to set it up in a .env file:\n$ touch .env\n$ echo \"GEMINI_API_KEY=&lt;YOUR_API_KEY&gt;\" &gt;&gt; .env\n\n\n\n\n\n\nTip\n\n\n\nAnother option is to set the API key as an environment variable. You can do this in your terminal with the following command:\n$ export GEMINI_API_KEY=\"&lt;YOUR_API_KEY&gt;\"\n\n\n\n\nLoad the API key\nTo load the API key from the .env file, we will use the dotenv package. This package loads environment variables from a .env file into process.env.\n$ npm install dotenv\nThen, we can load the API key in our code:\n\nconst dotenv = require(\"dotenv\") as typeof import(\"dotenv\");\n\ndotenv.config({\n  path: \"../.env\",\n});\n\nconst GEMINI_API_KEY = process.env.GEMINI_API_KEY ?? \"\";\nif (!GEMINI_API_KEY) {\n  throw new Error(\"GEMINI_API_KEY is not set in the environment variables\");\n}\nconsole.log(\"GEMINI_API_KEY is set in the environment variables\");\n\nGEMINI_API_KEY is set in the environment variables\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn our particular case the .env is is one directory up from the notebook, hence we need to use ../ to go up one directory. If the .env file is in the same directory as the notebook, you can omit it altogether.\n│\n├── .env\n└── quickstarts\n    └── JSON_mode.ipynb\n\n\n\n\nInitialize SDK Client\nWith the new SDK, now you only need to initialize a client with you API key (or OAuth if using Vertex AI). The model is now set in each call.\n\nconst google = require(\"@google/genai\") as typeof import(\"@google/genai\");\n\nconst ai = new google.GoogleGenAI({ apiKey: GEMINI_API_KEY });\n\n\n\nSelect a model\nNow select the model you want to use in this guide, either by selecting one in the list or writing it down. Keep in mind that some models, like the 2.5 ones are thinking models and thus take slightly more time to respond (cf. thinking notebook for more details and in particular learn how to switch the thiking off).\n\nconst tslab = require(\"tslab\") as typeof import(\"tslab\");\n\nconst MODEL_ID = \"gemini-2.5-flash-preview-05-20\";",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: JSON Mode Quickstart"
    ]
  },
  {
    "objectID": "quickstarts/JSON_mode.html#set-your-constrained-output-in-the-prompt",
    "href": "quickstarts/JSON_mode.html#set-your-constrained-output-in-the-prompt",
    "title": "Gemini API: JSON Mode Quickstart",
    "section": "Set your constrained output in the prompt",
    "text": "Set your constrained output in the prompt\nFor this first example just describe the schema you want back in the prompt:\n\nconst json_prompt = `\n  List a few popular cookie recipes using this JSON schema:\n\n  Recipe = {'recipe_name': str}\n  Return: list[Recipe]\n`;\n\nThen activate JSON mode by specifying resposeMimeType in the config parameter:\n\nconst json_prompt_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: json_prompt,\n  config: {\n    responseMimeType: \"application/json\",\n  },\n});\n// Parse the JSON response\nconsole.log(\"JSON Response\");\nconsole.log(JSON.parse(json_prompt_response.text!));\n\nJSON Response\n[\n  { recipe_name: 'Chocolate Chip Cookies' },\n  { recipe_name: 'Oatmeal Raisin Cookies' },\n  { recipe_name: 'Peanut Butter Cookies' },\n  { recipe_name: 'Sugar Cookies' },\n  { recipe_name: 'Snickerdoodles' }\n]",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: JSON Mode Quickstart"
    ]
  },
  {
    "objectID": "quickstarts/JSON_mode.html#supply-the-schema-to-the-model-directly",
    "href": "quickstarts/JSON_mode.html#supply-the-schema-to-the-model-directly",
    "title": "Gemini API: JSON Mode Quickstart",
    "section": "Supply the schema to the model directly",
    "text": "Supply the schema to the model directly\nThe newest models (1.5 and beyond) allow you to pass a Schema object directly and the output will strictly follow that schema.\nFollowing the same example as the previous section, here’s that recipe type:\n\nimport { Schema, Type } from \"@google/genai\";\n\nconst recipeSchema: Schema = {\n  type: Type.OBJECT,\n  description: \"A recipe object\",\n  properties: {\n    recipeName: {\n      type: Type.STRING,\n      description: \"The name of the recipe\",\n    },\n    recipeDescription: {\n      type: Type.STRING,\n      description: \"A brief description of the recipe\",\n    },\n    recipeIngredients: {\n      type: Type.ARRAY,\n      description: \"A list of ingredients for the recipe\",\n      items: {\n        type: Type.STRING,\n        description: \"An ingredient for the recipe\",\n      },\n    },\n  },\n  required: [\"recipeName\", \"recipeDescription\", \"recipeIngredients\"],\n};\n\nconst listRecipesSchema: Schema = {\n  type: Type.ARRAY,\n  description: \"A list of recipes\",\n  items: recipeSchema,\n};\n\nconst json_schema_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents:\n    \"List a few imaginative cookie recipes along with a one-sentence description as if you were a gourmet restaurant and their main ingredients\",\n  config: {\n    responseMimeType: \"application/json\",\n    responseSchema: listRecipesSchema,\n  },\n});\nconsole.log(\"JSON Schema Response\");\nconsole.log(JSON.parse(json_schema_response.text!));\n\nJSON Schema Response\n[\n  {\n    recipeDescription: 'A celestial delight, these cookies shimmer with edible glitter and a hint of lavender, promising an otherworldly experience with every ethereal bite.',\n    recipeIngredients: [\n      'All-purpose flour',\n      'Unsalted butter',\n      'Granulated sugar',\n      'Edible glitter',\n      'Culinary lavender',\n      'Eggs'\n    ],\n    recipeName: 'Stardust Serenade Cookies'\n  },\n  {\n    recipeDescription: 'Indulge in the opulent depth of these intensely dark chocolate cookies, encasing a molten truffle heart that melts sensuously on the palate.',\n    recipeIngredients: [\n      'Dark cocoa powder',\n      'Unsalted butter',\n      'Brown sugar',\n      'Granulated sugar',\n      'Chocolate truffles',\n      'Sea salt'\n    ],\n    recipeName: 'Velvet Midnight Truffle Cookies'\n  },\n  {\n    recipeDescription: 'These exquisite biscuits offer a harmonious blend of warm spices and tart dried cranberries, evoking a vibrant tapestry of autumnal flavors.',\n    recipeIngredients: [\n      'All-purpose flour',\n      'Unsalted butter',\n      'Brown sugar',\n      'Cinnamon',\n      'Dried cranberries',\n      'Orange zest'\n    ],\n    recipeName: 'Spiced Ruby Blossom Biscuits'\n  }\n]",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: JSON Mode Quickstart"
    ]
  },
  {
    "objectID": "quickstarts/JSON_mode.html#next-steps",
    "href": "quickstarts/JSON_mode.html#next-steps",
    "title": "Gemini API: JSON Mode Quickstart",
    "section": "Next Steps",
    "text": "Next Steps\n\nUseful API references:\nCheck the structured ouput documentation or the GenerationConfig API reference for more details\n\n\nRelated examples\n\nThe constrained output is used in the Text summarization example to provide the model a format to summarize a story (genre, characters, etc…)\nThe Object detection examples are using the JSON constrained output to uniiformize the output of the detection.\n\n\n\nContinue your discovery of the Gemini API\nJSON is not the only way to constrain the output of the model, you can also use an Enum, Function calling and Code execution are other ways to enhance your model by either using your own functions or by letting the model write and run them.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: JSON Mode Quickstart"
    ]
  },
  {
    "objectID": "quickstarts/PDF_Files.html",
    "href": "quickstarts/PDF_Files.html",
    "title": "Gemini API: Read a PDF",
    "section": "",
    "text": "This notebook demonstrates how you can convert a PDF file so that it can be read by the Gemini API.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Read a PDF"
    ]
  },
  {
    "objectID": "quickstarts/PDF_Files.html#setup",
    "href": "quickstarts/PDF_Files.html#setup",
    "title": "Gemini API: Read a PDF",
    "section": "Setup",
    "text": "Setup\n\nInstall the Google GenAI SDK\nInstall the Google GenAI SDK from npm.\n$ npm install @google/genai\n\n\nSetup your API key\nYou can create your API key using Google AI Studio with a single click.\nRemember to treat your API key like a password. Don’t accidentally save it in a notebook or source file you later commit to GitHub. In this notebook we will be storing the API key in a .env file. You can also set it as an environment variable or use a secret manager.\nHere’s how to set it up in a .env file:\n$ touch .env\n$ echo \"GEMINI_API_KEY=&lt;YOUR_API_KEY&gt;\" &gt;&gt; .env\n\n\n\n\n\n\nTip\n\n\n\nAnother option is to set the API key as an environment variable. You can do this in your terminal with the following command:\n$ export GEMINI_API_KEY=\"&lt;YOUR_API_KEY&gt;\"\n\n\n\n\nLoad the API key\nTo load the API key from the .env file, we will use the dotenv package. This package loads environment variables from a .env file into process.env.\n$ npm install dotenv\nThen, we can load the API key in our code:\n\nconst dotenv = require(\"dotenv\") as typeof import(\"dotenv\");\n\ndotenv.config({\n  path: \"../.env\",\n});\n\nconst GEMINI_API_KEY = process.env.GEMINI_API_KEY ?? \"\";\nif (!GEMINI_API_KEY) {\n  throw new Error(\"GEMINI_API_KEY is not set in the environment variables\");\n}\nconsole.log(\"GEMINI_API_KEY is set in the environment variables\");\n\nGEMINI_API_KEY is set in the environment variables\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn our particular case the .env is is one directory up from the notebook, hence we need to use ../ to go up one directory. If the .env file is in the same directory as the notebook, you can omit it altogether.\n│\n├── .env\n└── quickstarts\n    └── Get_started_TTS.ipynb\n\n\n\n\nInitialize SDK Client\nWith the new SDK, now you only need to initialize a client with you API key (or OAuth if using Vertex AI). The model is now set in each call.\n\nconst google = require(\"@google/genai\") as typeof import(\"@google/genai\");\n\nconst ai = new google.GoogleGenAI({ apiKey: GEMINI_API_KEY });\n\n\n\nSelect a model\nNow select the model you want to use in this guide, either by selecting one in the list or writing it down. Keep in mind that some models, like the 2.5 ones are thinking models and thus take slightly more time to respond (cf. thinking notebook for more details and in particular learn how to switch the thiking off).\n\nconst tslab = require(\"tslab\") as typeof import(\"tslab\");\n\nconst MODEL_ID = \"gemini-2.5-flash-preview-05-20\";",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Read a PDF"
    ]
  },
  {
    "objectID": "quickstarts/PDF_Files.html#download-and-inspect-the-pdf",
    "href": "quickstarts/PDF_Files.html#download-and-inspect-the-pdf",
    "title": "Gemini API: Read a PDF",
    "section": "Download and inspect the PDF",
    "text": "Download and inspect the PDF\nThis PDF page is an article titled Smoothly editing material properties of objects with text-to-image models and synthetic data available on the Google Research Blog.\n\nconst fs = require(\"fs\") as typeof import(\"fs\");\nconst path = require(\"path\") as typeof import(\"path\");\n\nconst downloadFile = async (url: string, filePath: string) =&gt; {\n  const response = await fetch(url);\n  if (!response.ok) {\n    throw new Error(`Failed to download image: ${response.statusText}`);\n  }\n  const buffer = await response.blob();\n  const bufferData = Buffer.from(await buffer.arrayBuffer());\n  fs.writeFileSync(filePath, bufferData);\n};\n\n\nconst PDF_URL =\n  \"https://storage.googleapis.com/generativeai-downloads/data/Smoothly%20editing%20material%20properties%20of%20objects%20with%20text-to-image%20models%20and%20synthetic%20data.pdf\";\n\nconst pdfPath = path.join(\"../assets\", \"article.pdf\");\nawait downloadFile(PDF_URL, pdfPath);\n\nLook at one of the pages:\n\nconst getPage = async (pdfPath: string, pageNumber: number): Promise&lt;Uint8Array&gt; =&gt; {\n  const { pdf } = await import(\"pdf-to-img\");\n  const document = await pdf(pdfPath);\n  let pageIndex = 0;\n  for await (const page of document) {\n    if (pageIndex === pageNumber) {\n      return new Uint8Array(page);\n    }\n    pageIndex++;\n  }\n  throw new Error(`Page ${pageNumber} not found in the PDF document.`);\n};\n\ntslab.display.png(await getPage(pdfPath, 0));",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Read a PDF"
    ]
  },
  {
    "objectID": "quickstarts/PDF_Files.html#upload-the-file-to-the-api",
    "href": "quickstarts/PDF_Files.html#upload-the-file-to-the-api",
    "title": "Gemini API: Read a PDF",
    "section": "Upload the file to the API",
    "text": "Upload the file to the API\nStart by uploading the PDF using the File API.\n\nconst file_response = await ai.files.upload({\n  file: pdfPath,\n  config: {\n    displayName: \"article.pdf\",\n    mimeType: \"application/pdf\",\n  },\n});",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Read a PDF"
    ]
  },
  {
    "objectID": "quickstarts/PDF_Files.html#try-it-out",
    "href": "quickstarts/PDF_Files.html#try-it-out",
    "title": "Gemini API: Read a PDF",
    "section": "Try it out",
    "text": "Try it out\nThe pages of the PDF file are each passed to the model as a screenshot of the page plus the text extracted by OCR.\n\nconst tokens_response = await ai.models.countTokens({\n  model: MODEL_ID,\n  contents: [\n    google.createPartFromUri(file_response.uri ?? \"\", file_response.mimeType ?? \"\"),\n    \"Can you summarize this file as a bulleted list?\",\n  ],\n});\ntslab.display.text(`Token count: ${tokens_response.totalTokens}`);\n\nToken count: 1560\n\n\n\nconst content_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: [\n    google.createPartFromUri(file_response.uri ?? \"\", file_response.mimeType ?? \"\"),\n    \"Can you summarize this file as a bulleted list?\",\n  ],\n});\ntslab.display.markdown(content_response.text ?? \"\");\n\nHere’s a summary of the provided document as a bulleted list:\n\nCore Innovation: Introduces a method to smoothly edit material properties (like color, shininess, transparency) of objects within any photograph.\nLeverages T2I Models: Augments existing generative text-to-image (T2I) models, specifically fine-tuning Stable Diffusion 1.5.\nSynthetic Data Training: The model is trained on a novel synthetic dataset created using computer graphics and physically based rendering. This dataset features 100 3D household objects, showing various material attributes changed with different “edit strengths” while keeping object shape, lighting, and camera angle constant.\nParametric Control: Users gain precise, parametric control over material attributes, defined by a scalar edit strength (e.g., -1 for minimum change, +1 for maximum change).\nPreserves Shape & Lighting: A key achievement is the ability to edit material properties without altering the object’s geometric shape or the scene’s lighting conditions.\nPhotorealistic Results: The method produces highly photorealistic edits, including realistic background fills and caustic effects for transparency changes.\nGeneralization to Real-World Images: Despite being trained on a relatively small synthetic dataset, the model generalizes effectively to real-world input images.\nSuperior User Preference: A user study showed that the method’s edits were significantly more photorealistic and preferred compared to a baseline method (InstructPix2Pix).\nApplications: Potential uses include visualizing room changes, mocking up new product designs for architects/designers, and achieving 3D consistent material edits when combined with NeRF for synthesizing new views.\nPublication: The work is detailed in the paper “Alchemist: Parametric Control of Material Properties with Diffusion Models,” published at CVPR 2024.\n\n\n\nIn addition, take a look at how the Gemini model responds when you ask questions about the images within the PDF.\n\nconst image_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: [\n    google.createPartFromUri(file_response.uri ?? \"\", file_response.mimeType ?? \"\"),\n    \"Can you explain the images on the first page of the document?\",\n  ],\n});\ntslab.display.markdown(image_response.text ?? \"\");\n\nThe images on the first page of the document serve as key examples demonstrating the core capability of the presented method: smoothly editing the material properties of objects in real-world photographs using text-to-image models.\nHere’s a breakdown of what each set of images illustrates:\n\nOverall Layout:\n\nThe page features four pairs of images, each labeled “Input” and “Output.”\nBelow each “Input” image is a text prompt, which is the instruction given to the model to perform the edit.\n\nPurpose:\n\nThe “Input” images are original, real-world photographs of various objects.\nThe “Output” images show the result after the model applies the specified material edit, aiming to preserve the object’s shape, lighting, and context while changing its material appearance.\n\nSpecific Examples:\n\nTop Left (Teapot):\n\nInput: A brown, relatively matte or rough-looking teapot.\nPrompt: “change the roughness of the teapot.”\nOutput: The same teapot, but its material has been changed to appear much smoother and shinier, reflecting light more sharply and clearly. This demonstrates the ability to control the roughness (or shininess/glossiness) property.\n\nTop Right (Cupid Statue):\n\nInput: An opaque, white-colored statue of Cupid.\nPrompt: “change the transparency of the cupid statue.”\nOutput: The statue now appears translucent or semi-transparent, allowing the background to be seen through it. This showcases the model’s ability to manipulate the transparency property.\n\nBottom Left (Pot):\n\nInput: A gray, non-metallic pot.\nPrompt: “change the metallic of the pot.”\nOutput: The pot has been transformed to look like a highly reflective, metallic object, accurately reflecting its surroundings. This highlights the control over the metallic appearance property.\n\nBottom Right (Buddha Statue):\n\nInput: A golden-yellow Buddha statue.\nPrompt: “change the albedo of the Buddha statue.”\nOutput: The statue’s base color (albedo) has been changed to a lighter, possibly white or light-gray, while its shape and the scene’s lighting remain consistent. This illustrates the ability to modify the albedo (base color) property.\n\n\n\nIn summary, the images on the first page vividly demonstrate how the proposed method can take an existing photograph and parametrically alter specific material attributes (like roughness, transparency, metallicity, and base color) of objects within it, producing photorealistic results while maintaining the object’s original form and lighting conditions.\n\n\nIf you observe the area of the header of the article, you can see that the model captures what is happening.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Read a PDF"
    ]
  },
  {
    "objectID": "quickstarts/PDF_Files.html#learning-more",
    "href": "quickstarts/PDF_Files.html#learning-more",
    "title": "Gemini API: Read a PDF",
    "section": "Learning more",
    "text": "Learning more\nThe File API lets you upload a variety of multimodal MIME types, including images, audio, and video formats. The File API handles inputs that can be used to generate content with model.generateContent or model.generateContentStream.\nThe File API accepts files under 2GB in size and can store up to 20GB of files per project. Files last for 2 days and cannot be downloaded from the API.\n\nLearn more about prompting with media files in the docs, including the supported formats and maximum length.\nLearn more about to extract structured outputs from PDFs in the Structured outputs on invoices and forms example.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Read a PDF"
    ]
  },
  {
    "objectID": "quickstarts/Code_Execution.html",
    "href": "quickstarts/Code_Execution.html",
    "title": "Gemini API: Code Execution",
    "section": "",
    "text": "The Gemini API code execution feature enables the model to generate and run Python code based on plain-text instructions that you give it, and even output graphs. It can learn iteratively from the results until it arrives at a final output.\nThis notebook is a walk through:",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Code Execution"
    ]
  },
  {
    "objectID": "quickstarts/Code_Execution.html#setup",
    "href": "quickstarts/Code_Execution.html#setup",
    "title": "Gemini API: Code Execution",
    "section": "Setup",
    "text": "Setup\n\nInstall the Google GenAI SDK\nInstall the Google GenAI SDK from npm.\n$ npm install @google/genai\n\n\nSetup your API key\nYou can create your API key using Google AI Studio with a single click.\nRemember to treat your API key like a password. Don’t accidentally save it in a notebook or source file you later commit to GitHub. In this notebook we will be storing the API key in a .env file. You can also set it as an environment variable or use a secret manager.\nHere’s how to set it up in a .env file:\n$ touch .env\n$ echo \"GEMINI_API_KEY=&lt;YOUR_API_KEY&gt;\" &gt;&gt; .env\n\n\n\n\n\n\nTip\n\n\n\nAnother option is to set the API key as an environment variable. You can do this in your terminal with the following command:\n$ export GEMINI_API_KEY=\"&lt;YOUR_API_KEY&gt;\"\n\n\n\n\nLoad the API key\nTo load the API key from the .env file, we will use the dotenv package. This package loads environment variables from a .env file into process.env.\n$ npm install dotenv\nThen, we can load the API key in our code:\n\nconst dotenv = require(\"dotenv\") as typeof import(\"dotenv\");\n\ndotenv.config({\n  path: \"../.env\",\n});\n\nconst GEMINI_API_KEY = process.env.GEMINI_API_KEY ?? \"\";\nif (!GEMINI_API_KEY) {\n  throw new Error(\"GEMINI_API_KEY is not set in the environment variables\");\n}\nconsole.log(\"GEMINI_API_KEY is set in the environment variables\");\n\nGEMINI_API_KEY is set in the environment variables\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn our particular case the .env is is one directory up from the notebook, hence we need to use ../ to go up one directory. If the .env file is in the same directory as the notebook, you can omit it altogether.\n│\n├── .env\n└── quickstarts\n    └── Code_Execution.ipynb\n\n\n\n\nInitialize SDK Client\nWith the new SDK, now you only need to initialize a client with you API key (or OAuth if using Vertex AI). The model is now set in each call.\n\nconst google = require(\"@google/genai\") as typeof import(\"@google/genai\");\n\nconst ai = new google.GoogleGenAI({ apiKey: GEMINI_API_KEY });\n\n\n\nSelect a model\nNow select the model you want to use in this guide, either by selecting one in the list or writing it down. Keep in mind that some models, like the 2.5 ones are thinking models and thus take slightly more time to respond (cf. thinking notebook for more details and in particular learn how to switch the thiking off).\n\nconst tslab = require(\"tslab\") as typeof import(\"tslab\");\n\nconst MODEL_ID = \"gemini-2.5-flash-preview-05-20\";",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Code Execution"
    ]
  },
  {
    "objectID": "quickstarts/Code_Execution.html#utilites",
    "href": "quickstarts/Code_Execution.html#utilites",
    "title": "Gemini API: Code Execution",
    "section": "Utilites",
    "text": "Utilites\nWhen using code execution as a tool, the model returns a list of parts including text, executableCode, executionResult, and inlineData parts. Use the function below to help you visualize and better display the code execution results. Here are a few details about the different fields of the results:\n\ntext: Inline text generated by the model.\nexecutableCode: Code generated by the model that is meant to be executed.\ncodeExecutionResult: Result of the executable_code.\ninlineData: Inline media generated by the model.\n\n\nimport { GenerateContentResponse } from \"@google/genai\";\n\nfunction displayCodeExecutionResult(code_response: GenerateContentResponse) {\n  const code_response_parts = (code_response.candidates ? code_response.candidates[0]?.content?.parts : []) ?? [];\n  for (const part of code_response_parts) {\n    if (part.text) {\n      tslab.display.markdown(part.text);\n    }\n    if (part.executableCode) {\n      tslab.display.markdown(`\\`\\`\\`python\\n${part.executableCode.code}\\n\\`\\`\\``);\n    }\n    if (part.codeExecutionResult) {\n      tslab.display.markdown(`\\`\\`\\`\\n${part.codeExecutionResult.output}\\n\\`\\`\\``);\n    }\n    if (part.inlineData) {\n      const imageData = part.inlineData.data!;\n      const buffer = Buffer.from(imageData, \"base64\");\n      tslab.display.png(buffer);\n    }\n  }\n}",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Code Execution"
    ]
  },
  {
    "objectID": "quickstarts/Code_Execution.html#use-codeexecution-with-a-single-call",
    "href": "quickstarts/Code_Execution.html#use-codeexecution-with-a-single-call",
    "title": "Gemini API: Code Execution",
    "section": "Use codeExecution with a single call",
    "text": "Use codeExecution with a single call\nWhen initiating the model, pass codeExecution as a tool to tell the model that it is allowed to generate and run code.\n\nconst code_response_1 = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: [\n    `\n    What is the sum of the first 50 prime numbers?\n    Generate and run code for the calculation, and make sure you get all 50.\n    `,\n  ],\n  config: {\n    tools: [{ codeExecution: {} }],\n  },\n});\ndisplayCodeExecutionResult(code_response_1);\n\nTo find the sum of the first 50 prime numbers, I will generate the primes using a primality test function and then sum them up.\n\n\nimport math\n\ndef is_prime(n):\n    \"\"\"Checks if a number is prime.\"\"\"\n    if n &lt; 2:\n        return False\n    if n == 2:\n        return True\n    if n % 2 == 0:\n        return False\n    for i in range(3, int(math.sqrt(n)) + 1, 2):\n        if n % i == 0:\n            return False\n    return True\n\nprimes = []\nnum = 2\nwhile len(primes) &lt; 50:\n    if is_prime(num):\n        primes.append(num)\n    num += 1\n\nsum_of_primes = sum(primes)\n\nprint(f\"The first 50 prime numbers are: {primes}\")\nprint(f\"The count of prime numbers found is: {len(primes)}\")\nprint(f\"The sum of the first 50 prime numbers is: {sum_of_primes}\")\n\n\nThe first 50 prime numbers are: [2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97, 101, 103, 107, 109, 113, 127, 131, 137, 139, 149, 151, 157, 163, 167, 173, 179, 181, 191, 193, 197, 199, 211, 223, 227, 229]\nThe count of prime numbers found is: 50\nThe sum of the first 50 prime numbers is: 5117\n\n\n\nThe first 50 prime numbers have been identified and listed, and their count has been verified to be 50.\nThe sum of the first 50 prime numbers is 5117.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Code Execution"
    ]
  },
  {
    "objectID": "quickstarts/Code_Execution.html#code-execution-with-file-io",
    "href": "quickstarts/Code_Execution.html#code-execution-with-file-io",
    "title": "Gemini API: Code Execution",
    "section": "Code execution with File I/O",
    "text": "Code execution with File I/O\nThe dataset you will use in this guide comes from the StatLib from the Department of Statistics at Carnegie Mellon University. It is made available by the scikit-learn under the 3-Clause BSD license.\nIt provides 20k information on various blocks in Californina, including the location (longitute/lattitude), average income, housing average age, average rooms, average bedrooms, population, average occupation.\nHere’s a breakdown of the columns and what the attributes represent:\n\nMedInc: median income in block group\nHouseAge: median house age in block group\nAveRooms: average number of rooms per household\nAveBedrms: average number of bedrooms per household\nPopulation: block group population\nAveOccup: average number of household members\nLatitude: block group latitude\nLongitude: block group longitude\n\n\n\n\n\n\n\nNote\n\n\n\nCode execution functionality works best with a .csv or .txt file.\n\n\n\nconst danfojs = require(\"danfojs-node\") as typeof import(\"danfojs-node\");\n\nconst DATASET_URL =\n  \"https://raw.githubusercontent.com/javascriptdata/scikit.js/refs/heads/main/docs/static/data/california_housing.csv\";\n\nconsole.log(\"Loading California housing dataset from:\", DATASET_URL);\n\nconst california_housing = await danfojs.readCSV(DATASET_URL);\n\nLoading California housing dataset from: https://raw.githubusercontent.com/javascriptdata/scikit.js/refs/heads/main/docs/static/data/california_housing.csv\n\n\n\nconst path = require(\"path\") as typeof import(\"path\");\n\nconst houses_data = california_housing.head(5000);\nhouses_data.print();\nconst csvFilePath = path.join(\"../assets\", \"california_housing.csv\");\ndanfojs.toCSV(houses_data, { filePath: csvFilePath });\n\n╔════════════╤═══════════════════╤═══════════════════╤═══════════════════╤═══════════════════╤═══════════════════╤═══════════════════╤═══════════════════╤═══════════════════╤═══════════════════╗\n║            │ MedInc            │ HouseAge          │ AveRooms          │ AveBedrms         │ Population        │ AveOccup          │ Latitude          │ Longitude         │ MedHouseVal       ║\n╟────────────┼───────────────────┼───────────────────┼───────────────────┼───────────────────┼───────────────────┼───────────────────┼───────────────────┼───────────────────┼───────────────────╢\n║ 0          │ 8.3252            │ 41                │ 6.9841269841269…  │ 1.0238095238095…  │ 322               │ 2.5555555555555…  │ 37.88             │ -122.23           │ 4.526             ║\n╟────────────┼───────────────────┼───────────────────┼───────────────────┼───────────────────┼───────────────────┼───────────────────┼───────────────────┼───────────────────┼───────────────────╢\n║ 1          │ 8.3014            │ 21                │ 6.2381370826010…  │ 0.9718804920913…  │ 2401              │ 2.1098418277680…  │ 37.86             │ -122.22           │ 3.585             ║\n╟────────────┼───────────────────┼───────────────────┼───────────────────┼───────────────────┼───────────────────┼───────────────────┼───────────────────┼───────────────────┼───────────────────╢\n║ 2          │ 7.2574            │ 52                │ 8.2881355932203…  │ 1.0734463276836…  │ 496               │ 2.8022598870056…  │ 37.85             │ -122.24           │ 3.521             ║\n╟────────────┼───────────────────┼───────────────────┼───────────────────┼───────────────────┼───────────────────┼───────────────────┼───────────────────┼───────────────────┼───────────────────╢\n║ 3          │ 5.6431            │ 52                │ 5.8173515981735…  │ 1.0730593607305…  │ 558               │ 2.5479452054794…  │ 37.85             │ -122.25           │ 3.413             ║\n╟────────────┼───────────────────┼───────────────────┼───────────────────┼───────────────────┼───────────────────┼───────────────────┼───────────────────┼───────────────────┼───────────────────╢\n║ 4          │ 3.8462            │ 52                │ 6.2818532818532…  │ 1.0810810810810…  │ 565               │ 2.1814671814671…  │ 37.85             │ -122.25           │ 3.422             ║\n╟────────────┼───────────────────┼───────────────────┼───────────────────┼───────────────────┼───────────────────┼───────────────────┼───────────────────┼───────────────────┼───────────────────╢\n║ 5          │ 4.0368            │ 52                │ 4.7616580310880…  │ 1.1036269430051…  │ 413               │ 2.1398963730569…  │ 37.85             │ -122.25           │ 2.697             ║\n╟────────────┼───────────────────┼───────────────────┼───────────────────┼───────────────────┼───────────────────┼───────────────────┼───────────────────┼───────────────────┼───────────────────╢\n║ 6          │ 3.6591            │ 52                │ 4.9319066147859…  │ 0.9513618677042…  │ 1094              │ 2.1284046692607…  │ 37.84             │ -122.25           │ 2.992             ║\n╟────────────┼───────────────────┼───────────────────┼───────────────────┼───────────────────┼───────────────────┼───────────────────┼───────────────────┼───────────────────┼───────────────────╢\n║ 7          │ 3.12              │ 52                │ 4.7975270479134…  │ 1.0618238021638…  │ 1157              │ 1.7882534775888…  │ 37.84             │ -122.25           │ 2.414             ║\n╟────────────┼───────────────────┼───────────────────┼───────────────────┼───────────────────┼───────────────────┼───────────────────┼───────────────────┼───────────────────┼───────────────────╢\n║ 8          │ 2.0804            │ 42                │ 4.2941176470588…  │ 1.1176470588235…  │ 1206              │ 2.0268907563025…  │ 37.84             │ -122.26           │ 2.267             ║\n╟────────────┼───────────────────┼───────────────────┼───────────────────┼───────────────────┼───────────────────┼───────────────────┼───────────────────┼───────────────────┼───────────────────╢\n║ 9          │ 3.6912            │ 52                │ 4.9705882352941…  │ 0.9901960784313…  │ 1551              │ 2.1722689075630…  │ 37.84             │ -122.25           │ 2.611             ║\n╚════════════╧═══════════════════╧═══════════════════╧═══════════════════╧═══════════════════╧═══════════════════╧═══════════════════╧═══════════════════╧═══════════════════╧═══════════════════╝\n\n\n\n\nconst houses_file = await ai.files.upload({\n  file: csvFilePath,\n  config: {\n    displayName: \"california_housing.csv\",\n    mimeType: \"text/csv\",\n  },\n});\nconsole.log(\"Uploaded file:\", JSON.stringify(houses_file, null, 2));\n\nUploaded file: {\n  \"name\": \"files/6k53cudpplen\",\n  \"displayName\": \"california_housing.csv\",\n  \"mimeType\": \"text/csv\",\n  \"sizeBytes\": \"437526\",\n  \"createTime\": \"2025-06-12T18:21:21.069662Z\",\n  \"expirationTime\": \"2025-06-14T18:21:20.867856110Z\",\n  \"updateTime\": \"2025-06-12T18:21:21.069662Z\",\n  \"sha256Hash\": \"ZDkxMjI4MWI1MTZkMmYyOWE0YjQzNTBjZDkzYWMxMjY3OGFhN2E3NDcxYTkyYTI3NjRlMWNjNDY0MDdkNDk4Yg==\",\n  \"uri\": \"https://generativelanguage.googleapis.com/v1beta/files/6k53cudpplen\",\n  \"state\": \"ACTIVE\",\n  \"source\": \"UPLOADED\"\n}\n\n\nLet’s try several queries about the dataset that you have. Starting off, it would be interesting to see the most expensive blocks and check wether there’s abnomal data.\n\nconst dataset_response_1 = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: [\n    \"This dataset provides information on various blocks in Californina.\",\n    \"Generate a scatterplot comparing the houses age with the median house value for the top-20 most expensive blocks.\",\n    \"Use each black as a different color, and include a legend of what each color represents.\",\n    \"Plot the age as the x-axis, and the median house value as the y-axis.\",\n    \"In addition, point out on the graph which points could be anomalies? Circle the anomaly in red on the graph.\",\n    \"Then save the plot as an image file and display the image.\",\n    google.createPartFromUri(houses_file.uri ?? \"\", houses_file.mimeType ?? \"\"),\n  ],\n  config: {\n    tools: [{ codeExecution: {} }],\n  },\n});\ndisplayCodeExecutionResult(dataset_response_1);\n\nimport pandas as pd\n\n# Load the dataset\ndf = pd.read_csv('input_file_0.csv')\n\n# Display basic information about the DataFrame\nprint(\"DataFrame Info:\")\nprint(df.info())\n\n# Display the first few rows of the DataFrame\nprint(\"\\nDataFrame Head:\")\nprint(df.head())\n\n\nDataFrame Info:\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 5000 entries, 0 to 4999\nData columns (total 9 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   MedInc       5000 non-null   float64\n 1   HouseAge     5000 non-null   int64  \n 2   AveRooms     5000 non-null   float64\n 3   AveBedrms    5000 non-null   float64\n 4   Population   5000 non-null   int64  \n 5   AveOccup     5000 non-null   float64\n 6   Latitude     5000 non-null   float64\n 7   Longitude    5000 non-null   float64\n 8   MedHouseVal  5000 non-null   float64\ndtypes: float64(7), int64(2)\nmemory usage: 351.7 KB\nNone\n\nDataFrame Head:\n   MedInc  HouseAge  AveRooms  ...  Latitude  Longitude  MedHouseVal\n0  8.3252        41  6.984127  ...     37.88    -122.23        4.526\n1  8.3014        21  6.238137  ...     37.86    -122.22        3.585\n2  7.2574        52  8.288136  ...     37.85    -122.24        3.521\n3  5.6431        52  5.817352  ...     37.85    -122.25        3.413\n4  3.8462        52  6.281853  ...     37.85    -122.25        3.422\n\n[5 rows x 9 columns]\n\n\n\n# Sort the DataFrame by 'MedHouseVal' in descending order and select the top 20\ntop_20_expensive_blocks = df.nlargest(20, 'MedHouseVal')\n\nprint(\"\\nTop 20 Most Expensive Blocks (Head):\")\nprint(top_20_expensive_blocks.head())\nprint(\"\\nTop 20 Most Expensive Blocks (Tail):\")\nprint(top_20_expensive_blocks.tail())\n\n\n\nTop 20 Most Expensive Blocks (Head):\n     MedInc  HouseAge  AveRooms  ...  Latitude  Longitude  MedHouseVal\n89   1.2434        52  2.929412  ...     37.80    -122.27      5.00001\n459  1.1696        52  2.436000  ...     37.87    -122.25      5.00001\n493  7.8521        52  7.794393  ...     37.86    -122.24      5.00001\n494  9.3959        52  7.512097  ...     37.85    -122.24      5.00001\n509  7.8772        52  8.282548  ...     37.83    -122.23      5.00001\n\n[5 rows x 9 columns]\n\nTop 20 Most Expensive Blocks (Tail):\n       MedInc  HouseAge  AveRooms  ...  Latitude  Longitude  MedHouseVal\n1585   9.5271        16  8.459251  ...     37.84    -122.00      5.00001\n1586  12.2478        10  8.186224  ...     37.85    -121.96      5.00001\n1591  10.4549        22  8.388136  ...     37.94    -122.12      5.00001\n1593  10.3224        32  7.838798  ...     37.90    -122.14      5.00001\n1617  11.7064        17  9.361702  ...     37.84    -122.08      5.00001\n\n[5 rows x 9 columns]\n\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Create the scatterplot\nplt.figure(figsize=(12, 8))\nsns.scatterplot(\n    data=top_20_expensive_blocks,\n    x='HouseAge',\n    y='MedHouseVal',\n    hue=top_20_expensive_blocks.index, # Use DataFrame index as hue for unique colors/labels\n    palette='tab20', # Use a palette with enough distinct colors for 20 points\n    s=100, # Size of the points\n    legend='full' # Display the full legend\n)\n\n# Add title and labels\nplt.title('House Age vs. Median House Value for Top 20 Most Expensive Blocks', fontsize=16)\nplt.xlabel('House Age', fontsize=12)\nplt.ylabel('Median House Value (in 100,000s USD)', fontsize=12)\n\n# Adjust legend placement to avoid overlapping with data points\nplt.legend(title='Block Index', bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)\n\n# Identify potential anomalies.\n# Visually, points that significantly deviate from the general cluster could be anomalies.\n# From the head/tail, many have MedHouseVal of 5.00001. Let's look for points that have\n# a very low HouseAge but high MedHouseVal, or very high HouseAge with lower MedHouseVal\n# compared to others at that age, if a trend exists.\n# Based on the printed data, most top values are capped at 5.00001.\n# Let's consider points with HouseAge significantly lower than others at this MedHouseVal\n# or significantly higher HouseAge but similar MedHouseVal as potential anomalies.\n\n# Let's re-examine the top 20 data to choose some potential anomalies based on HouseAge.\n# For example, if a block is very young (low HouseAge) but has a max value, it might be an anomaly.\n# Or an old block with a max value.\n\n# Re-display the top 20 blocks to pick some specific indices for anomalies\nprint(\"\\nTop 20 Most Expensive Blocks for Anomaly Selection:\")\nprint(top_20_expensive_blocks[['HouseAge', 'MedHouseVal']])\n\n# I will manually pick some potential anomalies after examining the data.\n# For example, blocks with 'HouseAge' on the lower end but still having 'MedHouseVal' 5.00001,\n# or very high 'HouseAge' but also having 'MedHouseVal' 5.00001, could be considered.\n# Let's pick a few with diverse `HouseAge` from the `MedHouseVal` 5.00001 group.\n\n# Example potential anomalies (indices from the original DataFrame based on the printed data)\n# Let's consider 89 (HouseAge 52, MedHouseVal 5.00001) - very old but max value\n# 1586 (HouseAge 10, MedHouseVal 5.00001) - relatively young but max value\n# 1617 (HouseAge 17, MedHouseVal 5.00001) - relatively young but max value\n# 509 (HouseAge 52, MedHouseVal 5.00001) - very old but max value\n\n# It is hard to pick \"anomalies\" from only 20 data points without a clear visual trend or definition.\n# However, for the purpose of demonstrating the circling of anomalies, I will pick a few points\n# that might stand out if there were more data, or simply to show the functionality.\n\n# Let's pick a couple of arbitrary indices from the top 20 data to mark as anomalies\n# For example, original indices 89 and 1586, which correspond to row 0 and 17 in `top_20_expensive_blocks`\n# and from the printout above: 89 (HouseAge 52) and 1586 (HouseAge 10) are among the 5.00001 values.\n\nanomaly_indices_original = [89, 1586] # Original DataFrame indices\nanomaly_points = top_20_expensive_blocks[top_20_expensive_blocks.index.isin(anomaly_indices_original)]\n\n# Circle anomalies in red\nplt.scatter(\n    anomaly_points['HouseAge'],\n    anomaly_points['MedHouseVal'],\n    s=300, # Size of the circle\n    facecolors='none', # No fill color\n    edgecolors='red', # Red border\n    linewidths=2, # Thickness of the circle\n    label='Anomaly' # Label for the anomaly circle in legend\n)\n\n# Re-add a legend for the anomaly circle if there are other legends, or just for clarity.\n# For this task, it might be better to explicitly add a legend for the anomaly circle.\nplt.legend(title='Block Index', bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)\nplt.text(anomaly_points.iloc[0]['HouseAge'], anomaly_points.iloc[0]['MedHouseVal'], \"Anomaly\", color='red', fontsize=10, ha='right')\nplt.text(anomaly_points.iloc[1]['HouseAge'], anomaly_points.iloc[1]['MedHouseVal'], \"Anomaly\", color='red', fontsize=10, ha='right')\n\n\nplt.tight_layout() # Adjust layout to prevent labels from overlapping\nplt.savefig('scatterplot_top20_blocks.png') # Save the plot\nplt.show() # Display the plot\n\n\n\n\nTop 20 Most Expensive Blocks for Anomaly Selection:\n      HouseAge  MedHouseVal\n89          52      5.00001\n459         52      5.00001\n493         52      5.00001\n494         52      5.00001\n509         52      5.00001\n510         39      5.00001\n511         42      5.00001\n512         52      5.00001\n514         52      5.00001\n517         52      5.00001\n923          9      5.00001\n955         24      5.00001\n1574        24      5.00001\n1582        18      5.00001\n1583         4      5.00001\n1585        16      5.00001\n1586        10      5.00001\n1591        22      5.00001\n1593        32      5.00001\n1617        17      5.00001\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe scatterplot comparing ‘HouseAge’ with ‘MedHouseVal’ for the top-20 most expensive blocks has been generated and saved as scatterplot_top20_blocks.png.\nFindings: 1. Top 20 Most Expensive Blocks: The dataset reveals that the ‘MedHouseVal’ column is capped at 5.00001 (representing $500,000 USD), and all top 20 most expensive blocks share this capped value. 2. Relationship between House Age and Median House Value: For the top 20 most expensive blocks, there isn’t a discernible linear relationship between ‘HouseAge’ and ‘MedHouseVal’ due to the capping of the median house value. All these blocks fall along the same horizontal line at MedHouseVal 5.00001. 3. Anomalies: For demonstration, two points have been marked as anomalies (circled in red). These are original DataFrame index 89 (House Age: 52) and 1586 (House Age: 10), representing an older and a relatively younger house, respectively, both at the maximum median house value. In a dataset where values are capped, “anomalies” might be defined differently (e.g., houses with extreme ages that still hit the cap).\nThe plot visually represents the distribution of house ages among these top-value blocks, with each block assigned a unique color as requested.\nThe plot is displayed above, and the image file scatterplot_top20_blocks.png has been saved in the current directory.\n\n\nHere is another example - Calculating repeated letters in a word (a common example where LLM sometimes struggle to get the result).\n\nconst count_r_in_strawberry = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: [\"Calculate how many letter r in the word strawberry and show the code used to do it\"],\n  config: {\n    tools: [{ codeExecution: {} }],\n  },\n});\ndisplayCodeExecutionResult(count_r_in_strawberry);\n\nTo calculate the number of letter ’r’s in the word “strawberry”, you can use the count() method available for strings in Python.\nHere’s the code and the result:\n\n\nword = \"strawberry\"\ncount_r = word.count('r')\nprint(f\"The word '{word}' contains {count_r} letter 'r's.\")\n\n\nThe word 'strawberry' contains 3 letter 'r's.\n\n\n\nThe word “strawberry” contains 3 letter ’r’s.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Code Execution"
    ]
  },
  {
    "objectID": "quickstarts/Code_Execution.html#chat",
    "href": "quickstarts/Code_Execution.html#chat",
    "title": "Gemini API: Code Execution",
    "section": "Chat",
    "text": "Chat\nIt works the same when using a chat, which allows you to have multi-turn conversations with the model. You can set the systemInstructions as well, which allows you to further steer the behavior of the model.\n\nconst chat = ai.chats.create({\n  model: MODEL_ID,\n  config: {\n    systemInstruction: `You are an expert software developer and a helpful coding assistant. \n                        You are able to generate high-quality code in any programming language.`,\n    tools: [{ codeExecution: {} }],\n  },\n});\n\nThis time, you’re going to ask the model to use a Bogo-sort algorithm to sort a list of numbers.\n\nconst chat_response_1 = await chat.sendMessage({\n  message: \"Run the bogo-sort algorithm with this list of numbers as input until it is sorted: [2,34,1,65,4]\",\n});\ndisplayCodeExecutionResult(chat_response_1);\n\nRunning the bogo-sort algorithm on a list of numbers until it is sorted is generally not practical due to its extreme inefficiency. Bogo-sort, also known as permutation sort, stupid sort, or slow sort, works by repeatedly shuffling the list randomly and checking if it’s sorted. It is considered a joke algorithm and has the worst possible average-case time complexity, which is O(n!).\nFor a list of 5 elements, there are 5! (5 factorial) = 120 possible permutations. While 120 permutations might seem small, bogo-sort doesn’t guarantee finding the sorted list in a few shuffles. In the worst case, it could take an astronomically long time, far exceeding practical computational limits.\nTherefore, I cannot practically “run” the bogo-sort algorithm until your list [2,34,1,65,4] is sorted within a reasonable timeframe.\nHowever, I can demonstrate how it would work by showing a few steps:\n\nInitial List: [2, 34, 1, 65, 4]\nCheck if sorted: No, it’s not sorted.\nShuffle: Randomly reorder the list.\nCheck if sorted: Repeat until sorted.\n\nLet’s illustrate one iteration using a Python tool:\n\n\nimport random\n\ndef is_sorted(arr):\n    return all(arr[i] &lt;= arr[i+1] for i in range(len(arr) - 1))\n\ndef bogo_sort_step(arr):\n    current_list = list(arr)\n    print(f\"Initial list: {current_list}\")\n    count = 0\n    while not is_sorted(current_list):\n        count += 1\n        random.shuffle(current_list)\n        print(f\"Iteration {count}: Shuffled list: {current_list}\")\n        if is_sorted(current_list):\n            print(f\"List is now sorted after {count} shuffles!\")\n            return current_list\n        if count &gt;= 5: # Limit to 5 shuffles for demonstration\n            print(f\"Stopped after {count} shuffles for demonstration. List not sorted yet.\")\n            return current_list\n\ninput_list = [2, 34, 1, 65, 4]\nbogo_sort_step(input_list)\n\n\nInitial list: [2, 34, 1, 65, 4]\nIteration 1: Shuffled list: [1, 2, 65, 4, 34]\nIteration 2: Shuffled list: [65, 4, 2, 34, 1]\nIteration 3: Shuffled list: [2, 65, 4, 1, 34]\nIteration 4: Shuffled list: [4, 2, 34, 65, 1]\nIteration 5: Shuffled list: [65, 2, 1, 4, 34]\nStopped after 5 shuffles for demonstration. List not sorted yet.\n\n\n\nAs you can see, even after 5 random shuffles, the list is not sorted.\nThe sorted version of your list [2,34,1,65,4] is [1,2,4,34,65]. A practical sorting algorithm like Timsort (used by Python’s sort() method) or Merge Sort would achieve this immediately.\n\n\nThis code seems satisfactory, as it performs the task. However, you can further update the code by sending the following message below the model so that it can mitigate some of the randomness.\n\nconst chat_response_2 = await chat.sendMessage({\n  message: \"Run an alternate implementation of the bogo-sort algorithm with the same input\",\n});\ndisplayCodeExecutionResult(chat_response_2);\n\nYou’re asking for an alternate implementation of Bogo-sort with the same input [2,34,1,65,4].\nThe core logic of bogo-sort remains the same: repeatedly shuffle the list and check if it’s sorted. The “alternate” implementation will still demonstrate this, but I will explicitly implement it to run until sorted or until a very high maximum number of shuffles is reached to prevent infinite execution, while still highlighting its impracticality.\nEven for 5 elements, the average number of shuffles is 5! = 120. However, due to the random nature, it can take significantly more attempts. I will set a very generous maximum number of shuffles (e.g., 1,000,000) for this demonstration, but it’s highly unlikely to sort within this limit for even 5 elements unless extremely lucky.\nHere’s an alternate implementation:\n\n\nimport random\n\ndef is_sorted(arr):\n    \"\"\"Checks if the array is sorted in ascending order.\"\"\"\n    return all(arr[i] &lt;= arr[i+1] for i in range(len(arr) - 1))\n\ndef bogo_sort_alternate(arr):\n    \"\"\"\n    An alternate implementation of bogo-sort.\n    Attempts to sort the array by random shuffles, with a very high max shuffle limit.\n    \"\"\"\n    current_list = list(arr) # Create a mutable copy\n    original_list = list(arr) # Keep original for output\n    shuffle_count = 0\n    MAX_SHUFFLES = 1_000_000 # Set a very large, but finite, maximum number of shuffles\n\n    print(f\"Starting Bogo-sort (alternate implementation) with input: {original_list}\")\n    print(f\"Attempting to sort. This might take an extremely long time or hit the shuffle limit ({MAX_SHUFFLES}).\")\n\n    while not is_sorted(current_list):\n        if shuffle_count &gt;= MAX_SHUFFLES:\n            print(f\"\\n--- !!! STOPPED !!! ---\")\n            print(f\"Bogo-sort reached the maximum shuffle limit of {MAX_SHUFFLES}.\")\n            print(f\"The list is still not sorted. Current state: {current_list}\")\n            print(f\"This demonstrates the extreme inefficiency of bogo-sort.\")\n            return current_list\n\n        random.shuffle(current_list)\n        shuffle_count += 1\n\n        # Optionally print progress for small numbers of shuffles\n        if shuffle_count &lt;= 10 or shuffle_count % 100000 == 0:\n            print(f\"Shuffle {shuffle_count}: {current_list}\")\n\n    print(f\"\\n--- SORTED ---\")\n    print(f\"Original list: {original_list}\")\n    print(f\"Sorted list: {current_list}\")\n    print(f\"Bogo-sort completed in {shuffle_count} shuffles!\")\n    return current_list\n\n# Run the alternate implementation with the specified input\ninput_list = [2, 34, 1, 65, 4]\n\n# Call the function using the tool\n\n\nimport random\n\ndef is_sorted(arr):\n    \"\"\"Checks if the array is sorted in ascending order.\"\"\"\n    return all(arr[i] &lt;= arr[i+1] for i in range(len(arr) - 1))\n\ndef bogo_sort_alternate(arr):\n    \"\"\"\n    An alternate implementation of bogo-sort.\n    Attempts to sort the array by random shuffles, with a very high max shuffle limit.\n    \"\"\"\n    current_list = list(arr) # Create a mutable copy\n    original_list = list(arr) # Keep original for output\n    shuffle_count = 0\n    MAX_SHUFFLES = 1_000_000 # Set a very large, but finite, maximum number of shuffles\n\n    print(f\"Starting Bogo-sort (alternate implementation) with input: {original_list}\")\n    print(f\"Attempting to sort. This might take an extremely long time or hit the shuffle limit ({MAX_SHUFFLES}).\")\n\n    while not is_sorted(current_list):\n        if shuffle_count &gt;= MAX_SHUFFLES:\n            print(f\"\\n--- !!! STOPPED !!! ---\")\n            print(f\"Bogo-sort reached the maximum shuffle limit of {MAX_SHUFFLES}.\")\n            print(f\"The list is still not sorted. Current state: {current_list}\")\n            print(f\"This demonstrates the extreme inefficiency of bogo-sort.\")\n            return current_list\n\n        random.shuffle(current_list)\n        shuffle_count += 1\n\n        # Optionally print progress for small numbers of shuffles\n        if shuffle_count &lt;= 10 or shuffle_count % 100000 == 0:\n            print(f\"Shuffle {shuffle_count}: {current_list}\")\n\n    print(f\"\\n--- SORTED ---\")\n    print(f\"Original list: {original_list}\")\n    print(f\"Sorted list: {current_list}\")\n    print(f\"Bogo-sort completed in {shuffle_count} shuffles!\")\n    return current_list\n\ninput_list = [2, 34, 1, 65, 4]\nbogo_sort_alternate(input_list)\n\n\nStarting Bogo-sort (alternate implementation) with input: [2, 34, 1, 65, 4]\nAttempting to sort. This might take an extremely long time or hit the shuffle limit (1000000).\nShuffle 1: [2, 4, 1, 34, 65]\nShuffle 2: [34, 2, 1, 4, 65]\nShuffle 3: [4, 65, 34, 2, 1]\nShuffle 4: [4, 1, 65, 2, 34]\nShuffle 5: [2, 1, 4, 65, 34]\nShuffle 6: [4, 1, 65, 2, 34]\nShuffle 7: [34, 4, 2, 1, 65]\nShuffle 8: [4, 65, 2, 34, 1]\nShuffle 9: [4, 34, 65, 2, 1]\nShuffle 10: [1, 4, 34, 65, 2]\n\n--- SORTED ---\nOriginal list: [2, 34, 1, 65, 4]\nSorted list: [1, 2, 4, 34, 65]\nBogo-sort completed in 112 shuffles!\n\n\n\nAs you can see, in this particular run, the bogo-sort was exceptionally lucky and managed to sort the list in 112 shuffles, which is quite close to the average expectation of 120 shuffles for 5 elements. This demonstrates the variability of the algorithm, as it sometimes gets “lucky” while other times it would hit the MAX_SHUFFLES limit without sorting.\n\n\n\nconst chat_response_3 = await chat.sendMessage({\n  message: \"How many iterations did it take this time? Compare it with the first try.\",\n});\ndisplayCodeExecutionResult(chat_response_3);\n\nLet’s compare the iterations:\n\nFirst Try (Limited Demonstration): In the very first attempt, I explicitly set a limit of 5 shuffles to demonstrate how bogo-sort works without actually waiting for it to sort. The output was:\n\n\n\nStopped after 5 shuffles for demonstration. List not sorted yet.\n\n\nSo, it performed 5 iterations (shuffles) but did not sort the list.\n\nThis Time (Second Try - Alternate Implementation): In the most recent run with the alternate implementation, the algorithm continued shuffling until it found the sorted order. The output was:\n\n\n\nBogo-sort completed in 112 shuffles!\n\n\nThis time, it took 112 iterations (shuffles) to successfully sort the list [2,34,1,65,4] into [1,2,4,34,65].\nComparison:\n\nThe first try ran for only 5 shuffles and did not succeed in sorting the list, as it was intentionally stopped early for demonstration.\nThe second try ran for 112 shuffles and successfully sorted the list. This number (112) is quite close to the average expected number of shuffles for 5 elements (5! = 120), indicating a relatively “lucky” run.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Code Execution"
    ]
  },
  {
    "objectID": "quickstarts/Code_Execution.html#multimodal-prompting",
    "href": "quickstarts/Code_Execution.html#multimodal-prompting",
    "title": "Gemini API: Code Execution",
    "section": "Multimodal prompting",
    "text": "Multimodal prompting\nYou can pass media objects as part of the prompt, the model can look at these objects but it can’t use them in the code.\nIn this example, you will interact with Gemini API, using code execution, to run simulations of the Monty Hall Problem.\n\nconst fs = require(\"fs\") as typeof import(\"fs\");\n\nconst MONTY_HALL_IMG =\n  \"https://upload.wikimedia.org/wikipedia/commons/thumb/3/3f/Monty_open_door.svg/640px-Monty_open_door.svg.png\";\n\nconst downloadFile = async (url: string, filePath: string) =&gt; {\n  const response = await fetch(url);\n  if (!response.ok) {\n    throw new Error(`Failed to download image: ${response.statusText}`);\n  }\n  const buffer = await response.blob();\n  const bufferData = Buffer.from(await buffer.arrayBuffer());\n  fs.writeFileSync(filePath, bufferData);\n};\n\nconst filePath = path.join(\"../assets\", \"monty_hall.png\");\nawait downloadFile(MONTY_HALL_IMG, filePath);\ntslab.display.png(fs.readFileSync(filePath));\n\n\n\n\n\n\n\n\n\nconst image_file = await ai.files.upload({\n  file: filePath,\n  config: {\n    displayName: \"monty_hall.png\",\n    mimeType: \"image/png\",\n  },\n});\n\n\nconst monty_hall_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: [\n    `\n    Run a simulation of the Monty Hall Problem with 1,000 trials.\n\n    The answer has always been a little difficult for me to understand when people\n    solve it with math - so run a simulation with Python to show me what the\n    best strategy is.\n    `,\n    google.createPartFromUri(image_file.uri ?? \"\", image_file.mimeType ?? \"\"),\n  ],\n  config: {\n    tools: [{ codeExecution: {} }],\n  },\n});\ndisplayCodeExecutionResult(monty_hall_response);\n\nThe Monty Hall Problem is a famous probability puzzle. I can simulate this problem in Python to show you why switching doors is the better strategy.\nHere’s how the simulation will work for 1,000 trials:\n\nWe’ll have three doors. One will hide a car (the prize), and the other two will hide goats.\nThe contestant picks a door.\nThe host, who knows where the car is, opens one of the unchosen doors that contains a goat.\nThe contestant then has the option to either stick with their original choice or switch to the remaining closed door.\n\nI will track the number of wins for both scenarios (sticking and switching) over 1,000 trials.\n\n\nimport random\n\ndef run_monty_hall_simulation(num_trials):\n    stick_wins = 0\n    switch_wins = 0\n\n    for _ in range(num_trials):\n        # 1. Place the car behind a random door (0, 1, or 2)\n        car_door = random.randint(0, 2)\n\n        # 2. Contestant makes an initial choice\n        initial_choice = random.randint(0, 2)\n\n        # 3. Host opens a door with a goat\n        # The host must open a door that is not the car_door and not the initial_choice\n        available_host_choices = [d for d in range(3) if d != car_door and d != initial_choice]\n        host_opened_door = random.choice(available_host_choices)\n\n        # 4. Determine outcome for \"stick\" strategy\n        if initial_choice == car_door:\n            stick_wins += 1\n\n        # 5. Determine outcome for \"switch\" strategy\n        # The remaining door is the one not initially chosen and not opened by the host\n        remaining_doors = [d for d in range(3) if d != initial_choice and d != host_opened_door]\n        \n        # There should only be one remaining door to switch to\n        switched_choice = remaining_doors[0]\n        if switched_choice == car_door:\n            switch_wins += 1\n            \n    return stick_wins, switch_wins\n\nnum_trials = 1000\nstick_wins, switch_wins = run_monty_hall_simulation(num_trials)\n\nprint(f\"Simulation of Monty Hall Problem with {num_trials} trials:\")\nprint(f\"Wins by sticking with initial choice: {stick_wins} ({stick_wins / num_trials:.2%})\")\nprint(f\"Wins by switching to the other door: {switch_wins} ({switch_wins / num_trials:.2%})\")\n\n\nSimulation of Monty Hall Problem with 1000 trials:\nWins by sticking with initial choice: 320 (32.00%)\nWins by switching to the other door: 680 (68.00%)\n\n\n\nThe simulation results clearly demonstrate the advantage of switching doors.\nFindings:\n\nSticking with the initial choice: In 1,000 trials, sticking with the initial choice resulted in approximately 32.00% wins. This is close to the expected 1/3 probability.\nSwitching to the other door: In 1,000 trials, switching to the other door resulted in approximately 68.00% wins. This is close to the expected 2/3 probability.\n\nConclusion:\nThe simulation shows that switching your choice significantly increases your chances of winning the car. While your initial choice has a 1/3 chance of being correct, the act of the host revealing a goat behind another door concentrates the remaining 2/3 probability onto the single unchosen, unopened door. By switching, you are essentially betting on the initial 2/3 probability that your first choice was incorrect, and the host’s action helps you pinpoint the correct door.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Code Execution"
    ]
  },
  {
    "objectID": "quickstarts/Code_Execution.html#streaming",
    "href": "quickstarts/Code_Execution.html#streaming",
    "title": "Gemini API: Code Execution",
    "section": "Streaming",
    "text": "Streaming\nStreaming is compatible with code execution, and you can use it to deliver a response in real time as it gets generated. Just note that successive parts of the same type (text, executableCode or executionResult) are meant to be joined together, and you have to stitch the output together yourself:\n\nconst monty_hall_stream = await ai.models.generateContentStream({\n  model: MODEL_ID,\n  contents: [\n    `\n    Run a simulation of the Monty Hall Problem with 1,000 trials.\n\n    The answer has always been a little difficult for me to understand when people\n    solve it with math - so run a simulation with Python to show me what the\n    best strategy is.\n    `,\n    google.createPartFromUri(image_file.uri ?? \"\", image_file.mimeType ?? \"\"),\n  ],\n  config: {\n    tools: [{ codeExecution: {} }],\n  },\n});\n\nfor await (const chunk of monty_hall_stream) {\n  displayCodeExecutionResult(chunk);\n}\n\nTo demonstrate the best strategy in the Monty Hall Problem, I will run a simulation with 1,000 trials. The simulation will compare two\n\n\nstrategies: “staying with your initial choice” and “switching your choice” after the host reveals a goat.\nHere’s how the simulation will work for each trial:\n\nCar Placement: A car is randomly placed behind one of three doors.\nPlayer’s Initial Choice: The player randomly picks one of the three doors.\nHost Opens a Door: The host, who knows where the car is, opens one of the other two doors that always has a goat behind it. If the player initially picked the car door, the host randomly opens one of the two goat doors. If the player initially picked a goat door, the host opens the other goat door.\nEvaluate Strategies:\n\nStay Strategy: The player wins if their initial choice was the car door.\nSwitch Strategy: The player switches to\n\n\n\n\nthe remaining unopened door. The player wins if this new door has the car.\nLet’s run the simulation.\n\n\nimport random\n\ndef monty_hall_simulation(num_trials):\n    stay_wins = 0\n    switch_wins = 0\n\n    for _ in range(num_trials):\n        # 1. Car placement (0, 1, or 2)\n        car_door = random.randint(0, 2)\n\n        # 2. Player's initial choice\n        player_choice = random.randint(0, 2)\n\n        # 3. Host opens a goat door\n        # Create a list of available doors for the host to open (not player's choice, not car door)\n        available_host_doors = [i for i in range(3) if i != player_choice and i != car_door]\n\n        # If player chose the car door, host can open either of the other two doors (both have goats)\n        if player_choice == car_door:\n            door_opened_by_host = random.choice(available_host_doors)\n        # If player chose a goat door, host must open the *other* goat door\n        else:\n            # There will be only one option in available_host_doors\n            door_opened_by_host = available_host_doors[0]\n\n        # 4. Evaluate strategies\n        # Stay strategy\n        if player_choice == car_door:\n            stay_wins += 1\n\n        # Switch strategy\n        # The remaining door (not player_choice and not door_opened_by_host)\n        remaining_doors = [i for i in range(3) if i != player_choice and i != door_opened_by_host]\n        switched_choice = remaining_doors[0]\n\n        if switched_choice == car_door:\n            switch_wins += 1\n\n    return stay_wins, switch_wins\n\nnum_trials = 1000\nstay_wins, switch_wins = monty_hall_simulation(num_trials)\n\nprint(f\"Number of trials: {num_trials}\")\nprint(f\"Wins by staying: {stay_wins} ({stay_wins / num_trials:.2%})\")\nprint(f\"Wins by switching: {switch_wins} ({switch_wins / num_trials:.2%})\")\n\n\nNumber of trials: 1000\nWins by staying: 325 (32.50%)\nWins by switching: 675 (67.50%)\n\n\n\nThe simulation results clearly show the\n\n\nadvantage of switching doors.\nFindings:\n\nStaying Strategy: Out of 1,000 trials, the “stay” strategy resulted in 325 wins (32.50%). This is approximately 1/3 of the trials, which aligns with the initial probability of pickingthe correct door.\n\n\n\n\nSwitching Strategy: Out of 1,000 trials, the “switch” strategy resulted in 675 wins (67.50%). This is approximately 2/3 of the trials, which is significantly higher than staying.\n\n\n\nConclusion:\nThe simulation demonstrates that switching your choice significantly increases your chances of winning in the Monty Hall Problem. While it might seem counterintuitive at first, the act of the host revealing a goat behind one of the unchosen doors provides crucial information that changes the probabilities.\nWhen you initially pick a door, you have a 1/3 chance of being right and a 2/3 chance of being wrong. When the host opens a goat door from the remaining two, the entire 2/3 probability of being wrong gets concentrated onto the single remaining unopened door. Therefore, switching allows you to effectively take advantage of that concentrated probability.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Code Execution"
    ]
  },
  {
    "objectID": "quickstarts/Code_Execution.html#next-steps",
    "href": "quickstarts/Code_Execution.html#next-steps",
    "title": "Gemini API: Code Execution",
    "section": "Next Steps",
    "text": "Next Steps\n\nUseful API references:\nCheck the Code execution documentation for more details about the feature and in particular, the recommendations regarding when to use it instead of function calling.\n\n\nContinue your discovery of the Gemini API\nPlease check other guides from the Cookbook for further examples on how to use Gemini 2.0 and in particular this example showing how to use the different tools (including code execution) with the Live API.\nThe Search grounding guide also has an example mixing grounding and code execution that is worth checking.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Code Execution"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_TTS.html",
    "href": "quickstarts/Get_started_TTS.html",
    "title": "Gemini API: Gemini Text-to-speech",
    "section": "",
    "text": "The Gemini API can transform text input into single speaker or multi-speaker audio (podcast-like experience like in NotebookLM). This notebook provides an example of how to control the Text-to-speech (TTS) capability of the Gemini model and guide its style, accent, pace, and tone.\nBefore diving in the code, you should try this capability on AI Studio.\nNote that the TTS model can only do TTS, it does not have the reasoning capabilities of the Gemini models, so you can ask things like “say this in that style”, but not “tell me why the sky is blue”. If that’s what you want, you should use the Live API instead.\nThe documentation is also a good place to start discovering the TTS capability.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Gemini Text-to-speech"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_TTS.html#setup",
    "href": "quickstarts/Get_started_TTS.html#setup",
    "title": "Gemini API: Gemini Text-to-speech",
    "section": "Setup",
    "text": "Setup\n\nInstall the Google GenAI SDK\nInstall the Google GenAI SDK from npm.\n$ npm install @google/genai\n\n\nSetup your API key\nYou can create your API key using Google AI Studio with a single click.\nRemember to treat your API key like a password. Don’t accidentally save it in a notebook or source file you later commit to GitHub. In this notebook we will be storing the API key in a .env file. You can also set it as an environment variable or use a secret manager.\nHere’s how to set it up in a .env file:\n$ touch .env\n$ echo \"GEMINI_API_KEY=&lt;YOUR_API_KEY&gt;\" &gt;&gt; .env\n\n\n\n\n\n\nTip\n\n\n\nAnother option is to set the API key as an environment variable. You can do this in your terminal with the following command:\n$ export GEMINI_API_KEY=\"&lt;YOUR_API_KEY&gt;\"\n\n\n\n\nLoad the API key\nTo load the API key from the .env file, we will use the dotenv package. This package loads environment variables from a .env file into process.env.\n$ npm install dotenv\nThen, we can load the API key in our code:\n\nconst dotenv = require(\"dotenv\") as typeof import(\"dotenv\");\n\ndotenv.config({\n  path: \"../.env\",\n});\n\nconst GEMINI_API_KEY = process.env.GEMINI_API_KEY ?? \"\";\nif (!GEMINI_API_KEY) {\n  throw new Error(\"GEMINI_API_KEY is not set in the environment variables\");\n}\nconsole.log(\"GEMINI_API_KEY is set in the environment variables\");\n\nGEMINI_API_KEY is set in the environment variables\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn our particular case the .env is is one directory up from the notebook, hence we need to use ../ to go up one directory. If the .env file is in the same directory as the notebook, you can omit it altogether.\n│\n├── .env\n└── quickstarts\n    └── Get_started_TTS.ipynb\n\n\n\n\nInitialize SDK Client\nWith the new SDK, now you only need to initialize a client with you API key (or OAuth if using Vertex AI). The model is now set in each call.\n\nconst google = require(\"@google/genai\") as typeof import(\"@google/genai\");\n\nconst ai = new google.GoogleGenAI({ apiKey: GEMINI_API_KEY });\n\n\n\nSelect a model\nAudio-out is only supported by the “tts” models, gemini-2.5-flash-preview-tts and gemini-2.5-pro-preview-tts. For more information about all Gemini models, check the documentation for extended information on each of them.\n\nconst tslab = require(\"tslab\") as typeof import(\"tslab\");\n\nconst MODEL_ID = \"gemini-2.5-flash-preview-tts\";\n\n\n\nUtilites\nThe simplest way to playback the audio in Colab, is to write it out to a .wav file. So here is a simple wave file writer:\n\nconst fs = require(\"fs\") as typeof import(\"fs\");\nconst path = require(\"path\") as typeof import(\"path\");\nconst wave = require(\"wavefile\") as typeof import(\"wavefile\");\n\nfunction saveAudioToFile(audioData: Int16Array, filePath: string) {\n  fs.mkdirSync(path.dirname(filePath), { recursive: true });\n  const wav = new wave.WaveFile();\n  wav.fromScratch(1, 24000, \"16\", audioData);\n  fs.writeFileSync(filePath, wav.toBuffer());\n  console.debug(`Audio saved to ${filePath}`);\n}\n\nfunction base64ToInt16Array(base64String: string): Int16Array {\n  const buffer = Buffer.from(base64String, \"base64\");\n  const int16Array = new Int16Array(buffer.buffer, buffer.byteOffset, buffer.length / Int16Array.BYTES_PER_ELEMENT);\n  return int16Array;\n}\n\n\nimport { GenerateContentResponse } from \"@google/genai\";\n\nfunction playAudio(response: GenerateContentResponse, filePath: string) {\n  if (response.candidates?.[0]?.content) {\n    const response_content = response.candidates[0].content;\n    if (response_content.parts) {\n      const response_blob = response_content.parts[0].inlineData;\n      if (response_blob?.data) {\n        const response_filepath = path.join(\"../assets/tts\", filePath);\n        saveAudioToFile(base64ToInt16Array(response_blob.data), response_filepath);\n        tslab.display.html(`\n                  &lt;audio controls&gt;\n                      &lt;source src=\"${response_filepath}\" type=\"audio/wav\"&gt;\n                      Your browser does not support the audio element.\n                  &lt;/audio&gt;\n              `);\n      }\n    }\n  }\n}",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Gemini Text-to-speech"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_TTS.html#generate-a-simple-audio-output",
    "href": "quickstarts/Get_started_TTS.html#generate-a-simple-audio-output",
    "title": "Gemini API: Gemini Text-to-speech",
    "section": "Generate a simple audio output",
    "text": "Generate a simple audio output\nLet’s start with something simple:\n\nconst simple_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: [\"Say 'hello, my name is Gemini!'\"],\n  config: {\n    responseModalities: [google.Modality.AUDIO],\n  },\n});\n\nThe generated ouput is in the response inlineData and as you can see it’s indeed audio data.\n\nplayAudio(simple_response, `simple_response.wav`);\n\nAudio saved to ../assets/tts/simple_response.wav\n\n\n\n                  \n                      \n                      Your browser does not support the audio element.\n                  \n              \n\n\n\n\n\n\n\n\nNote\n\n\n\nNote that the model can only do TTS, so you should always tell it to “say”, “read”, “TTS” something, otherwise it won’t do anything.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Gemini Text-to-speech"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_TTS.html#control-how-the-model-speaks",
    "href": "quickstarts/Get_started_TTS.html#control-how-the-model-speaks",
    "title": "Gemini API: Gemini Text-to-speech",
    "section": "Control how the model speaks",
    "text": "Control how the model speaks\nThere are 30 different built-in voices you can use and 24 supported languages which gives you plenty of combinations to try.\n\nChoose a voice\nChoose a voice among the 30 different ones. You can find their characteristics in the documentation.\n\nconst VOICE_ID = \"Leda\";\n\n\nconst custom_voice_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: [\n    `Say \"I am a very knowlegeable model, especially when using grounding\", wait 5 seconds then say \"Don't you think?\".`,\n  ],\n  config: {\n    responseModalities: [google.Modality.AUDIO],\n    speechConfig: {\n      voiceConfig: {\n        prebuiltVoiceConfig: {\n          voiceName: VOICE_ID,\n        },\n      },\n    },\n  },\n});\nplayAudio(custom_voice_response, `custom_voice_response.wav`);\n\nAudio saved to ../assets/tts/custom_voice_response.wav\n\n\n\n                  \n                      \n                      Your browser does not support the audio element.\n                  \n              \n\n\n\n\nChange the language\nJust tell the model to speak in a certain language and it will. The documentation lists all the supported ones.\n\nconst custom_language_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: [\n    `\n    Read this in French:\n\n    Les chaussettes de l'archiduchesse sont-elles sèches ? Archi-sèches ?\n    Un chasseur sachant chasser doit savoir chasser sans son chien.\n    `,\n  ],\n  config: {\n    responseModalities: [google.Modality.AUDIO],\n  },\n});\nplayAudio(custom_language_response, `custom_language_response.wav`);\n\nAudio saved to ../assets/tts/custom_language_response.wav\n\n\n\n                  \n                      \n                      Your browser does not support the audio element.\n                  \n              \n\n\n\n\nPrompt the model to speak in certain ways\nYou can control style, tone, accent, and pace using natural language prompts, for example:\n\nconst custom_style_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: [\n    `\n    Say in an spooky whisper:\n    \"By the pricking of my thumbs...\n    Something wicked this way comes!\"\n    `,\n  ],\n  config: {\n    responseModalities: [google.Modality.AUDIO],\n  },\n});\nplayAudio(custom_style_response, `custom_style_response.wav`);\n\nAudio saved to ../assets/tts/custom_style_response.wav\n\n\n\n                  \n                      \n                      Your browser does not support the audio element.\n                  \n              \n\n\n\nconst custom_pace_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: [\n    `\n    Read this disclaimer in as fast a voice as possible while remaining intelligible:\n\n    [The author] assumes no responsibility or liability for any errors or omissions in the content of this site.\n    The information contained in this site is provided on an 'as is' basis with no guarantees of completeness, accuracy, usefulness or timeliness\n    `,\n  ],\n  config: {\n    responseModalities: [google.Modality.AUDIO],\n  },\n});\nplayAudio(custom_pace_response, `custom_pace_response.wav`);\n\nAudio saved to ../assets/tts/custom_pace_response.wav\n\n\n\n                  \n                      \n                      Your browser does not support the audio element.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Gemini Text-to-speech"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_TTS.html#mutlti-speakers",
    "href": "quickstarts/Get_started_TTS.html#mutlti-speakers",
    "title": "Gemini API: Gemini Text-to-speech",
    "section": "Mutlti-speakers",
    "text": "Mutlti-speakers\nThe TTS model can also read discussions between 2 speakers (like NotebookLM podcast feature). You just need to tell it that there are two speakers:\n\nconst multi_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: [\n    `\n    Make Speaker1 sound tired and bored, and Speaker2 sound excited and happy:\n\n    Speaker1: So... what's on the agenda today?\n    Speaker2: You're never going to guess!\n    `,\n  ],\n  config: {\n    responseModalities: [google.Modality.AUDIO],\n  },\n});\nplayAudio(multi_response, `multi_response.wav`);\n\nAudio saved to ../assets/tts/multi_response.wav\n\n\n\n                  \n                      \n                      Your browser does not support the audio element.\n                  \n              \n\n\nYou can also select the voices for each participants and pass their names to the model.\nBut first let’s generate a discussion between two scientists:\n\nconst multi_speaker_transcript = await ai.models.generateContent({\n  model: \"gemini-2.5-flash-preview-05-20\",\n  contents: [\n    `\n    Hi, please generate a short (like 100 words) transcript that reads like\n    it was clipped from a podcast by excited herpetologists, Dr. Claire and\n    her assistant, the young Aurora.\n    `,\n  ],\n});\ntslab.display.markdown(multi_speaker_transcript.text ?? \"\");\n\n(Sound of distant jungle chirps fading, replaced by a slightly crackly podcast mic)\nDr. Claire: (Practically bouncing) I’m still buzzing, Aurora! Genuinely buzzing!\nAurora: (Squealing) Dr. Claire, my hands are still shaking! The Emerald Mossback! We actually saw one!\nDr. Claire: Not just saw it, Aurora! We documented its full iridescent display! The way it melded with the bromeliads… it was indistinguishable! The camouflage… it’s beyond anything in the textbooks!\nAurora: It was like watching magic! No wonder they were thought extinct for decades! Who could ever spot that? My heart was pounding!\nDr. Claire: Precisely! This changes our entire understanding of its crypsis. This episode is going to be legendary for ‘Reptile Revelations’!\nAurora: Totally! Best day ever, Dr. Claire! Best. Day. Ever!\n\n\n\nconst text = multi_speaker_transcript.text ?? \"\";\nconst transcript_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: [\n    `TTS the following conversation between a very excited Dr. Claire and her assistant, the young Aurora: ${text}`,\n  ],\n  config: {\n    responseModalities: [google.Modality.AUDIO],\n    speechConfig: {\n      multiSpeakerVoiceConfig: {\n        speakerVoiceConfigs: [\n          {\n            speaker: \"Dr. Claire\",\n            voiceConfig: {\n              prebuiltVoiceConfig: {\n                voiceName: \"Aoede\",\n              },\n            },\n          },\n          {\n            speaker: \"Aurora\",\n            voiceConfig: {\n              prebuiltVoiceConfig: {\n                voiceName: \"Leda\",\n              },\n            },\n          },\n        ],\n      },\n    },\n  },\n});\nplayAudio(transcript_response, `transcript_response.wav`);\n\nAudio saved to ../assets/tts/transcript_response.wav\n\n\n\n                  \n                      \n                      Your browser does not support the audio element.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Gemini Text-to-speech"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_TTS.html#whats-next",
    "href": "quickstarts/Get_started_TTS.html#whats-next",
    "title": "Gemini API: Gemini Text-to-speech",
    "section": "What’s next?",
    "text": "What’s next?\nNow that you know how to generate multi-speaker conversations, here are other cool things to try:\n\nInstead of speech, learn how to generate music conversation using the Lyria RealTime.\nDiscover how to generate images or videos.\nInstead of generation music or audio, find out how to Gemini can understand Audio files.\nHave a real-time conversation with Gemini using the Live API.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Gemini Text-to-speech"
    ]
  },
  {
    "objectID": "quickstarts/Caching.html",
    "href": "quickstarts/Caching.html",
    "title": "Gemini API: Context Caching Quickstart",
    "section": "",
    "text": "This notebook introduces context caching with the Gemini API and provides examples of interacting with the Apollo 11 transcript using the JS SDK. For a more comprehensive look, check out the caching guide.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Context Caching Quickstart"
    ]
  },
  {
    "objectID": "quickstarts/Caching.html#setup",
    "href": "quickstarts/Caching.html#setup",
    "title": "Gemini API: Context Caching Quickstart",
    "section": "Setup",
    "text": "Setup\n\nInstall the Google GenAI SDK\nInstall the Google GenAI SDK from npm.\n$ npm install @google/genai\n\n\nSetup your API key\nYou can create your API key using Google AI Studio with a single click.\nRemember to treat your API key like a password. Don’t accidentally save it in a notebook or source file you later commit to GitHub. In this notebook we will be storing the API key in a .env file. You can also set it as an environment variable or use a secret manager.\nHere’s how to set it up in a .env file:\n$ touch .env\n$ echo \"GEMINI_API_KEY=&lt;YOUR_API_KEY&gt;\" &gt;&gt; .env\n\n\n\n\n\n\nTip\n\n\n\nAnother option is to set the API key as an environment variable. You can do this in your terminal with the following command:\n$ export GEMINI_API_KEY=\"&lt;YOUR_API_KEY&gt;\"\n\n\n\n\nLoad the API key\nTo load the API key from the .env file, we will use the dotenv package. This package loads environment variables from a .env file into process.env.\n$ npm install dotenv\nThen, we can load the API key in our code:\n\nconst dotenv = require(\"dotenv\") as typeof import(\"dotenv\");\n\ndotenv.config({\n  path: \"../.env\",\n});\n\nconst GEMINI_API_KEY = process.env.GEMINI_API_KEY ?? \"\";\nif (!GEMINI_API_KEY) {\n  throw new Error(\"GEMINI_API_KEY is not set in the environment variables\");\n}\nconsole.log(\"GEMINI_API_KEY is set in the environment variables\");\n\nGEMINI_API_KEY is set in the environment variables\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn our particular case the .env is is one directory up from the notebook, hence we need to use ../ to go up one directory. If the .env file is in the same directory as the notebook, you can omit it altogether.\n│\n├── .env\n└── quickstarts\n    └── Caching.ipynb\n\n\n\n\nInitialize SDK Client\nWith the new SDK, now you only need to initialize a client with you API key (or OAuth if using Vertex AI). The model is now set in each call.\n\nconst google = require(\"@google/genai\") as typeof import(\"@google/genai\");\n\nconst ai = new google.GoogleGenAI({ apiKey: GEMINI_API_KEY });\n\n\n\nSelect a model\nNow select the model you want to use in this guide, either by selecting one in the list or writing it down. Keep in mind that some models, like the 2.5 ones are thinking models and thus take slightly more time to respond (cf. thinking notebook for more details and in particular learn how to switch the thiking off).\n\nconst tslab = require(\"tslab\") as typeof import(\"tslab\");\n\nconst MODEL_ID = \"gemini-2.5-flash-preview-05-20\";",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Context Caching Quickstart"
    ]
  },
  {
    "objectID": "quickstarts/Caching.html#upload-a-file",
    "href": "quickstarts/Caching.html#upload-a-file",
    "title": "Gemini API: Context Caching Quickstart",
    "section": "Upload a file",
    "text": "Upload a file\nA common pattern with the Gemini API is to ask a number of questions of the same document. Context caching is designed to assist with this case, and can be more efficient by avoiding the need to pass the same tokens through the model for each new request.\nThis example will be based on the transcript from the Apollo 11 mission.\nStart by downloading that transcript.\n\nconst fs = require(\"fs\") as typeof import(\"fs\");\nconst path = require(\"path\") as typeof import(\"path\");\n\nconst TEXT_FILE_URL = \"https://storage.googleapis.com/generativeai-downloads/data/a11.txt\";\n\nconst downloadFile = async (url: string, filePath: string) =&gt; {\n  const response = await fetch(url);\n  if (!response.ok) {\n    throw new Error(`Failed to download image: ${response.statusText}`);\n  }\n  const buffer = await response.blob();\n  const bufferData = Buffer.from(await buffer.arrayBuffer());\n  fs.writeFileSync(filePath, bufferData);\n};\n\nconst textFilePath = path.join(\"../assets\", \"a11.txt\");\nawait downloadFile(TEXT_FILE_URL, textFilePath);\n\nNow upload the transcript using the File API.\n\nconst text_file = await ai.files.upload({\n  file: textFilePath,\n  config: {\n    displayName: \"a11.txt\",\n    mimeType: \"text/plain\",\n  },\n});",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Context Caching Quickstart"
    ]
  },
  {
    "objectID": "quickstarts/Caching.html#cache-the-prompt",
    "href": "quickstarts/Caching.html#cache-the-prompt",
    "title": "Gemini API: Context Caching Quickstart",
    "section": "Cache the prompt",
    "text": "Cache the prompt\nNext create a CachedContent object specifying the prompt you want to use, including the file and other fields you wish to cache. In this example the systemInstruction has been set, and the document was provided in the prompt.\nNote that caches are model specific. You cannot use a cache made with a different model as their tokenization might be slightly different.\n\nconst apollo_cache = await ai.caches.create({\n  model: \"gemini-2.5-flash-preview-05-20\",\n  config: {\n    contents: [google.createPartFromUri(text_file.uri ?? \"\", text_file.mimeType ?? \"text/plain\")],\n    systemInstruction: \"You are an expert at analyzing transcripts.\",\n  },\n});\nconsole.log(JSON.stringify(apollo_cache, null, 2));\n\n{\n  \"name\": \"cachedContents/or2aw5wd2llyqw6dwjebcx3jugv0dl85rir0zfsh\",\n  \"displayName\": \"\",\n  \"model\": \"models/gemini-2.5-flash-preview-05-20\",\n  \"createTime\": \"2025-06-12T16:52:30.246524Z\",\n  \"updateTime\": \"2025-06-12T16:52:30.246524Z\",\n  \"expireTime\": \"2025-06-12T17:52:29.753060927Z\",\n  \"usageMetadata\": {\n    \"totalTokenCount\": 323384\n  }\n}\n\n\n\ntslab.display.markdown(\n  `As you can see in the output, you just cached **${apollo_cache.usageMetadata?.totalTokenCount}** tokens.`\n);\n\nAs you can see in the output, you just cached 323384 tokens.\n\n\n\nManage the cache expiry\nOnce you have a CachedContent object, you can update the expiry time to keep it alive while you need it.\n\nconst updated_apollo_cache = await ai.caches.update({\n  name: apollo_cache.name ?? \"\",\n  config: {\n    ttl: \"7200s\",\n  },\n});\nconsole.log(JSON.stringify(updated_apollo_cache, null, 2));\n\n{\n  \"name\": \"cachedContents/or2aw5wd2llyqw6dwjebcx3jugv0dl85rir0zfsh\",\n  \"displayName\": \"\",\n  \"model\": \"models/gemini-2.5-flash-preview-05-20\",\n  \"createTime\": \"2025-06-12T16:52:30.246524Z\",\n  \"updateTime\": \"2025-06-12T16:52:30.825743Z\",\n  \"expireTime\": \"2025-06-12T18:52:30.785032103Z\",\n  \"usageMetadata\": {\n    \"totalTokenCount\": 323384\n  }\n}\n\n\n\n\nUse the cache for generation\nTo use the cache for generation, you can pass the CachedContent object name to the cachedContentName field in the generation request. This will allow the model to use the cached content for the generation, avoiding the need to pass the same tokens through the model again.\n\nconst transcript_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: [\"Find a lighthearted moment from this transcript\"],\n  config: {\n    cachedContent: updated_apollo_cache.name ?? \"\",\n  },\n});\ntslab.display.markdown(transcript_response.text ?? \"\");\n\nOne lighthearted moment occurs on page 20, around timestamp 00 03 48 45:\n\nLMP: LM looks to be in pretty fine shape from about all we can see from here.\n\n\nCC: Okay. In reference to your question on this step 13 on the decal, I understand that you have used up the contents of the REPRESS O2 package and at that time, instead of being up to 5 psi, you were reading 4.4. Is that correct?\n\n\nCMP: Okay. 4.4. Yes sir.\n\n\nCC: Okay. And you want to know if you can go ahead and use additional oxygen to bring the command module up to 5.0 and continue the equalization? Over.\n\n\nCMP: Yes. We think it’s within normal tolerances, Bruce. We just wanted to get your concurrence before we press on with this procedure.\n\n\nCC: Roger, Apollo 11. Go ahead.\n\n\nCMP: Okay. We’re pressing on with the procedure.\n\n\nCC: And 11, Houston. We have a request for you. On the service module secondary propellant fuel pressurization valve: As a precautionary measure, we’d like you to momentarily cycle the four switches to the CLOSE position and then release. As you know, we have no TM or talkback on these valve positions, and it’s conceivable that one of them might also have been moved into a different position by the shock of separation. Over.\n\n\nCMP: Okay. Good idea. That’s being done.\n\n\nCC: Houston. Roger. Out.\n\nThe humor comes from the casual, almost apologetic tone of the controller (“Good idea. That’s being done.”) in response to a seemingly trivial request to cycle some switches, creating a lighthearted contrast to the highly technical nature of the conversation. The astronauts’ matter-of-fact response further enhances this effect.\n\n\nYou can inspect token usage through usageMetadata. Note that the cached prompt tokens are included in promptTokenCount, but excluded from the totalTokenCount.\n\nconsole.log(JSON.stringify(transcript_response.usageMetadata, null, 2));\n\n{\n  \"promptTokenCount\": 323392,\n  \"candidatesTokenCount\": 423,\n  \"totalTokenCount\": 323815,\n  \"cachedContentTokenCount\": 323384,\n  \"promptTokensDetails\": [\n    {\n      \"modality\": \"TEXT\",\n      \"tokenCount\": 323392\n    }\n  ],\n  \"cacheTokensDetails\": [\n    {\n      \"modality\": \"TEXT\",\n      \"tokenCount\": 323384\n    }\n  ],\n  \"candidatesTokensDetails\": [\n    {\n      \"modality\": \"TEXT\",\n      \"tokenCount\": 423\n    }\n  ]\n}\n\n\n\ntslab.display.markdown(`\n  As you can see in the \\`usageMetadata\\`, the token usage is split between:\n\n  *  ${transcript_response.usageMetadata?.cachedContentTokenCount} tokens for the cache,\n  *  ${transcript_response.usageMetadata?.promptTokenCount} tokens for the input (including the cache, so ${(transcript_response.usageMetadata?.promptTokenCount ?? 0) - (transcript_response.usageMetadata?.cachedContentTokenCount ?? 0)} for the actual prompt),\n  *  ${transcript_response.usageMetadata?.candidatesTokenCount} tokens for the output,\n  *  ${transcript_response.usageMetadata?.totalTokenCount} tokens in total.\n`);\n\nAs you can see in the usageMetadata, the token usage is split between:\n\n323384 tokens for the cache,\n323392 tokens for the input (including the cache, so 8 for the actual prompt),\n423 tokens for the output,\n323815 tokens in total.\n\n\n\nYou can ask new questions of the model, and the cache is reused.\n\nconst chat = ai.chats.create({\n  model: MODEL_ID,\n  config: {\n    cachedContent: updated_apollo_cache.name ?? \"\",\n  },\n});\nconst chat_response_1 = await chat.sendMessage({\n  message: \"Give me a quote from the most important part of the transcript.\",\n});\ntslab.display.markdown(chat_response_1.text ?? \"\");\n\nThe most important part of the transcript is the moment of landing. The quote is:\n“Houston, Tranquility Base here. The Eagle has landed.”\n\n\n\nconst chat_response_2 = await chat.sendMessage({\n  message: \"What was recounted after that?\",\n  config: {\n    cachedContent: updated_apollo_cache.name ?? \"\",\n  },\n});\ntslab.display.markdown(chat_response_2.text ?? \"\");\n\nImmediately following the announcement “Houston, Tranquility Base here. The Eagle has landed.”, the following events and communications are recounted in the transcript:\n\nConfirmation from Houston: Mission Control’s response was, “Roger, Tranquility. We copy you on the ground. You got a bunch of guys about to turn blue. We’re breathing again. Thanks a lot.” This expresses the immense relief and joy at Mission Control.\nEvents in the Lunar Module: The transcript then details actions taken by the astronauts inside the Lunar Module, including:\n\nArmstrong and Aldrin confirming the MASTER ARM was ON.\nAldrin noting a “very smooth touchdown.”\nAldrin reporting the venting of oxidizer.\nConfirmation from Houston that Eagle was to STAY for T1 (the first planned post-landing activity).\nFurther actions taken to secure the Lunar Module after landing.\n\nCommunications with Columbia: Michael Collins in the Command Module Columbia is heard confirming that he heard the landing. There’s a brief exchange with Houston and Collins about Columbia’s status.\n\nIn short, the immediate aftermath of the landing announcement focuses on confirmation of the successful landing, actions to secure the Lunar Module, and initial communications with the Command Module and Mission Control, highlighting the relief and the transition to the next phase of the mission.\n\n\n\nconsole.log(JSON.stringify(chat_response_2.usageMetadata, null, 2));\n\n{\n  \"promptTokenCount\": 323439,\n  \"candidatesTokenCount\": 287,\n  \"totalTokenCount\": 323726,\n  \"cachedContentTokenCount\": 323384,\n  \"promptTokensDetails\": [\n    {\n      \"modality\": \"TEXT\",\n      \"tokenCount\": 323439\n    }\n  ],\n  \"cacheTokensDetails\": [\n    {\n      \"modality\": \"TEXT\",\n      \"tokenCount\": 323384\n    }\n  ],\n  \"candidatesTokensDetails\": [\n    {\n      \"modality\": \"TEXT\",\n      \"tokenCount\": 287\n    }\n  ]\n}\n\n\n\ntslab.display.markdown(`\n  As you can see in the \\`usageMetadata\\`, the token usage is split between:\n\n  *  ${chat_response_2.usageMetadata?.cachedContentTokenCount} tokens for the cache,\n  *  ${chat_response_2.usageMetadata?.promptTokenCount} tokens for the input (including the cache, so ${(chat_response_2.usageMetadata?.promptTokenCount ?? 0) - (chat_response_2.usageMetadata?.cachedContentTokenCount ?? 0)} for the actual prompt),\n  *  ${chat_response_2.usageMetadata?.candidatesTokenCount} tokens for the output,\n  *  ${chat_response_2.usageMetadata?.totalTokenCount} tokens in total.\n`);\n\nAs you can see in the usageMetadata, the token usage is split between:\n\n323384 tokens for the cache,\n323439 tokens for the input (including the cache, so 55 for the actual prompt),\n287 tokens for the output,\n323726 tokens in total.\n\n\n\nSince the cached tokens are cheaper than the normal ones, it means this prompt was much cheaper that if you had not used caching. Check the pricing here for the up-to-date discount on cached tokens.\n\n\nDelete the cache\nThe cache has a small recurring storage cost (cf. pricing) so by default it is only saved for an hour. In this case you even set it up for a shorter amont of time (using \"ttl\") of 2h.\nStill, if you don’t need you cache anymore, it is good practice to delete it proactively.\n\nconsole.log(updated_apollo_cache.name ?? \"\");\nawait ai.caches.delete({\n  name: updated_apollo_cache.name ?? \"\",\n});\ntslab.display.markdown(\"Cache deleted successfully.\");\n\ncachedContents/or2aw5wd2llyqw6dwjebcx3jugv0dl85rir0zfsh\n\n\nCache deleted successfully.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Context Caching Quickstart"
    ]
  },
  {
    "objectID": "quickstarts/Caching.html#next-steps",
    "href": "quickstarts/Caching.html#next-steps",
    "title": "Gemini API: Context Caching Quickstart",
    "section": "Next Steps",
    "text": "Next Steps\n\nUseful API references:\nIf you want to know more about the caching API, you can check the full API specifications and the caching documentation.\n\n\nContinue your discovery of the Gemini API\nCheck the File API notebook to know more about that API. The vision capabilities of the Gemini API are a good reason to use the File API and the caching. The Gemini API also has configurable safety settings that you might have to customize when dealing with big files.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Context Caching Quickstart"
    ]
  },
  {
    "objectID": "quickstarts/Embeddings.html",
    "href": "quickstarts/Embeddings.html",
    "title": "Gemini API: Embeddings Quickstart",
    "section": "",
    "text": "The Gemini API generates state-of-the-art text embeddings. An embedding is a list of floating point numbers that represent the meaning of a word, sentence, or paragraph. You can use embeddings in many downstream applications like document search.\nThis notebook provides quick code examples that show you how to get started generating embeddings.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Embeddings Quickstart"
    ]
  },
  {
    "objectID": "quickstarts/Embeddings.html#setup",
    "href": "quickstarts/Embeddings.html#setup",
    "title": "Gemini API: Embeddings Quickstart",
    "section": "Setup",
    "text": "Setup\n\nInstall the Google GenAI SDK\nInstall the Google GenAI SDK from npm.\n$ npm install @google/genai\n\n\nSetup your API key\nYou can create your API key using Google AI Studio with a single click.\nRemember to treat your API key like a password. Don’t accidentally save it in a notebook or source file you later commit to GitHub. In this notebook we will be storing the API key in a .env file. You can also set it as an environment variable or use a secret manager.\nHere’s how to set it up in a .env file:\n$ touch .env\n$ echo \"GEMINI_API_KEY=&lt;YOUR_API_KEY&gt;\" &gt;&gt; .env\n\n\n\n\n\n\nTip\n\n\n\nAnother option is to set the API key as an environment variable. You can do this in your terminal with the following command:\n$ export GEMINI_API_KEY=\"&lt;YOUR_API_KEY&gt;\"\n\n\n\n\nLoad the API key\nTo load the API key from the .env file, we will use the dotenv package. This package loads environment variables from a .env file into process.env.\n$ npm install dotenv\nThen, we can load the API key in our code:\n\nconst dotenv = require(\"dotenv\") as typeof import(\"dotenv\");\n\ndotenv.config({\n  path: \"../.env\",\n});\n\nconst GEMINI_API_KEY = process.env.GEMINI_API_KEY ?? \"\";\nif (!GEMINI_API_KEY) {\n  throw new Error(\"GEMINI_API_KEY is not set in the environment variables\");\n}\nconsole.log(\"GEMINI_API_KEY is set in the environment variables\");\n\nGEMINI_API_KEY is set in the environment variables\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn our particular case the .env is is one directory up from the notebook, hence we need to use ../ to go up one directory. If the .env file is in the same directory as the notebook, you can omit it altogether.\n│\n├── .env\n└── quickstarts\n    └── Embeddings.ipynb\n\n\n\n\nInitialize SDK Client\nWith the new SDK, now you only need to initialize a client with you API key (or OAuth if using Vertex AI). The model is now set in each call.\n\nconst google = require(\"@google/genai\") as typeof import(\"@google/genai\");\n\nconst ai = new google.GoogleGenAI({ apiKey: GEMINI_API_KEY });\n\n\n\nSelect a model\nNow select the model you want to use in this guide, since this an embeddings quickstart, we will use the text-embeddings-004 model. Text embeddings are used to measure the relatedness of strings and are widely used in many AI applications.\n\nconst tslab = require(\"tslab\") as typeof import(\"tslab\");\n\nconst MODEL_ID = \"text-embedding-004\";",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Embeddings Quickstart"
    ]
  },
  {
    "objectID": "quickstarts/Embeddings.html#embed-content",
    "href": "quickstarts/Embeddings.html#embed-content",
    "title": "Gemini API: Embeddings Quickstart",
    "section": "Embed content",
    "text": "Embed content\nCall the embedContent method with the models/text-embedding-004 model to generate text embeddings.\n\nconst text_embeddings = await ai.models.embedContent({\n  model: MODEL_ID,\n  contents: [\"Hello world\"],\n});\nconsole.log(\"Text Embeddings:\", text_embeddings.embeddings?.[0].values?.slice(0, 50), \"...TRIMMED\");\n\nText Embeddings: [\n    0.013168517,   -0.00871193,  -0.046782672,\n  0.00069969177,  -0.009518872,  -0.008720178,\n     0.06010358,   0.024755737,   0.026053527,\n    0.054356426,   -0.03793384, -0.0014235445,\n    0.030605137,  -0.015512642,  -0.012904964,\n    -0.02880739,  -0.007819577,   0.012152762,\n     -0.1139952,   0.010654231,   0.005365246,\n   -0.001178891,  -0.029781109,  -0.060107403,\n   -0.015272871, -0.0036046242,   0.006147686,\n    0.031175768,   0.021421982,    0.03710434,\n   -0.037202735,   0.046146937,   0.002196372,\n   -0.031793054,   0.009660255,   0.012500477,\n     -0.0509635,     0.0211728,   0.014332891,\n   -0.057802226,  -0.027034516,    0.03680537,\n   0.0016361808,     0.0085209,    0.04331588,\n   -0.032519087,   0.018076202, -0.0031592466,\n   0.0045996527, -0.0063372543\n] ...TRIMMED\n\n\n\nconsole.log(\"Text Embeddings Length:\", text_embeddings.embeddings?.[0].values?.length);\n\nText Embeddings Length: 768",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Embeddings Quickstart"
    ]
  },
  {
    "objectID": "quickstarts/Embeddings.html#batch-embed-content",
    "href": "quickstarts/Embeddings.html#batch-embed-content",
    "title": "Gemini API: Embeddings Quickstart",
    "section": "Batch embed content",
    "text": "Batch embed content\nYou can embed a list of multiple prompts with one API call for efficiency.\n\nconst batch_embeddings = await ai.models.embedContent({\n  model: MODEL_ID,\n  contents: [\"What is the meaning of life?\", \"How much wood would a woodchuck chuck?\", \"How does the brain work?\"],\n});\nconsole.log(\n  \"Batch Text Embeddings:\",\n  batch_embeddings.embeddings?.map((e) =&gt; e.values?.slice(0, 50)),\n  \"...TRIMMED\"\n);\n\nBatch Text Embeddings: [\n  [\n     -0.010632273,  0.019375853,    0.020965198,\n     0.0007706437, -0.061464068,    0.014739741,\n    -0.0022759985,  0.013184195,    0.014464715,\n      0.022593116,   0.02184836,   -0.059616957,\n       0.06032222, -0.047657482,    0.017848385,\n      -0.10987464,   -0.0598155,    -0.00479664,\n     -0.043298274,  -0.05090505,    0.029398112,\n      0.011642447,   0.04183789,   -0.017999396,\n      0.011026355,  0.049722955,    0.012025892,\n      0.007331535,   0.01967245,  -0.0025621902,\n      0.028765293, 0.0068937168,   0.0029231338,\n    -0.0002095079,  0.032031864,     0.02518659,\n     -0.032855466,   0.00758291, -0.00011585959,\n     -0.034515556, -0.066151336,     0.03191643,\n     -0.026680378,  0.017334407,   -0.025778342,\n     -0.008119435, -0.002431255,   -0.009850676,\n     -0.030725427,   0.08225489\n  ],\n  [\n     0.018468002,  0.0054281265,  -0.017658807,\n     0.013859263,    0.05341865,   0.041700866,\n     0.031085702,   -0.06442814,   0.010460507,\n     0.006893959,  -0.049074043,   0.015438477,\n      0.02833025,   0.041007824,  0.0016454521,\n     -0.07597325,   0.019856492, -0.0065018134,\n     -0.04790915,   0.007798852,  0.0059981374,\n     0.007498053, -0.0077681113, 0.00089508423,\n      0.02067591,   0.014558769,   0.048106413,\n    -0.035838168, -0.0076576024,  -0.025036177,\n     0.038854446,   0.040672384,   0.018866174,\n     -0.09118198,   0.043987993,    0.09125325,\n    -0.019055402,   0.058025226,     0.0337241,\n    -0.007700461,  -0.051549293,   0.033333547,\n     -0.04249377,   0.036880627,   0.030723767,\n    -0.010024755,  -0.039599802,    0.04335347,\n    -0.013405023,   0.027561612\n  ],\n  [\n     0.058089074,   0.020941732,   -0.10872878,\n     -0.04039259,  -0.044404425,   0.008074058,\n     0.029530387,    0.06106795,  -0.040581264,\n    0.0076176887,  -0.052324653, -0.0017251427,\n     0.011229625,  -0.022567088,  -0.039304733,\n     -0.06767425,   0.050662234,  -0.043872133,\n     -0.06564835, -0.0030828284,   0.031009678,\n     -0.06744081,   -0.02735545,   -0.04357851,\n    -0.015614915,   0.021484021,    0.02469868,\n    -0.022331946,  -0.027676083,   0.000084142,\n     0.033065446,   0.008663191,   0.010000569,\n     -0.03285902,   0.025153043,   -0.07011023,\n    -0.021991147,   -0.05085551,   0.033521175,\n     -0.06584735,  -0.045584362,  -0.027165776,\n    -0.025329018,    0.03824336,  -0.036927626,\n     0.062992625,   0.004152124,   0.049990047,\n    -0.016771948,    0.02209841\n  ]\n] ...TRIMMED",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Embeddings Quickstart"
    ]
  },
  {
    "objectID": "quickstarts/Embeddings.html#truncating-embeddings",
    "href": "quickstarts/Embeddings.html#truncating-embeddings",
    "title": "Gemini API: Embeddings Quickstart",
    "section": "Truncating embeddings",
    "text": "Truncating embeddings\nThe text-embedding-004 model also supports lower embedding dimensions. Specify outputDimensionality to truncate the output.\n\nconst trucated_embeddings = await ai.models.embedContent({\n  model: MODEL_ID,\n  contents: [\"Hello world\"],\n  config: {\n    outputDimensionality: 10,\n  },\n});\nconsole.log(\"Truncated Text Embeddings:\", trucated_embeddings.embeddings?.[0].values);\nconsole.log(\"Truncated Text Embeddings Length:\", trucated_embeddings.embeddings?.[0].values?.length);\n\nTruncated Text Embeddings: [\n   0.013168517,   -0.00871193,\n  -0.046782672, 0.00069969177,\n  -0.009518872,  -0.008720178,\n    0.06010358,   0.024755737,\n   0.026053527,   0.054356426\n]\nTruncated Text Embeddings Length: 10",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Embeddings Quickstart"
    ]
  },
  {
    "objectID": "quickstarts/Embeddings.html#specify-tasktype",
    "href": "quickstarts/Embeddings.html#specify-tasktype",
    "title": "Gemini API: Embeddings Quickstart",
    "section": "Specify taskType",
    "text": "Specify taskType\nFor details on how to call embedContent, check out the Embeddings API reference, including the section on taskType.\n\nconst with_no_task_type = await ai.models.embedContent({\n  model: MODEL_ID,\n  contents: [\"Hello world\"],\n});\nconsole.log(\"With No Task Type Text Embeddings:\", with_no_task_type.embeddings?.[0].values?.slice(0, 50), \"...TRIMMED\");\n\nconst with_task_type = await ai.models.embedContent({\n  model: MODEL_ID,\n  contents: [\"Hello world\"],\n  config: {\n    taskType: \"retrieval_document\",\n  },\n});\nconsole.log(\"With Task Type Text Embeddings:\", with_task_type.embeddings?.[0].values?.slice(0, 50), \"...TRIMMED\");\n\nWith No Task Type Text Embeddings: [\n    0.013168517,   -0.00871193,  -0.046782672,\n  0.00069969177,  -0.009518872,  -0.008720178,\n     0.06010358,   0.024755737,   0.026053527,\n    0.054356426,   -0.03793384, -0.0014235445,\n    0.030605137,  -0.015512642,  -0.012904964,\n    -0.02880739,  -0.007819577,   0.012152762,\n     -0.1139952,   0.010654231,   0.005365246,\n   -0.001178891,  -0.029781109,  -0.060107403,\n   -0.015272871, -0.0036046242,   0.006147686,\n    0.031175768,   0.021421982,    0.03710434,\n   -0.037202735,   0.046146937,   0.002196372,\n   -0.031793054,   0.009660255,   0.012500477,\n     -0.0509635,     0.0211728,   0.014332891,\n   -0.057802226,  -0.027034516,    0.03680537,\n   0.0016361808,     0.0085209,    0.04331588,\n   -0.032519087,   0.018076202, -0.0031592466,\n   0.0045996527, -0.0063372543\n] ...TRIMMED\nWith Task Type Text Embeddings: [\n   0.023399517,  -0.008547142, -0.052534223,\n  -0.012143115,  0.0055042417, -0.001007702,\n   0.028462889,   0.037563253, -0.023794618,\n   0.045095872,  -0.014744246, 0.0034827266,\n    0.07847462, -0.0088950405, -0.008143913,\n  -0.045579128,   0.017674835,  0.010504201,\n   -0.11951811,   0.025116991,   0.02234638,\n  -0.031670354,   0.009702842, -0.011191917,\n  -0.020584144,   -0.01684894, -0.000543171,\n   0.003336858, -0.0024763693,  0.018954858,\n    0.03398058,    0.07639708, 0.0037753258,\n  -0.055841617,    0.01699452,  0.011088602,\n  -0.016603941,    0.03163179,  0.039773043,\n   -0.04514968,   -0.07688451,  0.030634686,\n  -0.003726261,   0.042314958,  0.014746045,\n   -0.03570954,  -0.008157468,  0.026814101,\n  -0.031223731,   0.022471815\n] ...TRIMMED",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Embeddings Quickstart"
    ]
  },
  {
    "objectID": "quickstarts/Embeddings.html#learning-more",
    "href": "quickstarts/Embeddings.html#learning-more",
    "title": "Gemini API: Embeddings Quickstart",
    "section": "Learning more",
    "text": "Learning more\nCheck out these examples in the Cookbook to learn more about what you can do with embeddings:\n\nSearch Reranking: Use embeddings from the Gemini API to rerank search results from Wikipedia.\nAnomaly detection with embeddings: Use embeddings from the Gemini API to detect potential outliers in your dataset.\nTrain a text classifier: Use embeddings from the Gemini API to train a model that can classify different types of newsgroup posts based on the topic.\nEmbeddings have many applications in Vector Databases, too. Check out this example with Chroma DB.\n\nYou can learn more about embeddings in general on ai.google.dev in the embeddings guide\n\nYou can find additional code examples with the JS SDK here.\nYou can also find more details in the API Reference for embedContent and batchEmbedContents.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Embeddings Quickstart"
    ]
  },
  {
    "objectID": "quickstarts/File_API.html",
    "href": "quickstarts/File_API.html",
    "title": "Gemini API: File API Quickstart",
    "section": "",
    "text": "The Gemini API supports prompting with text, image, and audio data, also known as multimodal prompting. You can include text, image, and audio in your prompts. For small images, you can point the Gemini model directly to a local file when providing a prompt. For larger text files, images, videos, and audio, upload the files with the File API before including them in prompts.\nThe File API lets you store up to 20GB of files per project, with each file not exceeding 2GB in size. Files are stored for 48 hours and can be accessed with your API key for generation within that time period. It is available at no cost in all regions where the Gemini API is available.\nFor information on valid file formats (MIME types) and supported models, see the documentation on supported file formats and view the text examples at the end of this guide.\nThis guide shows how to use the File API to upload a media file and include it in a generateContent call to the Gemini API. For more information, see the code samples.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: File API Quickstart"
    ]
  },
  {
    "objectID": "quickstarts/File_API.html#setup",
    "href": "quickstarts/File_API.html#setup",
    "title": "Gemini API: File API Quickstart",
    "section": "Setup",
    "text": "Setup\n\nInstall the Google GenAI SDK\nInstall the Google GenAI SDK from npm.\n$ npm install @google/genai\n\n\nSetup your API key\nYou can create your API key using Google AI Studio with a single click.\nRemember to treat your API key like a password. Don’t accidentally save it in a notebook or source file you later commit to GitHub. In this notebook we will be storing the API key in a .env file. You can also set it as an environment variable or use a secret manager.\nHere’s how to set it up in a .env file:\n$ touch .env\n$ echo \"GEMINI_API_KEY=&lt;YOUR_API_KEY&gt;\" &gt;&gt; .env\n\n\n\n\n\n\nImportant\n\n\n\nThe File API uses API keys for authentication and access. Uploaded files are associated with the API key’s cloud project. Unlike other Gemini APIs that use API keys, your API key also grants access data you’ve uploaded to the File API, so take extra care in keeping your API key secure. For best practices on securing API keys, refer to Google’s documentation.\n\n\n\n\n\n\n\n\nTip\n\n\n\nAnother option is to set the API key as an environment variable. You can do this in your terminal with the following command:\n$ export GEMINI_API_KEY=\"&lt;YOUR_API_KEY&gt;\"\n\n\n\n\nLoad the API key\nTo load the API key from the .env file, we will use the dotenv package. This package loads environment variables from a .env file into process.env.\n$ npm install dotenv\nThen, we can load the API key in our code:\n\nconst dotenv = require(\"dotenv\") as typeof import(\"dotenv\");\n\ndotenv.config({\n  path: \"../.env\",\n});\n\nconst GEMINI_API_KEY = process.env.GEMINI_API_KEY ?? \"\";\nif (!GEMINI_API_KEY) {\n  throw new Error(\"GEMINI_API_KEY is not set in the environment variables\");\n}\nconsole.log(\"GEMINI_API_KEY is set in the environment variables\");\n\nGEMINI_API_KEY is set in the environment variables\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn our particular case the .env is is one directory up from the notebook, hence we need to use ../ to go up one directory. If the .env file is in the same directory as the notebook, you can omit it altogether.\n│\n├── .env\n└── quickstarts\n    └── File_API.ipynb\n\n\n\n\nInitialize SDK Client\nWith the new SDK, now you only need to initialize a client with you API key (or OAuth if using Vertex AI). The model is now set in each call.\n\nconst google = require(\"@google/genai\") as typeof import(\"@google/genai\");\n\nconst ai = new google.GoogleGenAI({ apiKey: GEMINI_API_KEY });",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: File API Quickstart"
    ]
  },
  {
    "objectID": "quickstarts/File_API.html#upload-a-file",
    "href": "quickstarts/File_API.html#upload-a-file",
    "title": "Gemini API: File API Quickstart",
    "section": "Upload a file",
    "text": "Upload a file\nThe File API lets you upload a variety of multimodal MIME types, including images and audio formats. The File API handles inputs that can be used to generate content with model.generateContent or model.streamGenerateContent.\nThe File API accepts files under 2GB in size and can store up to 20GB of files per project. Files last for 2 days and cannot be downloaded from the API.\nFirst, you will prepare a sample image to upload to the API.\nNote: You can also upload your own files to use.\n\nconst fs = require(\"fs\") as typeof import(\"fs\");\nconst path = require(\"path\") as typeof import(\"path\");\n\nconst IMG_URL = \"https://storage.googleapis.com/generativeai-downloads/data/jetpack.png\";\n\nconst downloadFile = async (url: string, filePath: string) =&gt; {\n  const response = await fetch(url);\n  if (!response.ok) {\n    throw new Error(`Failed to download image: ${response.statusText}`);\n  }\n  const buffer = await response.blob();\n  const bufferData = Buffer.from(await buffer.arrayBuffer());\n  fs.writeFileSync(filePath, bufferData);\n};\n\nconst filePath = path.join(\"../assets\", \"jetpack.png\");\nawait downloadFile(IMG_URL, filePath);\n\n\nconst tslab = require(\"tslab\") as typeof import(\"tslab\");\n\ntslab.display.png(fs.readFileSync(\"../assets/jetpack.png\"));\n\n\n\n\n\n\n\n\nNext, you will upload that file to the File API.\n\nconst file = await ai.files.upload({\n  file: filePath,\n  config: {\n    displayName: \"jetpack.png\",\n    mimeType: \"image/png\",\n  },\n});\nconsole.log(\"File uploaded:\", JSON.stringify(file, null, 2));\n\nFile uploaded: {\n  \"name\": \"files/h10fapw8h9ra\",\n  \"displayName\": \"jetpack.png\",\n  \"mimeType\": \"image/png\",\n  \"sizeBytes\": \"1567837\",\n  \"createTime\": \"2025-06-15T12:38:56.523729Z\",\n  \"expirationTime\": \"2025-06-17T12:38:56.339515281Z\",\n  \"updateTime\": \"2025-06-15T12:38:56.523729Z\",\n  \"sha256Hash\": \"YWFjOTU0ZGRjYzg4ZTc0YmIxMGYwZDZjZmVjMWFhZGJjZDU0OTJmMzI5NTZmM2E0OGM3NDc0NzI1YTNkZDIzOQ==\",\n  \"uri\": \"https://generativelanguage.googleapis.com/v1beta/files/h10fapw8h9ra\",\n  \"state\": \"ACTIVE\",\n  \"source\": \"UPLOADED\"\n}\n\n\nThe response shows that the File API stored the specified displayName for the uploaded file and a uri to reference the file in Gemini API calls. Use response to track how uploaded files are mapped to URIs.\nDepending on your use cases, you could store the URIs in structures such as a dict or a database.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: File API Quickstart"
    ]
  },
  {
    "objectID": "quickstarts/File_API.html#get-a-file",
    "href": "quickstarts/File_API.html#get-a-file",
    "title": "Gemini API: File API Quickstart",
    "section": "Get a file",
    "text": "Get a file\nAfter uploading the file, you can verify the API has successfully received the files by calling files.get.\nIt lets you get the file metadata that have been uploaded to the File API that are associated with the Cloud project your API key belongs to. Only the name (and by extension, the uri) are unique. Only use the displayName to identify files if you manage uniqueness yourself.\n\nconst retreived_file = await ai.files.get({\n  name: file.name!,\n});\nconsole.log(\"File retrieved:\", JSON.stringify(retreived_file, null, 2));\n\nFile retrieved: {\n  \"name\": \"files/h10fapw8h9ra\",\n  \"displayName\": \"jetpack.png\",\n  \"mimeType\": \"image/png\",\n  \"sizeBytes\": \"1567837\",\n  \"createTime\": \"2025-06-15T12:38:56.523729Z\",\n  \"expirationTime\": \"2025-06-17T12:38:56.339515281Z\",\n  \"updateTime\": \"2025-06-15T12:38:56.523729Z\",\n  \"sha256Hash\": \"YWFjOTU0ZGRjYzg4ZTc0YmIxMGYwZDZjZmVjMWFhZGJjZDU0OTJmMzI5NTZmM2E0OGM3NDc0NzI1YTNkZDIzOQ==\",\n  \"uri\": \"https://generativelanguage.googleapis.com/v1beta/files/h10fapw8h9ra\",\n  \"state\": \"ACTIVE\",\n  \"source\": \"UPLOADED\"\n}",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: File API Quickstart"
    ]
  },
  {
    "objectID": "quickstarts/File_API.html#generate-content",
    "href": "quickstarts/File_API.html#generate-content",
    "title": "Gemini API: File API Quickstart",
    "section": "Generate content",
    "text": "Generate content\nAfter uploading the file, you can make generateContent requests that reference the file by providing the URI. In the JS SDK, you need to construct a Part object that includes the file URI and mime type and then pass it to the generateContent method.\nHere you create a prompt that starts with text and includes the uploaded image.\n\nconst content_response = await ai.models.generateContent({\n  model: \"gemini-2.5-flash-preview-05-20\",\n  contents: [\n    \"Describe the image with a creative description.\",\n    google.createPartFromUri(retreived_file.uri!, retreived_file.mimeType!),\n  ],\n});\ntslab.display.markdown(content_response.text ?? \"\");\n\nOn a classic lined notebook page, a visionary (or perhaps a very enthusiastic student) has penned the blueprint for the ultimate urban escape vehicle: the “JETPACK BACKPACK.” Rendered in crisp blue ink, the drawing features a sleek, almost minimalist backpack design, looking unassuming enough to carry textbooks or gym clothes.\nBut beneath its innocent exterior, magic awaits. From its base, two stubby, retractable boosters are poised to unleash their power, depicted by playful, swirling plumes of steam that hint at a “green/clean”, eco-friendly propulsion system. The designer promises it’s lightweight, fits an 18-inch laptop, and boasts padded strap support for comfortable ascent.\nYet, a mischievous detail catches the eye: while it proudly features “USB-C CHARGING” for modern convenience, the “15-MIN BATTERY LIFE” suggests that adventures in the sky will be thrillingly brief dashes, perhaps just enough to leapfrog over a traffic jam or make a truly grand entrance (or exit) before gravity reclaims its due. It’s a delightful doodle of daring, a dream of everyday flight captured on paper, proving that sometimes, the most imaginative journeys begin with a pen and a blank page.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: File API Quickstart"
    ]
  },
  {
    "objectID": "quickstarts/File_API.html#delete-files",
    "href": "quickstarts/File_API.html#delete-files",
    "title": "Gemini API: File API Quickstart",
    "section": "Delete files",
    "text": "Delete files\nFiles are automatically deleted after 2 days or you can manually delete them using files.delete().\n\nawait ai.files.delete({\n  name: retreived_file.name!,\n});\nconsole.log(\"File deleted:\", retreived_file.name);\n\nFile deleted: files/h10fapw8h9ra",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: File API Quickstart"
    ]
  },
  {
    "objectID": "quickstarts/File_API.html#supported-text-types",
    "href": "quickstarts/File_API.html#supported-text-types",
    "title": "Gemini API: File API Quickstart",
    "section": "Supported text types",
    "text": "Supported text types\nAs well as supporting media uploads, the File API can be used to embed text files, such as Python code, or Markdown files, into your prompts.\nThis example shows you how to load a markdown file into a prompt using the File API.\n\nconst MD_URL = \"https://raw.githubusercontent.com/google-gemini/cookbook/main/CONTRIBUTING.md\";\n\nconst mdFilePath = path.join(\"../assets\", \"contrib.md\");\nawait downloadFile(MD_URL, mdFilePath);\n\nconst mdFile = await ai.files.upload({\n  file: mdFilePath,\n  config: {\n    displayName: \"contrib.md\",\n    mimeType: \"text/markdown\",\n  },\n});\n\nconst md_content_response = await ai.models.generateContent({\n  model: \"gemini-2.5-flash-preview-05-20\",\n  contents: [\n    \"What should I do before I start writing, when following these guidelines?\",\n    google.createPartFromUri(mdFile.uri!, mdFile.mimeType!),\n  ],\n});\ntslab.display.markdown(md_content_response.text ?? \"\");\n\nBefore you start writing a contribution to the Gemini API Cookbook, you should do the following:\n\nSign the Contributor License Agreement (CLA):\n\nVisit https://cla.developers.google.com/ to see your current agreements or to sign a new one.\nThis gives Google permission to use and redistribute your contributions. (Check if you or your employer have already signed a Google CLA for another project, as you likely won’t need to do it again.)\n\nReview Style Guides:\n\nRead the highlights of the technical writing style guide to anticipate common feedback.\nCheck the relevant style guide for the programming language you will be using (e.g., Python).\n(For Python notebooks specifically): Consider running pyink over your notebook, as it helps with style consistency (though not strictly required before starting, it’s good to keep in mind).\n\nFile an Issue:\n\nBefore writing a single line or sending a pull request, file an issue on the GitHub repository.\nThis step is crucial for discussing your idea, getting guidance, and ensuring your content has full support before you invest time in writing it.\n\n\n\n\nSome common text formats are automatically detected, such as text/x-python, text/html and text/markdown. If you are using a file that you know is text, but is not automatically detected by the API as such, you can specify the MIME type as text/plain explicitly.\n\nconst CODE_URL = \"https://raw.githubusercontent.com/google/gemma.cpp/main/../examples/hello_world/run.cc\";\n\nconst codeFilePath = path.join(\"../assets\", \"gemma.cpp\");\nawait downloadFile(CODE_URL, codeFilePath);\n\nconst codeFile = await ai.files.upload({\n  file: codeFilePath,\n  config: {\n    displayName: \"gemma.cpp\",\n    mimeType: \"text/plain\",\n  },\n});\n\nconst code_content_response = await ai.models.generateContent({\n  model: \"gemini-2.5-flash-preview-05-20\",\n  contents: [\"What does this program do?\", google.createPartFromUri(codeFile.uri!, codeFile.mimeType!)],\n});\ntslab.display.markdown(code_content_response.text ?? \"\");\n\nThis C++ program is a demonstration of how to run the Gemma large language model for text generation, specifically highlighting a “constrained decoding” feature.\nHere’s a breakdown of what it does:\n\nInitialization and Argument Parsing:\n\nIt starts by parsing command-line arguments using gcpp::LoaderArgs, gcpp::InferenceArgs, and gcpp::AppArgs. These arguments likely configure how the Gemma model is loaded (e.g., model path, precision) and how inference is performed (e.g., batch size).\nIt supports a --help flag for displaying usage information.\n\nConstrained Decoding (Token Rejection):\n\nThis is a key feature demonstrated in this specific example. It iterates through command-line arguments to find --reject. Any subsequent numeric arguments after --reject are treated as token IDs to be excluded from generation. These “rejected” tokens are stored in a std::set&lt;int&gt;.\nThis allows the user to specify tokens that the model must not output during the generation process.\n\nGemma Model Setup:\n\nIt initializes the gcpp::Gemma model, which is the core large language model. This involves setting up the model’s topology, pools (likely for thread management and parallel computation, given the hwy and thread_pool includes), and MatMulEnv (matrix multiplication environment).\nA gcpp::KVCache is also created, which is essential for efficient sequence generation in transformer models.\n\nPrompt Definition and Tokenization:\n\nThe program defines a hardcoded prompt: \"Write a greeting to the world.\"\nThis prompt is then converted into a sequence of integer tokens using the model’s Tokenizer().\n\nText Generation and Output:\n\nIt sets up a std::mt19937 random number generator, seeded by std::random_device, which is used for sampling tokens during generation (e.g., when temperature is applied).\nstream_token Callback: A lambda function stream_token is defined. This function is called every time the model generates a new token. It decodes the token back into readable text and prints it to std::cout, giving a real-time streaming output experience.\naccept_token Callback (Constrained Decoding in action): A crucial part of runtime_config is the accept_token lambda. This function takes a candidate token and returns true if it’s allowed, false if it should be rejected. In this program, it checks if the candidate token is present in the reject_tokens set. If it is, false is returned, preventing the model from outputting that specific token.\nFinally, it calls model.Generate() with the prepared runtime_config, the tokenized prompt, the KV cache, and other parameters. The model then generates text based on the prompt, adhering to the specified constraints (like max_generated_tokens, temperature, and the accept_token filter).\n\n\nIn essence, this program demonstrates a basic text generation pipeline with the Gemma model, with a specific focus on showing how to implement constrained decoding by preventing the model from generating certain specified tokens. It’s a command-line application that will print the generated text to standard output.\nExample Usage (Hypothetical):\n# Basic run, model will generate freely\n./gemma_example --model=/path/to/gemma.ckpt\n\n# Run and prevent the model from generating tokens 123, 456, and 789\n./gemma_example --model=/path/to/gemma.ckpt --reject 123 456 789",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: File API Quickstart"
    ]
  },
  {
    "objectID": "quickstarts/File_API.html#next-steps",
    "href": "quickstarts/File_API.html#next-steps",
    "title": "Gemini API: File API Quickstart",
    "section": "Next Steps",
    "text": "Next Steps\n\nUseful API references:\nFor more information about the File API, check its API reference.\n\n\nRelated examples\nCheck those examples using the File API to give you more ideas on how to use that very useful feature:\n\nShare Voice memos with Gemini API and brainstorm ideas\nAnalyze videos to classify or summarize them\n\n\n\nContinue your discovery of the Gemini API\nIf you’re not already familiar with it, learn how tokens are counted. Then check how to use the File API to use Audio or Video files with the Gemini API.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: File API Quickstart"
    ]
  },
  {
    "objectID": "quickstarts/Image_out.html",
    "href": "quickstarts/Image_out.html",
    "title": "Gemini API: Gemini 2.0 Image output",
    "section": "",
    "text": "This notebook will show you how to use the Image-out feature of Gemini, using the model multimodal capabilities to output both images and texts, and iterate on an image through a discussion.\nThis feature is very close to what Imagen offers but in a slightly different way as the Image-out feature has been developed to work iteratively so if you want to make sure certain details are clearly followed, and you are ready to iterate on the image until it’s exactly what you envision, Image-out is for you.\nCheck the documentation for more details on both features and some more advice on when to use each one.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Gemini 2.0 Image output"
    ]
  },
  {
    "objectID": "quickstarts/Image_out.html#setup",
    "href": "quickstarts/Image_out.html#setup",
    "title": "Gemini API: Gemini 2.0 Image output",
    "section": "Setup",
    "text": "Setup\n\nInstall the Google GenAI SDK\nInstall the Google GenAI SDK from npm.\n$ npm install @google/genai\n\n\nSetup your API key\nYou can create your API key using Google AI Studio with a single click.\nRemember to treat your API key like a password. Don’t accidentally save it in a notebook or source file you later commit to GitHub. In this notebook we will be storing the API key in a .env file. You can also set it as an environment variable or use a secret manager.\nHere’s how to set it up in a .env file:\n$ touch .env\n$ echo \"GEMINI_API_KEY=&lt;YOUR_API_KEY&gt;\" &gt;&gt; .env\n\n\n\n\n\n\nTip\n\n\n\nAnother option is to set the API key as an environment variable. You can do this in your terminal with the following command:\n$ export GEMINI_API_KEY=\"&lt;YOUR_API_KEY&gt;\"\n\n\n\n\nLoad the API key\nTo load the API key from the .env file, we will use the dotenv package. This package loads environment variables from a .env file into process.env.\n$ npm install dotenv\nThen, we can load the API key in our code:\n\nconst dotenv = require(\"dotenv\") as typeof import(\"dotenv\");\n\ndotenv.config({\n  path: \"../.env\",\n});\n\nconst GEMINI_API_KEY = process.env.GEMINI_API_KEY ?? \"\";\nif (!GEMINI_API_KEY) {\n  throw new Error(\"GEMINI_API_KEY is not set in the environment variables\");\n}\nconsole.log(\"GEMINI_API_KEY is set in the environment variables\");\n\nGEMINI_API_KEY is set in the environment variables\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn our particular case the .env is is one directory up from the notebook, hence we need to use ../ to go up one directory. If the .env file is in the same directory as the notebook, you can omit it altogether.\n│\n├── .env\n└── quickstarts\n    └── Image_out.ipynb\n\n\n\n\nInitialize SDK Client\nWith the new SDK, now you only need to initialize a client with you API key (or OAuth if using Vertex AI). The model is now set in each call.\n\nconst google = require(\"@google/genai\") as typeof import(\"@google/genai\");\n\nconst ai = new google.GoogleGenAI({ apiKey: GEMINI_API_KEY });\n\n\n\nSelect a model\nImage-out is available through the gemini-2.0-flash-preview-image-generation model.\nFor more information about all Gemini models, check the documentation for extended information.\n\nconst tslab = require(\"tslab\") as typeof import(\"tslab\");\n\nconst MODEL_ID = \"gemini-2.0-flash-preview-image-generation\";",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Gemini 2.0 Image output"
    ]
  },
  {
    "objectID": "quickstarts/Image_out.html#utils",
    "href": "quickstarts/Image_out.html#utils",
    "title": "Gemini API: Gemini 2.0 Image output",
    "section": "Utils",
    "text": "Utils\nDefine helper functions for visualizing model responses.\n\nimport { GenerateContentResponse } from \"@google/genai\";\n\nconst fs = require(\"fs\") as typeof import(\"fs\");\nconst path = require(\"path\") as typeof import(\"path\");\n\nfunction displayResponse(response: GenerateContentResponse) {\n  for (const part of response.candidates?.[0]?.content?.parts ?? []) {\n    if (part.text) {\n      tslab.display.markdown(part.text);\n    } else if (part.inlineData) {\n      const imageData = part.inlineData.data!;\n      const buffer = Buffer.from(imageData, \"base64\");\n      tslab.display.png(buffer);\n    }\n  }\n}\n\nfunction saveImage(response: GenerateContentResponse, fileName: string) {\n  const imagePath = path.join(\"../assets\", fileName);\n  for (const part of response.candidates?.[0]?.content?.parts ?? []) {\n    if (part.inlineData) {\n      const imageData = part.inlineData.data!;\n      const buffer = Buffer.from(imageData, \"base64\");\n      fs.writeFileSync(imagePath, buffer);\n    }\n  }\n  console.log(`Image saved to ${imagePath}`);\n}",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Gemini 2.0 Image output"
    ]
  },
  {
    "objectID": "quickstarts/Image_out.html#generate-images",
    "href": "quickstarts/Image_out.html#generate-images",
    "title": "Gemini API: Gemini 2.0 Image output",
    "section": "Generate images",
    "text": "Generate images\nUse responseModalities to indicate to the model that you are expecting an image in the output. You’ll need to specify both \"text\" and \"image\" in your generation configuration. If you set only \"image\" in response_modalities, you’ll get an error.\nIf you want to generate image only outputs, you can use Imagen.\nRemember that generating people is not allowed at the moment.\n\nconst generate_image_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents:\n    \"create a 3d rendered image of a pig with wings and a top hat flying over a happy futuristic scifi city with lots of greenery\",\n  config: {\n    responseModalities: [google.Modality.TEXT, google.Modality.IMAGE],\n  },\n});\ndisplayResponse(generate_image_response);\nsaveImage(generate_image_response, \"pig_with_wings_and_top_hat.png\");\n\nI will create a 3D rendered image showing a whimsical scene: a pink pig wearing a distinguished grey top hat and a pair of small, delicate white wings soaring through the air. Below, a vibrant, futuristic cityscape unfolds with sleek, modern buildings interspersed with abundant lush greenery, suggesting a harmonious blend of nature and technology, all under a bright, cheerful sky.\n\n\n\n\n\n\n\n\n\nImage saved to ../assets/pig_with_wings_and_top_hat.png",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Gemini 2.0 Image output"
    ]
  },
  {
    "objectID": "quickstarts/Image_out.html#edit-images",
    "href": "quickstarts/Image_out.html#edit-images",
    "title": "Gemini API: Gemini 2.0 Image output",
    "section": "Edit images",
    "text": "Edit images\nYou can also do image editing, simply pass the original image as part of the prompt.\n\nconst edit_image_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: [\n    \"could you edit this image to make it look like a cat instead of a pig?\",\n    google.createPartFromBase64(fs.readFileSync(\"../assets/pig_with_wings_and_top_hat.png\", \"base64\"), \"image/png\"),\n  ],\n  config: {\n    responseModalities: [google.Modality.TEXT, google.Modality.IMAGE],\n  },\n});\ndisplayResponse(edit_image_response);\nsaveImage(edit_image_response, \"cat_with_wings_and_top_hat.png\");\n\nOkay, I will transform the flying pig into a flying cat while keeping the hat, wings, and the futuristic city background consistent. The new image will feature a fluffy cat with feline features, similar wings, and the same top hat, soaring over the cityscape.\n\n\n\n\n\n\n\n\n\nImage saved to ../assets/cat_with_wings_and_top_hat.png",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Gemini 2.0 Image output"
    ]
  },
  {
    "objectID": "quickstarts/Image_out.html#get-multiple-images",
    "href": "quickstarts/Image_out.html#get-multiple-images",
    "title": "Gemini API: Gemini 2.0 Image output",
    "section": "Get multiple images",
    "text": "Get multiple images\nSo far you’ve only generated one image per call, but you can request way more than that! Let’s try a baking receipe.\n\nconst multi_image_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: \"Show me how to bake a macaron with images.\",\n  config: {\n    responseModalities: [google.Modality.TEXT, google.Modality.IMAGE],\n  },\n});\ndisplayResponse(multi_image_response);\n\nOkay, here are the steps to bake macarons, with an image for each step.\nStep 1: Prepare the ingredients.\nGather your ingredients: almond flour, powdered sugar, granulated sugar, egg whites (aged), and food coloring (optional). Have your piping bag fitted with a round tip and parchment paper ready.\n\n\n\n\n\n\n\n\n\nStep 2: Sift the dry ingredients.\nThoroughly whisk together the almond flour and powdered sugar. Then, sift the mixture through a fine-mesh sieve into a large bowl to remove any lumps. This step is crucial for smooth macaron shells.\n\n\n\n\n\n\n\n\n\nStep 3: Make the meringue.\nIn a clean, grease-free bowl of a stand mixer fitted with the whisk attachment, beat the aged egg whites on medium-low speed until foamy. Gradually add the granulated sugar while continuing to beat on medium-high speed until stiff, glossy peaks form. The meringue should hold its shape when the whisk is lifted. If using, gently fold in the food coloring at the end.\n\n\n\n\n\n\n\n\n\nStep 4: Macaronage.\nPour the sifted dry ingredients into the meringue. Using a spatula, gently fold the mixture together using a circular motion, scraping down the sides of the bowl. Continue folding until the batter flows off your spatula in a thick ribbon and slowly melts back into the mixture in about 10-20 seconds. This is the macaronage stage, and it’s crucial not to overmix or undermix.\n\n\n\n\n\n\n\n\n\nStep 5: Pipe the macarons.\nTransfer the macaron batter into the prepared piping bag. Pipe small, even circles onto the parchment-lined baking sheets. Aim for consistent sizes, about 1.5 inches in diameter.\n\n\n\n\n\n\n\n\n\nStep 6: Tap the trays.\nFirmly tap the baking sheets several times on the counter to release any trapped air bubbles. Use a toothpick to pop any visible bubbles on the surface of the macarons.\n\n\n\n\n\n\n\n\n\nStep 7: Rest the macarons.\nLet the piped macaron shells sit at room temperature for 30-60 minutes, or until a skin forms on the surface. You should be able to gently touch them without the batter sticking to your finger. This step is crucial for developing the “feet.”\n\n\n\n\n\n\n\n\n\nStep 8: Bake the macarons.\nPreheat your oven to the correct temperature (usually between 285-325°F or 140-160°C, depending on your oven). Bake one tray at a time for 12-18 minutes, or until the “feet” have formed and the shells don’t wiggle when gently touched.\n\n\n\n\n\n\n\n\n\nStep 9: Cool and fill.\nLet the baked macaron shells cool completely on the baking sheets before attempting to remove them. Once cooled, match the shells by size and pipe your desired filling (like buttercream, ganache, or jam) onto the flat side of one shell. Gently sandwich it with another shell.\n\n\n\n\n\n\n\n\n\nStep 10: Mature (optional but recommended).\nFor the best flavor and texture, place the filled macarons in an airtight container and refrigerate them for at least 12-24 hours. This allows the flavors to meld and the filling to soften the shells slightly.\n\n\n\n\n\n\n\n\n\nEnjoy your homemade macarons! They are delicate and can be tricky, so don’t be discouraged if your first attempt isn’t perfect. Practice makes perfect!",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Gemini 2.0 Image output"
    ]
  },
  {
    "objectID": "quickstarts/Image_out.html#chat-recommended",
    "href": "quickstarts/Image_out.html#chat-recommended",
    "title": "Gemini API: Gemini 2.0 Image output",
    "section": "Chat (recommended)",
    "text": "Chat (recommended)\nSo far you’ve used unary calls, but Image-out is actually made to work better with chat mode as it’s easier to iterate on an image turn after turn.\n\nconst chat = ai.chats.create({\n  model: MODEL_ID,\n  config: {\n    responseModalities: [google.Modality.TEXT, google.Modality.IMAGE],\n  },\n});\n\n\nconst chat_response_1 = await chat.sendMessage({\n  message: \"create a photorealistic image of a giraffe with a tiny bird on its head\",\n});\ndisplayResponse(chat_response_1);\n\nI will generate a photorealistic image of a majestic giraffe in its natural habitat, perhaps near some acacia trees, with a tiny, colorful bird with blue and red plumage resting on its head, seemingly unfazed by the height.\n\n\n\n\n\n\n\n\n\n\nconst chat_response_2 = await chat.sendMessage({\n  message: \"use a cartoon style instead of photorealistic\",\n});\ndisplayResponse(chat_response_2);\n\nI will create a cartoon-style image of a friendly-looking giraffe with large, expressive eyes, and a small, round, blue cartoon bird cheerfully perched on its head. The style will be bright and colorful.\n\n\n\n\n\n\n\n\n\n\nconst chat_response_3 = await chat.sendMessage({\n  message: \"add a rainbow and some colorful birds\",\n});\ndisplayResponse(chat_response_3);\n\nI will generate a cheerful cartoon image of the same friendly giraffe with the blue bird on its head, now with a bright, curved rainbow arcing across a sunny sky in the background. Several other small, colorful cartoon birds of different shapes and sizes will be flying around the giraffe and the rainbow.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Gemini 2.0 Image output"
    ]
  },
  {
    "objectID": "quickstarts/Image_out.html#next-steps",
    "href": "quickstarts/Image_out.html#next-steps",
    "title": "Gemini API: Gemini 2.0 Image output",
    "section": "Next Steps",
    "text": "Next Steps\n\nUseful documentation references:\nCheck the documentation for more details about the image generation capabilities of the model. To improve your prompting skills, check out the Imagen prompt guide for great advices on creating your prompts.\n\n\nCheck-out Imagen as well:\nThe Imagen model is another way to generate images. Check out the Get Started with Imagen notebook to start playing with it too.\nHere are some Imagen examples to get your imagination started on how to use it in creative ways:\n\nIllustrate a book: Use Gemini and Imagen to create illustration for an open-source book\n\n\n\nContinue your discovery of the Gemini API\nGemini is not only good at generating images, but also at understanding them. Check the Spatial understanding guide for an introduction on those capabilities, and the Video understanding one for video examples.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Gemini 2.0 Image output"
    ]
  },
  {
    "objectID": "assets/contrib.html",
    "href": "assets/contrib.html",
    "title": "Contributing to the Gemini API Cookbook",
    "section": "",
    "text": "We would love to accept your patches and contributions to the Gemini API Cookbook. We are excited that you are considering donating some of your time, and this guide will help us be respectful of that time."
  },
  {
    "objectID": "assets/contrib.html#sign-our-contributor-agreement",
    "href": "assets/contrib.html#sign-our-contributor-agreement",
    "title": "Contributing to the Gemini API Cookbook",
    "section": "Sign our contributor agreement",
    "text": "Sign our contributor agreement\nAll contributions to this project must be accompanied by a Contributor License Agreement (CLA). You (or your employer) retain the copyright to your contribution; this simply gives us permission to use and redistribute your contributions as part of the project.\nIf you or your current employer have already signed the Google CLA (even if it was for a different project), you probably don’t need to do it again.\nVisit https://cla.developers.google.com/ to see your current agreements or to sign a new one."
  },
  {
    "objectID": "assets/contrib.html#style-guides",
    "href": "assets/contrib.html#style-guides",
    "title": "Contributing to the Gemini API Cookbook",
    "section": "Style guides",
    "text": "Style guides\nBefore you start writing, take a look at the technical writing style guide. You don’t need to fully digest the whole document, but do read the highlights so you can anticipate the most common feedback.\nAlso check out the relevant style guide for the language you will be using. These apply strictly to raw code files (e.g. .py, .js), though code fragments in documentation (such as markdown files or notebooks) tend to favor readability over strict adherence.\nFor Python notebooks (*.ipynb files), consider running pyink over your notebook. It is not required, but it will avoid style-related nits.\nSee below for more detailed guidelines specific to writing notebooks and guides."
  },
  {
    "objectID": "assets/contrib.html#small-fixes",
    "href": "assets/contrib.html#small-fixes",
    "title": "Contributing to the Gemini API Cookbook",
    "section": "Small fixes",
    "text": "Small fixes\nSmall fixes, such as typos or bug fixes, can be submitted directly via a pull request."
  },
  {
    "objectID": "assets/contrib.html#content-submission",
    "href": "assets/contrib.html#content-submission",
    "title": "Contributing to the Gemini API Cookbook",
    "section": "Content submission",
    "text": "Content submission\nBefore you send a PR, or even write a single line, please file an issue. There we can discuss the request and provide guidance about how to structure any content you write.\nAdding a new guide often involves lots of detailed reviews and we want to make sure that your idea is fully formed and has full support before you start writing anything. If you want to port an existing guide across (e.g. if you have a guide for Gemini on your own GitHub), feel free to link to it in the issue.\nWhen you’re ready, start by using the notebook template and following the guidance within.\nBefore submitting your notebook, it’s recommended to run linting and formatting tools locally to ensure consistency and adherence to style guidelines.\n\nInstall Dependencies:\n\nFirst, install the necessary packages using pip:\npip install -U tensorflow-docs\n\nFormat the Notebook:\n\nUse the nbfmt tool from tensorflow-docs to automatically format your notebook:\npython -m tensorflow_docs.tools.nbfmt path/to/notebook\nReplace path/to/notebook with the actual path to your notebook file.\n\nLint the Notebook:\n\nUse the nblint tool to check for style and consistency issues:\npython -m tensorflow_docs.tools.nblint \\\n            --styles=google,tensorflow \\\n            --arg=repo:google-gemini/cookbook \\\n            --arg=branch:main \\\n            --exclude_lint=tensorflow::button_download \\\n            --exclude_lint=tensorflow::button_website \\\n            --arg=base_url:https://ai.google.dev/ \\\n            --exclude_lint=tensorflow::button_github \\\n            path/to/notebook\nReplace path/to/notebook with the actual path to your notebook file."
  },
  {
    "objectID": "assets/contrib.html#things-we-consider",
    "href": "assets/contrib.html#things-we-consider",
    "title": "Contributing to the Gemini API Cookbook",
    "section": "Things we consider",
    "text": "Things we consider\nWhen accepting a new guide, we want to balance a few aspects. * Originality - e.g. Is there another guide that does the same thing? * Pedagogy - e.g. Does this guide teach something useful? Specifically for a Gemini API feature? * Quality - e.g. Does this guide contain clear, descriptive prose? Is the code easy to understand?\nIt is not crucial for a submission to be strong along all of these dimensions, but the stronger the better. Old submissions may be replaced in favor of newer submissions that exceed these properties."
  },
  {
    "objectID": "assets/contrib.html#attribution",
    "href": "assets/contrib.html#attribution",
    "title": "Contributing to the Gemini API Cookbook",
    "section": "Attribution",
    "text": "Attribution\nIf you have authored a new guide from scratch, you are welcome to include a byline at the top of the document with your name and GitHub username."
  },
  {
    "objectID": "assets/contrib.html#notebook-style",
    "href": "assets/contrib.html#notebook-style",
    "title": "Contributing to the Gemini API Cookbook",
    "section": "Notebook Style",
    "text": "Notebook Style\n\nInclude the collapsed license at the top (uses the Colab “Form” mode to hide the cells).\nSave the notebook with the table of contents open.\nUse one H1 header (# in Markdown) for the title.\nInclude the “Open in Colab” button immediately after the H1. It should look like: html     &lt;a target=\"_blank\" href=\"URL\"&gt;&lt;img src=\"[https://colab.research.google.com/assets/colab-badge.svg](https://colab.research.google.com/assets/colab-badge.svg)\" height=30/&gt;&lt;/a&gt; where URL should be https://colab.research.google.com/github/google-gemini/cookbook/blob/main/ followed by the notebook location in the cookbook.\nInclude an overview section before any code.\nPut all your installs (using %pip instead of !pip) and imports in a dedicated setup section near the beginning.\nKeep code and text cells as brief as possible.\nBreak text cells at headings.\nBreak code cells between distinct logical steps, such as “building” and “running”, or between processing/printing different results.\nNecessary but uninteresting code (like helper functions) should be hidden in a toggleable code cell by putting # @title as the first line."
  },
  {
    "objectID": "assets/contrib.html#code-style",
    "href": "assets/contrib.html#code-style",
    "title": "Contributing to the Gemini API Cookbook",
    "section": "Code Style",
    "text": "Code Style\n\nNotebooks are for people. Write code optimized for clarity.\nUse the Google Python Style Guide, where applicable. Code formatted by pyink will always be accepted.\nUse 4 spaces per indentation level. (PEP 8 recommendation)\nIn particular, defining functions and variables takes extra spaces around the = sign, while function parameters don’t: ```python var = value\nfunction( parameter=value ) ```\nWhen a function has multiple parameters, expand it onto multiple lines with proper indentation for better readability: python     response = client.models.generate_content(         model=MODEL_ID,         contents=\"Here's my prompt\",         config={             \"response_mime_type\": \"application/json\",             \"response_schema\": Schema         }     ) Notice the line break after the opening parenthesis and before the closing parenthesis.\nLong text variables should use triple double quotes and proper indentation for better readability: python     long_prompt = \"\"\"         Cupcake ipsum dolor. Sit amet marshmallow topping cheesecake muffin.         Halvah croissant candy canes bonbon candy. Apple pie jelly beans topping carrot cake danish tart cake cheesecake.         Muffin danish chocolate soufflé pastry icing bonbon oat cake. Powder cake jujubes oat cake.         Lemon drops tootsie roll marshmallow halvah carrot cake.     \"\"\" Notice the line break after the opening triple quotes and before the closing triple quotes.\nDemonstrate small parts before combining them into something more complex.\nIf you define a function, ideally run it and show its output immediately before using it in another function or a more complex block.\nOnly use helper functions when necessary (e.g., for code reuse or complexity management). If a piece of logic is only a couple of lines and used once, it’s often clearer to write it inline so readers don’t have to look up the function definition. Hide helper function definitions using # @title.\nKeep examples quick and concise. Do not add extra options or parameters (like temperature) without explaining them; focus on what you want to showcase.\nIf you must use extra parameters, explain why they are needed and the reasoning behind the specific value the first time you use them.\nWhen selecting a model, use a Colab form selector for easier maintainability: python     MODEL_ID=\"gemini-1.5-flash\" # @param [\"gemini-1.0-pro\", \"gemini-1.5-flash\", \"gemini-1.5-pro\"] {\"allow-input\":true, isTemplate: true}\nSome notebooks can benefit from having a form to update the prompt: python     prompt = \"Detect the 2d bounding boxes of the cupcakes (with 'label' as topping description')\"  # @param {type:\"string\"} or a list of prompts: python     prompt = \"Draw a square around the fox' shadow\"  # @param [\"Find the two origami animals.\", \"Where are the origamis' shadows?\",\"Draw a square around the fox' shadow\"] {\"allow-input\":true}\nTo ensure notebook text remains accurate, present model metadata (like context window size) by executing code, not by hardcoding it in Markdown.\n\nExample: Instead of writing “This model has a 1M token context window”, display the output of genai.get_model('models/gemini-1.5-pro-latest').input_token_limit."
  },
  {
    "objectID": "assets/contrib.html#naming-conventions",
    "href": "assets/contrib.html#naming-conventions",
    "title": "Contributing to the Gemini API Cookbook",
    "section": "Naming Conventions",
    "text": "Naming Conventions\n\nVariables: Use lowercase with underscores (snake_case): user_name, total_count\nConstants: Use uppercase with underscores: MAX_VALUE, DATABASE_NAME\nFunctions: Use lowercase with underscores (snake_case): calculate_total(), process_data()\nClasses: Use CapWords (CamelCase): UserManager, PaymentProcessor\nModules: Use lowercase with underscores (snake_case): user_utils, payment_gateway"
  },
  {
    "objectID": "assets/contrib.html#comments",
    "href": "assets/contrib.html#comments",
    "title": "Contributing to the Gemini API Cookbook",
    "section": "Comments",
    "text": "Comments\n\nWrite clear and concise comments: Explain the “why” behind the code, not just the “what”.\nComment sparingly: Well-written code should be self-documenting where possible.\nUse complete sentences: Start comments with a capital letter and use proper punctuation."
  },
  {
    "objectID": "assets/contrib.html#outputs",
    "href": "assets/contrib.html#outputs",
    "title": "Contributing to the Gemini API Cookbook",
    "section": "Outputs",
    "text": "Outputs\n\nWhenever possible, simply use print() for basic output.\nWhen needed, use display(Markdown()) for formatted Markdown text, print(json.dumps(json_string, indent=4)) for readable JSON, or display(Image()) for images."
  },
  {
    "objectID": "assets/contrib.html#text",
    "href": "assets/contrib.html#text",
    "title": "Contributing to the Gemini API Cookbook",
    "section": "Text",
    "text": "Text\n\nUse an imperative style: “Run a prompt using the API.”\nUse sentence case in titles/headings: “Download the data”, “Call the API”, “Process the results”.\nUse short titles/headings.\nUse the Google developer documentation style guide.\nUse second person: “you” rather than “we”. (You will fail the lint check otherwise).\nExplain what you are doing and the features you are using. Link to relevant documentation or other notebooks for more details where appropriate."
  },
  {
    "objectID": "assets/contrib.html#github-workflow",
    "href": "assets/contrib.html#github-workflow",
    "title": "Contributing to the Gemini API Cookbook",
    "section": "GitHub Workflow",
    "text": "GitHub Workflow\n\nBe consistent about how you save your notebooks (e.g., with ToC open, potentially omitting outputs) to keep JSON diffs manageable. Tools like nbfmt and nblint can help enforce consistency.\nConsider setting the “Omit code cell output when saving this notebook” option if outputs (like inline images) make diffs too large for GitHub.\nReviewNB.com can assist with reviewing notebook diffs in pull requests.\nUse the Open in Colab browser extension to easily open GitHub notebooks in Colab.\nThe easiest way to edit a notebook tracked in GitHub is often:\n\nOpen the notebook in Colab directly from the GitHub branch you intend to edit.\nMake your changes in Colab.\nUse Colab’s “File” -&gt; “Save a copy in GitHub” menu option to save it back to the same branch.\n\nFor Pull Requests (PRs), it’s helpful to include a direct Colab link to the notebook version in the PR head for easier review: https://colab.research.google.com/github/{USER}/{REPO}/blob/{BRANCH}/{PATH}.ipynb"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Gemini API Cookbook",
    "section": "",
    "text": "This cookbook provides a structured learning path for using the Gemini API, focusing on hands-on tutorials and practical examples.\nFor comprehensive API documentation, visit ai.google.dev.\n\n\nThis cookbook is organized into two main categories:\n\nQuick Starts: Step-by-step guides covering both introductory topics (“Get Started”) and specific API features.\nExamples: Practical use cases demonstrating how to combine multiple features.\n\nWe also showcase Demos in separate repositories, illustrating end-to-end applications of the Gemini API.\n\n\n\nHere are the recent additions and updates to the Gemini API and the Cookbook:\n\nGemini 2.5 models: Explore the capabilities of the latest Gemini 2.5 models (Flash and Pro)! See the Get Started Guide and the thinking guide as they’ll all be thinking ones.\nImagen and Veo: Get started with our media generation model with this brand new Veo guide and Imagen guide!\nLyria and TTS: Get started with podcast and music generation with the TTS and Lyria RealTime models.\nLiveAPI: Get started with the multimodal Live API and unlock new interactivity with Gemini.\nRecently Added Guides:\n\nBrowser as a tool: Use a web browser for live and internal (intranet) web interactions\nGrounding: Discover different ways to ground Gemini’s answer using different tools, from Google Search to Youtube and the new url context tool.\n\n\n\n\n\nThe quickstarts section contains step-by-step tutorials to get you started with Gemini and learn about its specific features.\nTo begin, you’ll need:\n\nA Google account.\nAn API key (create one in Google AI Studio).\n\nWe recommend starting with the following:\n\nAuthentication: Set up your API key for access.\nGet started: Get started with Gemini models and the Gemini API, covering basic prompting and multimodal input.\n\nThen, explore the other quickstarts tutorials to learn about individual features:\n\nGet started with Live API: Get started with the live API with this comprehensive overview of its capabilities\nGet started with Veo: Get started with our video generation capabilities\nGet started with Imagen and Image-out: Get started with our image generation capabilities\nGrounding: use Google Search for grounded responses\nCode execution: Generating and running Python code to solve complex tasks and even ouput graphs\nAnd many more\n\n\n\n\nThese examples demonstrate how to combine multiple Gemini API features or 3rd-party tools to build more complex applications. - Illustrate a book: Use Gemini and Imagen to create illustration for an open-source book - Animated Story Generation: Create animated videos by combining Gemini’s story generation, Imagen, and audio synthesis - Plotting and mapping Live: Mix Live API and Code execution to solve complex tasks live - 3D Spatial understanding: Use Gemini 3D spatial abilities to understand 3D scenes - Gradio and live API: Use gradio to deploy your own instance of the Live API - And many many more\n\n\n\nThese fully functional, end-to-end applications showcase the power of Gemini in real-world scenarios.\n\nGemini API quickstart: Python Flask App running with the Google AI Gemini API, designed to get you started building with Gemini’s multi-modal capabilities\nMultimodal Live API Web Console: React-based starter app for using the Multimodal Live API over a websocket\nGoogle AI Studio Starter Applets: A collection of small apps that demonstrate how Gemini can be used to create interactive experiences\n\n\n\n\nThe Gemini API is a REST API. You can call it directly using tools like curl (see REST examples or the great Postman workspace), or use one of our official SDKs:\n\nPython\nGo\nNode.js\nDart (Flutter)\nAndroid\nSwift\n\n\n\n\nWith Gemini 2 we are offering a new SDK (@google/genai, v1.0). The updated SDK is fully compatible with all Gemini API models and features, including recent additions like the live API (audio + video streaming), improved tool usage ( code execution, function calling and integrated Google search grounding), and media generation (Imagen and Veo). This SDK allows you to connect to the Gemini API through either Google AI Studio or Vertex AI.\nThe generative-ai-js package will continue to support the original Gemini models. It can also be used with Gemini 2 models, just with a limited feature set. All new features will be developed in the new Google GenAI SDK.\nSee the migration guide for details.\n\n\n\nAsk a question on the Google AI Developer Forum.\n\n\n\nFor enterprise developers, the Gemini API is also available on Google Cloud Vertex AI. See this repo for examples.\n\n\n\nContributions are welcome! See CONTRIBUTING.md for details.\nThank you for developing with the Gemini API! We’re excited to see what you create."
  },
  {
    "objectID": "index.html#navigating-the-cookbook",
    "href": "index.html#navigating-the-cookbook",
    "title": "Gemini API Cookbook",
    "section": "",
    "text": "This cookbook is organized into two main categories:\n\nQuick Starts: Step-by-step guides covering both introductory topics (“Get Started”) and specific API features.\nExamples: Practical use cases demonstrating how to combine multiple features.\n\nWe also showcase Demos in separate repositories, illustrating end-to-end applications of the Gemini API."
  },
  {
    "objectID": "index.html#whats-new",
    "href": "index.html#whats-new",
    "title": "Gemini API Cookbook",
    "section": "",
    "text": "Here are the recent additions and updates to the Gemini API and the Cookbook:\n\nGemini 2.5 models: Explore the capabilities of the latest Gemini 2.5 models (Flash and Pro)! See the Get Started Guide and the thinking guide as they’ll all be thinking ones.\nImagen and Veo: Get started with our media generation model with this brand new Veo guide and Imagen guide!\nLyria and TTS: Get started with podcast and music generation with the TTS and Lyria RealTime models.\nLiveAPI: Get started with the multimodal Live API and unlock new interactivity with Gemini.\nRecently Added Guides:\n\nBrowser as a tool: Use a web browser for live and internal (intranet) web interactions\nGrounding: Discover different ways to ground Gemini’s answer using different tools, from Google Search to Youtube and the new url context tool."
  },
  {
    "objectID": "index.html#quick-starts",
    "href": "index.html#quick-starts",
    "title": "Gemini API Cookbook",
    "section": "",
    "text": "The quickstarts section contains step-by-step tutorials to get you started with Gemini and learn about its specific features.\nTo begin, you’ll need:\n\nA Google account.\nAn API key (create one in Google AI Studio).\n\nWe recommend starting with the following:\n\nAuthentication: Set up your API key for access.\nGet started: Get started with Gemini models and the Gemini API, covering basic prompting and multimodal input.\n\nThen, explore the other quickstarts tutorials to learn about individual features:\n\nGet started with Live API: Get started with the live API with this comprehensive overview of its capabilities\nGet started with Veo: Get started with our video generation capabilities\nGet started with Imagen and Image-out: Get started with our image generation capabilities\nGrounding: use Google Search for grounded responses\nCode execution: Generating and running Python code to solve complex tasks and even ouput graphs\nAnd many more"
  },
  {
    "objectID": "index.html#examples-practical-use-cases",
    "href": "index.html#examples-practical-use-cases",
    "title": "Gemini API Cookbook",
    "section": "",
    "text": "These examples demonstrate how to combine multiple Gemini API features or 3rd-party tools to build more complex applications. - Illustrate a book: Use Gemini and Imagen to create illustration for an open-source book - Animated Story Generation: Create animated videos by combining Gemini’s story generation, Imagen, and audio synthesis - Plotting and mapping Live: Mix Live API and Code execution to solve complex tasks live - 3D Spatial understanding: Use Gemini 3D spatial abilities to understand 3D scenes - Gradio and live API: Use gradio to deploy your own instance of the Live API - And many many more"
  },
  {
    "objectID": "index.html#demos-end-to-end-applications",
    "href": "index.html#demos-end-to-end-applications",
    "title": "Gemini API Cookbook",
    "section": "",
    "text": "These fully functional, end-to-end applications showcase the power of Gemini in real-world scenarios.\n\nGemini API quickstart: Python Flask App running with the Google AI Gemini API, designed to get you started building with Gemini’s multi-modal capabilities\nMultimodal Live API Web Console: React-based starter app for using the Multimodal Live API over a websocket\nGoogle AI Studio Starter Applets: A collection of small apps that demonstrate how Gemini can be used to create interactive experiences"
  },
  {
    "objectID": "index.html#official-sdks",
    "href": "index.html#official-sdks",
    "title": "Gemini API Cookbook",
    "section": "",
    "text": "The Gemini API is a REST API. You can call it directly using tools like curl (see REST examples or the great Postman workspace), or use one of our official SDKs:\n\nPython\nGo\nNode.js\nDart (Flutter)\nAndroid\nSwift"
  },
  {
    "objectID": "index.html#important-migration",
    "href": "index.html#important-migration",
    "title": "Gemini API Cookbook",
    "section": "",
    "text": "With Gemini 2 we are offering a new SDK (@google/genai, v1.0). The updated SDK is fully compatible with all Gemini API models and features, including recent additions like the live API (audio + video streaming), improved tool usage ( code execution, function calling and integrated Google search grounding), and media generation (Imagen and Veo). This SDK allows you to connect to the Gemini API through either Google AI Studio or Vertex AI.\nThe generative-ai-js package will continue to support the original Gemini models. It can also be used with Gemini 2 models, just with a limited feature set. All new features will be developed in the new Google GenAI SDK.\nSee the migration guide for details."
  },
  {
    "objectID": "index.html#get-help",
    "href": "index.html#get-help",
    "title": "Gemini API Cookbook",
    "section": "",
    "text": "Ask a question on the Google AI Developer Forum."
  },
  {
    "objectID": "index.html#the-gemini-api-on-google-cloud-vertex-ai",
    "href": "index.html#the-gemini-api-on-google-cloud-vertex-ai",
    "title": "Gemini API Cookbook",
    "section": "",
    "text": "For enterprise developers, the Gemini API is also available on Google Cloud Vertex AI. See this repo for examples."
  },
  {
    "objectID": "index.html#contributing",
    "href": "index.html#contributing",
    "title": "Gemini API Cookbook",
    "section": "",
    "text": "Contributions are welcome! See CONTRIBUTING.md for details.\nThank you for developing with the Gemini API! We’re excited to see what you create."
  },
  {
    "objectID": "quickstarts/Get_started_thinking.html",
    "href": "quickstarts/Get_started_thinking.html",
    "title": "Use Gemini thinking",
    "section": "",
    "text": "Gemini 2.5 Flash, Gemini 2.5 Pro and Gemini Flash 2.0 Thinking are models that are trained to do a thinking process (or reasoning) before getting to a final answer. As a result, those models are capable of stronger reasoning capabilities in its responses than previous models.\nYou’ll see examples of those reasoning capabilities with code understanding, geometry and math problems.\nAs you will see, the model is exposing its thoughts so you can have a look at its reasoning and how it did reach its conclusions.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Use Gemini thinking"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_thinking.html#understanding-the-thinking-models",
    "href": "quickstarts/Get_started_thinking.html#understanding-the-thinking-models",
    "title": "Use Gemini thinking",
    "section": "Understanding the thinking models",
    "text": "Understanding the thinking models\nGemini 2.5 models are optimized for complex tasks that need multiple rounds of strategyzing and iteratively solving.\nGemini 2.5 Flash in particular, brings the flexibility of using thinking_budget - a parameter that offers fine-grained control over the maximum number of tokens a model can generate while thinking. Alternatively, you can designate a precise token allowance for the “thinking” stage through the adjusment of the thinking_budget parameter. This allowance can vary between 0 and 24576 tokens for 2.5 Flash.\nFor more information about all Gemini models, check the documentation for extended information on each of them.\n\n\n\n\n\n\nNote\n\n\n\nOn this notebook all examples are using Gemini 2.5 Flash and the new thinking_budget parameter. The thinking_budget parameter is not available for Gemini 2.5 Pro for now - If you want to use Gemini 2.5 Pro, you can remove the thinking_budget parameter from the code.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Use Gemini thinking"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_thinking.html#setup",
    "href": "quickstarts/Get_started_thinking.html#setup",
    "title": "Use Gemini thinking",
    "section": "Setup",
    "text": "Setup\n\nInstall the Google GenAI SDK\nInstall the Google GenAI SDK from npm.\n$ npm install @google/genai\n\n\nSetup your API key\nYou can create your API key using Google AI Studio with a single click.\nRemember to treat your API key like a password. Don’t accidentally save it in a notebook or source file you later commit to GitHub. In this notebook we will be storing the API key in a .env file. You can also set it as an environment variable or use a secret manager.\nHere’s how to set it up in a .env file:\n$ touch .env\n$ echo \"GEMINI_API_KEY=&lt;YOUR_API_KEY&gt;\" &gt;&gt; .env\n\n\n\n\n\n\nTip\n\n\n\nAnother option is to set the API key as an environment variable. You can do this in your terminal with the following command:\n$ export GEMINI_API_KEY=\"&lt;YOUR_API_KEY&gt;\"\n\n\n\n\nLoad the API key\nTo load the API key from the .env file, we will use the dotenv package. This package loads environment variables from a .env file into process.env.\n$ npm install dotenv\nThen, we can load the API key in our code:\n\nconst dotenv = require(\"dotenv\") as typeof import(\"dotenv\");\n\ndotenv.config({\n  path: \"../.env\",\n});\n\nconst GEMINI_API_KEY = process.env.GEMINI_API_KEY ?? \"\";\nif (!GEMINI_API_KEY) {\n  throw new Error(\"GEMINI_API_KEY is not set in the environment variables\");\n}\nconsole.log(\"GEMINI_API_KEY is set in the environment variables\");\n\nGEMINI_API_KEY is set in the environment variables\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn our particular case the .env is is one directory up from the notebook, hence we need to use ../ to go up one directory. If the .env file is in the same directory as the notebook, you can omit it altogether.\n│\n├── .env\n└── quickstarts\n    └── Get_started_thinking.ipynb\n\n\n\n\nInitialize SDK Client\nWith the new SDK, now you only need to initialize a client with you API key (or OAuth if using Vertex AI). The model is now set in each call.\n\nconst google = require(\"@google/genai\") as typeof import(\"@google/genai\");\n\nconst ai = new google.GoogleGenAI({ apiKey: GEMINI_API_KEY });\n\n\n\nSelect a model\n\nconst tslab = require(\"tslab\") as typeof import(\"tslab\");\n\nconst MODEL_ID = \"gemini-2.5-flash-preview-05-20\";",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Use Gemini thinking"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_thinking.html#using-the-thinking-models",
    "href": "quickstarts/Get_started_thinking.html#using-the-thinking-models",
    "title": "Use Gemini thinking",
    "section": "Using the thinking models",
    "text": "Using the thinking models\nHere are some quite complex examples of what Gemini thinking models can solve.\nIn each of them you can select different models to see how this new model compares to its predecesors.\nIn some cases, you’ll still get the good answer from the other models, in that case, re-run it a couple of times and you’ll see that Gemini thinking models are more consistent thanks to their thinking step.\n\n\n\n\n\n\nNote\n\n\n\nFor now, thinking budget is a feature available for Gemini 2.5 Flash model only.\n\n\n\nUsing adaptive thinking\nYou can start by asking the model to explain a concept and see how it does reasoning before answering.\nStarting with the adaptive thinkingBudget - which is the default when you don’t specify a budget - the model will dynamically adjust the budget based on the complexity of the request.\n\nconst adaptive_thinking_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: [\n    `\n    You are playing the 20 question game. You know that what you are looking for\n    is a aquatic mammal that doesn't live in the sea, and that's smaller than a\n    cat. What could that be and how could you make sure?\n    `,\n  ],\n});\ntslab.display.markdown(adaptive_thinking_response.text ?? \"\");\n\nGiven your criteria, here are the most likely candidates for an aquatic mammal that doesn’t live in the sea and is smaller than a cat:\n\nWater Shrew (e.g., Common Water Shrew, American Water Shrew, Eurasian Water Shrew):\n\nAquatic Mammal: Yes, they are true mammals that are highly adapted to aquatic life, often hunting insects and small fish in streams and ponds.\nDoesn’t live in the sea: Correct, they are freshwater dwellers.\nSmaller than a cat: Absolutely, they are tiny, only a few inches long, resembling a mouse with a pointed snout.\n\nDesman (e.g., Pyrenean Desman, Russian Desman):\n\nAquatic Mammal: Yes, they are semi-aquatic insectivorous mammals related to moles, living in freshwater rivers and lakes.\nDoesn’t live in the sea: Correct, exclusively freshwater.\nSmaller than a cat: Yes, their body length is usually around 4-8 inches, not including their tail, making them significantly smaller than a cat.\n\nWater Vole:\n\nAquatic Mammal: Yes, these rodents are semi-aquatic, living along the banks of rivers, streams, and ditches, feeding on vegetation.\nDoesn’t live in the sea: Correct, they are freshwater animals.\nSmaller than a cat: Yes, generally smaller than a cat, though some larger individuals might approach the size of a very small kitten.\n\nPlatypus:\n\nAquatic Mammal: Yes, it’s one of only two monotremes (egg-laying mammals) and is highly aquatic.\nDoesn’t live in the sea: Correct, found in freshwater rivers and lakes in Australia.\nSmaller than a cat: Yes, typically about 12-15 inches long, body-wise, which is smaller than most domestic cats.\n\n\n\n\nHow to make sure (using 20 Questions style):\nTo narrow it down and confirm, you’d ask questions that target the specific characteristics and differences between these potential answers.\nGeneral Confirming Questions:\n\nIs it typically found in freshwater environments like rivers, streams, or ponds? (Confirms “doesn’t live in the sea”)\nIs its primary diet composed of insects or small aquatic invertebrates? (Helps differentiate shrews/desmans/platypus from voles)\nDoes it have webbed feet? (Most candidates have some webbing, but it’s more pronounced in desmans and platypus)\nIs it native to North America, Europe, Asia, or Australia? (Geographic location can help narrow it down significantly)\n\nDifferentiating Questions:\n\nDoes it lay eggs?\n\nIf YES: It’s almost certainly a Platypus. (This is a huge differentiator for mammals)\nIf NO: It’s a placental mammal (shrew, desman, vole).\n\nDoes it have a very long, pointed snout, almost like a trunk?\n\nIf YES: Likely a Desman. (Very distinctive feature)\nIf NO: Could be a shrew or vole.\n\nIs it known for building elaborate burrows along riverbanks?\n\nIf YES: Could be a Water Vole or a Platypus. (Platypus builds extensive burrows; water voles are also strong burrowers).\n\nIs it considered a rodent?\n\nIf YES: It’s a Water Vole. (This would rule out shrews, desmans, and platypus as they are not rodents).\n\nDoes it have extremely small eyes and ears, often hidden in its fur?\n\nIf YES: Consistent with Water Shrews and Desmans, which rely more on touch and smell.\n\n\nBy asking a series of these specific questions, you can logically eliminate options and pinpoint the correct aquatic mammal.\n\n\n\nLooking to the response metadata, you can see not only the amount of tokens on your input and the amount of tokens used for the response, but also the amount of tokens used for the thinking step - As you can see here, the model used around 1500 tokens in the thinking steps:\n\nconsole.log(JSON.stringify(adaptive_thinking_response.usageMetadata, null, 2));\n\n{\n  \"promptTokenCount\": 60,\n  \"candidatesTokenCount\": 939,\n  \"totalTokenCount\": 2494,\n  \"promptTokensDetails\": [\n    {\n      \"modality\": \"TEXT\",\n      \"tokenCount\": 60\n    }\n  ],\n  \"thoughtsTokenCount\": 1495\n}\n\n\n\n\nDisabling the thinking steps\nYou can also disable the thinking steps by setting the thinkingBudget to 0. You’ll see that in this case, the model doesn’t think of the platipus as a possible answer.\n\nconst adaptive_thinking_response_2 = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: [\n    `\n      You are playing the 20 question game. You know that what you are looking for\n      is a aquatic mammal that doesn't live in the sea, and that's smaller than a\n      cat. What could that be and how could you make sure?\n      `,\n  ],\n  config: {\n    thinkingConfig: {\n      thinkingBudget: 0,\n    },\n  },\n});\ntslab.display.markdown(adaptive_thinking_response_2.text ?? \"\");\n\nThis is a fun one! Let’s break it down.\nThe Answer:\nBased on your criteria, the most likely aquatic mammal that doesn’t live in the sea and is smaller than a cat is a Pygmy Shrew (genus Sorex or Suncus) – specifically, some species are semi-aquatic.\nWhy?\n\nAquatic Mammal: While many people think of shrews as terrestrial, several species are highly adapted to a semi-aquatic lifestyle, often found near streams, ponds, and wetlands. They are excellent swimmers and divers.\nDoesn’t Live in the Sea: They are exclusively freshwater or terrestrial/wetland creatures.\nSmaller Than a Cat: Absolutely! Pygmy shrews are among the smallest mammals in the world, often weighing only a few grams and measuring just a few centimeters long, excluding the tail. They are significantly smaller than a cat.\n\nHow could you make sure (in a 20 questions game context)?\nHere’s how you could use your 20 questions to narrow it down and confirm, leading to Pygmy Shrew:\nInitial Questions to Confirm Categories:\n\n“Is it a mammal?” (You already know this, but it’s good to confirm the broader category for the game.)\n“Does it live in water for a significant part of its life?” (Confirming “aquatic.”)\n“Is its primary habitat freshwater?” (Confirming “doesn’t live in the sea.”)\n“Is it smaller than a typical house cat?” (Confirming “smaller than a cat.”)\n\nNarrowing Down Questions (if the opponent keeps saying “yes”):\n\n“Does it primarily eat fish or other vertebrates?” (No - shrews primarily eat invertebrates)\n“Does it have webbed feet?” (Some semi-aquatic shrews do have fringe-like hairs that aid in swimming, but not true webbing like a duck or otter.)\n“Does it lay eggs?” (No - mammal)\n“Is it a rodent?” (No - shrews are in the order Eulipotyphla, not Rodentia)\n“Does it have long, prominent whiskers?” (Yes - shrews have very sensitive whiskers)\n“Is it blind or have very poor eyesight?” (Yes - shrews rely more on hearing and smell)\n“Does it have a very high metabolism and need to eat constantly?” (Yes - a key characteristic of shrews)\n“Does it typically live underground or in dense vegetation near water?” (Yes)\n“Is it known for being extremely small, possibly one of the smallest mammals?” (Yes!)\n\nCloser to the Reveal:\n\n“Is it often mistaken for a mouse?” (Yes, due to size and general appearance, but they are very different animals)\n“Does it have a long, pointed snout?” (Yes! A defining feature of shrews.)\n\nConfirmation Question:\n\n“Is it a type of shrew?” (Hopefully “Yes!”)\n“More specifically, is it a Pygmy Shrew or a similar very small semi-aquatic shrew?” (This is where you’d confirm your hypothesis.)\n\nBy asking these types of questions, you’d eliminate other possibilities (like very small rodents that aren’t aquatic, or larger aquatic mammals) and hone in on the unique characteristics of a Pygmy Shrew.\n\n\nNow you can see that the response is faster as the model didn’t perform any thinking step. Also you can see that no tokens were used for the thinking step:\n\nconsole.log(JSON.stringify(adaptive_thinking_response_2.usageMetadata, null, 2));\n\n{\n  \"promptTokenCount\": 60,\n  \"candidatesTokenCount\": 805,\n  \"totalTokenCount\": 865,\n  \"promptTokensDetails\": [\n    {\n      \"modality\": \"TEXT\",\n      \"tokenCount\": 60\n    }\n  ]\n}\n\n\n\n\nSolving a physics problem\nNow, try with a simple physics comprehension example. First you can disable the thinkingBudget to see how the model performs:\n\nconst physics_problem_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: [\n    `\n    A cantilever beam of length L=3m has a rectangular cross-section (width b=0.1m, height h=0.2m) and is made of steel (E=200 GPa).\n    It is subjected to a uniformly distributed load w=5 kN/m along its entire length and a point load P=10 kN at its free end.\n    Calculate the maximum bending stress (σ_max).\n    `,\n  ],\n  config: {\n    thinkingConfig: {\n      thinkingBudget: 0,\n    },\n  },\n});\ntslab.display.markdown(physics_problem_response.text ?? \"\");\n\nHere’s a step-by-step calculation of the maximum bending stress in the cantilever beam:\n1. Define Given Parameters:\n\nLength of beam (L) = 3 m\nWidth of cross-section (b) = 0.1 m\nHeight of cross-section (h) = 0.2 m\nModulus of Elasticity (E) = 200 GPa = 200 x 10^9 N/m^2 (Note: E is not directly used for stress calculation but is usually given in beam problems)\nUniformly distributed load (w) = 5 kN/m = 5000 N/m\nPoint load (P) = 10 kN = 10000 N\n\n2. Calculate the Area Moment of Inertia (I):\nFor a rectangular cross-section, the area moment of inertia about the neutral axis is: I = (b * h^3) / 12\nI = (0.1 m * (0.2 m)^3) / 12 I = (0.1 m * 0.008 m^3) / 12 I = 0.0008 m^4 / 12 I = 6.6667 x 10^-5 m^4\n3. Determine the Maximum Bending Moment (M_max):\nFor a cantilever beam, the maximum bending moment occurs at the fixed support. It’s the sum of the moments due to the distributed load and the point load.\n\nMoment due to uniformly distributed load (M_w): M_w = w * L^2 / 2 M_w = (5000 N/m) * (3 m)^2 / 2 M_w = 5000 N/m * 9 m^2 / 2 M_w = 45000 Nm / 2 M_w = 22500 Nm\nMoment due to point load (M_P): M_P = P * L M_P = (10000 N) * (3 m) M_P = 30000 Nm\nTotal Maximum Bending Moment (M_max): M_max = M_w + M_P M_max = 22500 Nm + 30000 Nm M_max = 52500 Nm\n\n4. Determine the Distance to the Extreme Fiber (y_max):\nFor a rectangular cross-section, the neutral axis is at the geometric center. The maximum stress occurs at the top or bottom surface. y_max = h / 2\ny_max = 0.2 m / 2 y_max = 0.1 m\n5. Calculate the Maximum Bending Stress (σ_max):\nThe bending stress formula is: σ_max = (M_max * y_max) / I\nσ_max = (52500 Nm * 0.1 m) / (6.6667 x 10^-5 m^4) σ_max = 5250 Nm^2 / (6.6667 x 10^-5 m^4) σ_max = 78749439.06 N/m^2\nConvert to Pascals (Pa) or MegaPascals (MPa): σ_max = 78,749,439.06 Pa σ_max ≈ 78.75 x 10^6 Pa σ_max ≈ 78.75 MPa\nConclusion:\nThe maximum bending stress in the cantilever beam is approximately 78.75 MPa.\n\n\nYou can see that the model used no tokens for the thinking step:\n\nconsole.log(JSON.stringify(physics_problem_response.usageMetadata, null, 2));\n\n{\n  \"promptTokenCount\": 97,\n  \"candidatesTokenCount\": 871,\n  \"totalTokenCount\": 968,\n  \"promptTokensDetails\": [\n    {\n      \"modality\": \"TEXT\",\n      \"tokenCount\": 97\n    }\n  ]\n}\n\n\nThen you can set a fixed maximum budget (thinkingBudget=4096, or 4096 tokens) for the thinking step to see how the model performs.\nYou can see that, even producing a similar result for the same prompt, the amount of details shared in the answer makes it deeper and more consistent.\n\nconst physics_problem_response_2 = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: [\n    `\n      A cantilever beam of length L=3m has a rectangular cross-section (width b=0.1m, height h=0.2m) and is made of steel (E=200 GPa).\n      It is subjected to a uniformly distributed load w=5 kN/m along its entire length and a point load P=10 kN at its free end.\n      Calculate the maximum bending stress (σ_max).\n      `,\n  ],\n  config: {\n    thinkingConfig: {\n      thinkingBudget: 4096,\n    },\n  },\n});\ntslab.display.markdown(physics_problem_response_2.text ?? \"\");\n\nTo calculate the maximum bending stress (σ_max) in the cantilever beam, we need to follow these steps:\n\nCalculate the moment of inertia (I) of the cross-section.\nDetermine the maximum bending moment (M_max) acting on the beam.\nCalculate the maximum distance (y_max) from the neutral axis to the extreme fiber.\nApply the bending stress formula: σ_max = (M_max * y_max) / I.\n\nLet’s break down each step:\nGiven Data: * Length L = 3 m * Width b = 0.1 m * Height h = 0.2 m * Young’s Modulus E = 200 GPa = 200 × 10^9 Pa (or N/m²) - Note: E is not needed for stress calculation directly, only for deflection. * Uniformly distributed load w = 5 kN/m = 5 × 10^3 N/m * Point load P = 10 kN = 10 × 10^3 N\n\n1. Calculate the Moment of Inertia (I) for a rectangular cross-section: The formula for the moment of inertia of a rectangle about its centroidal axis is: I = (b * h^3) / 12\nI = (0.1 m * (0.2 m)^3) / 12 I = (0.1 * 0.008) / 12 I = 0.0008 / 12 I = 6.6667 × 10^-5 m^4\n\n2. Determine the Maximum Bending Moment (M_max): For a cantilever beam, the maximum bending moment occurs at the fixed support (where it connects to the wall). We need to sum the moments caused by each load at this point.\n\nMoment due to the point load P at the free end (M_P): M_P = P * L M_P = (10 × 10^3 N) * (3 m) M_P = 30 × 10^3 N·m = 30 kN·m\nMoment due to the uniformly distributed load w along the entire length (M_w): This can be thought of as a concentrated load (w * L) acting at the centroid of the distributed load, which is L/2 from the fixed support. M_w = (w * L) * (L/2) M_w = w * L^2 / 2 M_w = (5 × 10^3 N/m) * (3 m)^2 / 2 M_w = (5 × 10^3 * 9) / 2 M_w = 45 × 10^3 / 2 M_w = 22.5 × 10^3 N·m = 22.5 kN·m\nTotal Maximum Bending Moment (M_max): M_max = M_P + M_w M_max = 30 × 10^3 N·m + 22.5 × 10^3 N·m M_max = 52.5 × 10^3 N·m = 52.5 kN·m\n\n\n3. Calculate the Maximum Distance (y_max) from the neutral axis: For a rectangular cross-section, the neutral axis is at the geometric center, and the maximum distance to the extreme fibers (top or bottom surface) is half the height. y_max = h / 2 y_max = 0.2 m / 2 y_max = 0.1 m\n\n4. Apply the Bending Stress Formula (σ_max): The maximum bending stress occurs at the location of M_max (fixed support) and at y_max (top/bottom surface). σ_max = (M_max * y_max) / I\nσ_max = ( (52.5 × 10^3 N·m) * (0.1 m) ) / (6.6667 × 10^-5 m^4) σ_max = (5.25 × 10^3 N·m²) / (6.6667 × 10^-5 m^4) σ_max = 78,750,000 N/m²\nConvert to Megapascals (MPa), where 1 MPa = 10^6 Pa: σ_max = 78.75 × 10^6 Pa σ_max = 78.75 MPa\n\nFinal Answer: The maximum bending stress (σ_max) in the cantilever beam is 78.75 MPa.\n\n\nNow you can see that the model used around 1300 tokens for the thinking step (not necessarily using the full budget you set):\n\nconsole.log(JSON.stringify(physics_problem_response_2.usageMetadata, null, 2));\n\n{\n  \"promptTokenCount\": 97,\n  \"candidatesTokenCount\": 1125,\n  \"totalTokenCount\": 2564,\n  \"promptTokensDetails\": [\n    {\n      \"modality\": \"TEXT\",\n      \"tokenCount\": 97\n    }\n  ],\n  \"thoughtsTokenCount\": 1342\n}\n\n\n\n\nWorking with multimodal problems\nThis geometry problem requires complex reasoning and is also using Gemini multimodal abilities to read the image. In this case, you are fixing a value to the thinkingBudget so the model will use up to 8196 tokens for the thinking step.\n\nconst fs = require(\"fs\") as typeof import(\"fs\");\nconst path = require(\"path\") as typeof import(\"path\");\n\nconst IMG_URL = \"https://storage.googleapis.com/generativeai-downloads/images/geometry.png\";\n\nconst downloadFile = async (url: string, filePath: string) =&gt; {\n  if (!fs.existsSync(path.dirname(filePath))) {\n    fs.mkdirSync(path.dirname(filePath), { recursive: true });\n  }\n  const response = await fetch(url);\n  if (!response.ok) {\n    throw new Error(`Failed to download image: ${response.statusText}`);\n  }\n  const buffer = await response.blob();\n  const bufferData = Buffer.from(await buffer.arrayBuffer());\n  fs.writeFileSync(filePath, bufferData);\n};\n\nconst geometry_img_path = path.join(\"../assets/thinking\", \"geometry.png\");\nawait downloadFile(IMG_URL, geometry_img_path);\ntslab.display.png(fs.readFileSync(geometry_img_path));\n\n\n\n\n\n\n\n\n\nconst image_file = await ai.files.upload({\n  file: geometry_img_path,\n  config: {\n    mimeType: \"image/png\",\n    displayName: \"geometry.png\",\n  },\n});\n\nconst geometry_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: [\n    google.createPartFromUri(image_file.uri ?? \"\", image_file.mimeType ?? \"image/png\"),\n    \"What's the area of the overlapping region?\",\n  ],\n  config: {\n    thinkingConfig: {\n      thinkingBudget: 8192,\n    },\n  },\n});\ntslab.display.markdown(geometry_response.text ?? \"\");\n\nLet’s break down the problem by identifying the shapes and their dimensions from the given image.\n\nThe Circle:\n\nThe numbers ‘3’ within the blue circle clearly indicate that the radius of the circle, r, is 3.\nThe green lines originating from the center of the circle (which is also the right-angle vertex of the triangle) extend along the axes. These lines form a right angle (90 degrees), indicating that the portion of the circle relevant to the overlap is a quarter circle.\nThe area of a full circle is πr². The area of this quarter circle is (1/4)πr² = (1/4)π(3)² = 9π/4.\n\nThe Triangle:\n\nIt’s a right-angled triangle, with its right angle coinciding with the center of the circle. Let’s place this vertex at the origin (0,0) of a coordinate system.\nThe vertical leg has a ‘3’ next to it, indicating its length is 3. So, this leg extends from (0,0) to (0,3).\nThe horizontal leg has two ’3’s next to it. This means its total length is 3 + 3 = 6. So, this leg extends from (0,0) to (6,0).\nThe vertices of the triangle are (0,0), (6,0), and (0,3).\nThe hypotenuse connects (0,3) and (6,0). The equation of the line passing through these points can be found: Slope m = (0 - 3) / (6 - 0) = -3/6 = -1/2. Using the point-slope form y - y1 = m(x - x1) with (6,0): y - 0 = (-1/2)(x - 6), so y = -1/2 x + 3.\n\nThe Overlapping Region:\n\nThe green-shaded area represents the overlapping region between the circle and the triangle.\nThis region is bounded by the x-axis, the y-axis, the hypotenuse of the triangle, and the arc of the circle.\nWe need to determine which boundary is “inner” in the first quadrant.\nThe hypotenuse y = -1/2 x + 3 intersects the circle x² + y² = 3² at two points. Let’s find them: Substitute y from the line equation into the circle equation: x² + (-1/2 x + 3)² = 9 x² + (1/4 x² - 3x + 9) = 9 5/4 x² - 3x = 0 Factor out x: x(5/4 x - 3) = 0 This gives two solutions for x:\n\nx = 0: If x=0, then y = -1/2(0) + 3 = 3. This is the point (0,3). This point is a vertex of the triangle and also on the circle (0² + 3² = 9).\n5/4 x - 3 = 0: 5/4 x = 3 =&gt; x = 12/5 = 2.4. If x=2.4, then y = -1/2(2.4) + 3 = -1.2 + 3 = 1.8. This is the point (2.4, 1.8). Let’s call this point P = (2.4, 1.8). We check if it’s on the circle: 2.4² + 1.8² = 5.76 + 3.24 = 9. Yes, it is.\n\nNow we understand the boundaries of the overlapping region:\n\nIt starts at the origin (0,0).\nIt goes up along the y-axis to (0,3).\nThen it follows the hypotenuse (line segment from (0,3) to (2.4, 1.8)).\nFrom (2.4, 1.8), it follows the arc of the circle to (3,0) (the point where the circle intersects the positive x-axis).\nFinally, it goes along the x-axis back to the origin (0,0).\n\nWe can calculate this area by splitting it into two parts:\n\nA polygon (a trapezoid or a triangle + rectangle)\nA circular sector.\n\nMethod 1: Decomposition Let O = (0,0), A = (0,3), P = (2.4, 1.8), B = (3,0). The overlapping region is the area of quadrilateral OAPB. We can split it into:\n\nTriangle OAP’: A triangle with vertices O=(0,0), A=(0,3), and P=(2.4, 1.8). The area of a triangle given coordinates (0,0), (x1, y1), (x2, y2) is 1/2 |x1*y2 - x2*y1|. This is simpler. Using the base along y-axis, (0,3) as vertex A, and (0,0) as vertex O, and the x-coordinate of P as height: Area(OAP) = (1/2) * base * height = (1/2) * OA * (x-coordinate of P) = (1/2) * 3 * 2.4 = 3.6.\nCircular Sector OPB: A sector of the circle with radius 3, from the x-axis (angle 0) to the angle θ corresponding to point P=(2.4, 1.8). We find cos(θ) = x/r = 2.4/3 = 4/5 and sin(θ) = y/r = 1.8/3 = 3/5. So, θ = arcsin(3/5). The angle for point B=(3,0) is 0 radians. The angle for point P=(2.4,1.8) is θ = arcsin(3/5). Area of sector OPB = (1/2)r² * (angle in radians) = (1/2)(3)² * arcsin(3/5) = (9/2)arcsin(3/5).\nTotal Area = Area(OAP) + Area(Sector OPB) Total Area = 3.6 + (9/2)arcsin(3/5).\n\nMethod 2: Integration The area can be expressed as the sum of two integrals: Area = ∫[from 0 to 2.4] (-1/2 x + 3) dx + ∫[from 2.4 to 3] sqrt(9 - x²) dx The first integral calculates the area under the hypotenuse from x=0 to x=2.4. This is a trapezoid with vertices (0,0), (2.4,0), (2.4,1.8), (0,3). Its area is (1/2) * (3 + 1.8) * 2.4 = (1/2) * 4.8 * 2.4 = 5.76. The second integral calculates the area under the circle’s arc from x=2.4 to x=3. This is [ (x/2)sqrt(9 - x²) + (9/2)arcsin(x/3) ] from 2.4 to 3. Evaluating at x=3: (3/2)sqrt(0) + (9/2)arcsin(1) = 9π/4. Evaluating at x=2.4: (2.4/2)sqrt(9 - 2.4²) + (9/2)arcsin(2.4/3) = 1.2 * sqrt(9 - 5.76) + (9/2)arcsin(0.8) = 1.2 * sqrt(3.24) + (9/2)arcsin(4/5) = 1.2 * 1.8 + (9/2)arcsin(4/5) = 2.16 + (9/2)arcsin(4/5). So the second integral’s value is 9π/4 - (2.16 + (9/2)arcsin(4/5)). Total Area = 5.76 + 9π/4 - 2.16 - (9/2)arcsin(4/5) Total Area = 3.6 + 9π/4 - (9/2)arcsin(4/5).\nWe know that arcsin(4/5) = π/2 - arcsin(3/5). (Since sin(θ) = 3/5 means cos(θ) = 4/5, so θ is arcsin(3/5) and π/2 - θ is arccos(3/5) or arcsin(4/5)). Substitute this into the expression: Total Area = 3.6 + 9π/4 - (9/2)(π/2 - arcsin(3/5)) = 3.6 + 9π/4 - 9π/4 + (9/2)arcsin(3/5) = 3.6 + (9/2)arcsin(3/5). Both methods yield the same result.\n\n\nThe area of the overlapping region is 3.6 + (9/2)arcsin(3/5).\nThe final answer is \\(\\boxed{3.6 + \\frac{9}{2}\\arcsin\\left(\\frac{3}{5}\\right)}\\)\n\n\n\n\nSolving brain teasers\nHere’s another brain teaser based on an image, this time it looks like a mathematical problem, but it cannot actually be solved mathematically. If you check the toughts of the model you’ll see that it will realize it and come up with an out-of-the-box solution.\nIn this case, you are fixing a value to the thinkingBudget so the model will use up to 24576 tokens for the thinking step\n\nconst POOL_IMG_URL = \"https://storage.googleapis.com/generativeai-downloads/images/pool.png\";\n\nconst pool_img_path = path.join(\"../assets/thinking\", \"pool.png\");\nawait downloadFile(POOL_IMG_URL, pool_img_path);\ntslab.display.png(fs.readFileSync(pool_img_path));\n\n\n\n\n\n\n\n\nFirst you can check how the model performs without reasoning (thinkingBudget=0):\n\nconst pool_image_file = await ai.files.upload({\n  file: pool_img_path,\n  config: {\n    mimeType: \"image/png\",\n    displayName: \"pool.png\",\n  },\n});\nconst pool_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: [\n    google.createPartFromUri(pool_image_file.uri ?? \"\", pool_image_file.mimeType ?? \"image/png\"),\n    \"How do I use those three pool balls to sum up to 30?\",\n  ],\n  config: {\n    thinkingConfig: {\n      thinkingBudget: 0,\n    },\n  },\n});\ntslab.display.markdown(pool_response.text ?? \"\");\n\nThis is a classic riddle! Here’s how you can do it:\nYou can sum up to 30 using three of the pool balls by flipping the number “9” upside down to make it a “6”.\nThen you have:\n\n7\n6 (from the flipped 9)\n13\n\n7 + 6 + 13 = 26\nAh, I re-read your request and realized you need to sum to 30, not 26. My apologies. Let’s try again with the balls available: 7, 9, 11, 13.\nThe numbers on the balls are 7, 9, 11, and 13. All of these are odd numbers.\nThe sum of any three odd numbers will always be an odd number.\nSince 30 is an even number, it’s impossible to sum to 30 using only three of these particular pool balls (7, 9, 11, 13) in their given numerical values.\nTherefore, the only way to solve this riddle is usually by flipping the 9 to make it a 6. However, that would only get you to 26, as I demonstrated above.\nIf the rule is that you must use only the numbers as they appear on the balls (7, 9, 11, 13) and can’t flip any numbers, then it’s a trick question and it’s impossible.\n\n\nAs you can notice, the model struggled to find a way to get to the result - and ended up suggesting to use different pool balls.\nNow you can use the model reasoning to solve the riddle:\n\nconst pool_response_2 = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: [\n    google.createPartFromUri(pool_image_file.uri ?? \"\", pool_image_file.mimeType ?? \"image/png\"),\n    \"How do I use those three pool balls to sum up to 30?\",\n  ],\n  config: {\n    thinkingConfig: {\n      thinkingBudget: 24576,\n    },\n  },\n});\ntslab.display.markdown(pool_response_2.text ?? \"\");\n\nThis is a classic riddle!\nYou need to use the trick of flipping the 9 ball upside down to make it a 6.\nThen, you can use these three balls:\n\n6 (the flipped 9 ball)\n11\n13\n\n6 + 11 + 13 = 30\n\n\n\n\nSolving a math puzzle with the maximum thinkingBudget\nThis is typically a case where you want to fix a budget, as the model can spend a lot of time thinking in all directions before finding the right answer. It should not be too low either as non-thinking models have trouble with such questions.\nPlay with the thinking budget and try to find how much it needs to be able to find the right answer most of the time.\n\nconst math_problem_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: [\n    `\n    How can you obtain 565 with 10 8 3 7 1 and 5 and the common operations?\n    You can only use a number once.\n    `,\n  ],\n  config: {\n    thinkingConfig: {\n      thinkingBudget: 24576,\n    },\n  },\n});\ntslab.display.markdown(math_problem_response.text ?? \"\");\n\nHere’s how you can obtain 565 using the given numbers and common operations:\n\nMultiply 8 by 7: 8 * 7 = 56\nMultiply the result by 10: 56 * 10 = 560\nAdd 5 to the result: 560 + 5 = 565\n\nSo, the full expression is: (8 * 7 * 10) + 5 = 565\nNumbers used: 8, 7, 10, 5 (each used only once). Numbers not used: 3, 1 (which is allowed).\n\n\n\n\nWorking thoughts summaries\nSummaries of the model’s thinking reveal its internal problem-solving pathway. Users can leverage this feature to check the model’s strategy and remain informed during complex tasks.\nFor more details about Gemini 2.5 thinking capabilities, take a look at the Gemini models thinking guide.\n\nconst thought_experiment_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: [\n    `\n    Alice, Bob, and Carol each live in a different house on the same street: red, green, and blue.\n    The person who lives in the red house owns a cat.\n    Bob does not live in the green house.\n    Carol owns a dog.\n    The green house is to the left of the red house.\n    Alice does not own a cat.\n    Who lives in each house, and what pet do they own?\n    `,\n  ],\n  config: {\n    thinkingConfig: {\n      includeThoughts: true,\n    },\n  },\n});\n\nYou can check both the thought summaries and the final model response:\n\nfor (const part of thought_experiment_response.candidates?.[0]?.content?.parts ?? []) {\n  if (!part.text) continue;\n  if (part.thought) {\n    tslab.display.markdown(`## **Thought Summary**`);\n    tslab.display.markdown(part.text ?? \"\");\n  } else {\n    tslab.display.markdown(`## **Response**`);\n    tslab.display.markdown(part.text ?? \"\");\n  }\n}\n\nThought Summary\n\n\nInitial Thoughts and Problem Decomposition\nAlright, let’s break this down. The goal is to match people to houses and pets. I’m thinking a table or matrix is the best way to keep track of this. I need to list all the entities: people (Alice, Bob, Carol), houses (red, green, blue), and pets (cat, dog, and implicitly a third, or maybe not). I’ll go through the clues one by one and mark my deductions. This is all about finding relationships and cross-references.\nProcessing the Clues and Building Deductions\nFirst clue: Red house = Cat. Easy. Then, Bob doesn’t live in the green house. Noted. Carol owns a dog. Alright, Carol = Dog. The green house is to the left of the red house. Got it, Green &lt; Red. Alice doesn’t own a cat. Now things get interesting!\nI’m starting to see some links here. If the red house has a cat and Alice doesn’t, Alice can’t be in the red house. And since Carol has a dog and the red house has the cat, Carol can’t be in the red house either. That leaves Bob! Bob must be in the Red House with the Cat. Major breakthrough!\nFilling the Gaps and Combining Information\nNow, I know Bob is in the red house. Bob does not live in the green house, that’s already true. I know Carol owns a dog. The houses are arranged Green &lt; Red &lt; Blue. So Green is to the left of Bob. That’s clear.\nSo, it’s either Green, Red, Blue OR Blue, Green, Red. Because Bob is in the Red House, it becomes Green, Bob (Red), Blue. Carol owns the dog, Alice is left. I’m at the point now where I’m trying to decide the houses for the dog and Alice!\nCarol must have the dog in Green or Blue house. No direct information!\nConfronting the Ambiguity and Re-Evaluating\nHmm, this is tricky. No clue is helping me directly match either Carol to Green, or Carol to Blue. I need to rethink the pets. Is there an unstated assumption here? It seems to imply that everyone has a named pet but I’m not seeing how to pair them. I’ve re-read the puzzle and clues many times.\nIf Carol owns the dog, and Bob owns the cat, then what about Alice? No specific pet is associated with Alice. This implies Alice has the unnamed pet or none. My working theory is that there’s an ambiguity in the pet assignments.\nThe crucial constraint is that the green house is to the left of the red house. In three possible arrangements: Green - Red - Blue, Blue - Green - Red, and Green - Blue - Red. I know Bob lives in the Red House, with the cat, that means it’s Green - Bob (Red) - Blue. Then I need to decide, who lives in which house?\nCarol owns the Dog and must be in either the Green or Blue house! Alice is the remaining house. There’s nothing that helps me, at all, decide which person is in what color house, for green and blue!\nAcknowledging the Underspecification\nThere seems to be an ambiguity here. This is a common situation with these puzzles! There might be an unstated assumption that everyone must have a pet in this context. However, I’m stuck with two possible solutions for Alice and Carol, and I don’t see any logic to eliminate either of these solutions.\nFinalized Solution & Acknowledgment of Limitations\nSo, the definite part is: Bob lives in the Red House and owns the Cat. The green house is to the left of Bob’s red house.\nThere are two possibilities:\n\nCarol (Dog) is in the Green House, Alice owns the “unspecified” pet, and lives in Blue house\nCarol (Dog) is in the Blue House, Alice owns the “unspecified” pet, and lives in Green house\n\nI can’t pinpoint a specific pet for Alice. Unless there’s an extremely subtle detail, these are the only two possible consistent solutions.\n\n\nResponse\n\n\nLet’s break this down clue by clue to solve the puzzle:\n\n“The person who lives in the red house owns a cat.”\n\nDeduction: Red House = Cat Owner.\n\n“Carol owns a dog.”\n\nDeduction: Carol = Dog.\n\n“Alice does not own a cat.”\n\nDeduction: Alice is not the Cat Owner.\n\nCombine 1, 2, and 3:\n\nThe Red House owner has a cat.\nAlice does not own a cat, so Alice doesn’t live in the Red House.\nCarol owns a dog, so Carol doesn’t live in the Red House (because the Red House has a cat).\nThis leaves only one person for the Red House: Bob.\nDeduction: Bob lives in the Red House and owns a Cat.\n\n“Bob does not live in the green house.”\n\nThis is consistent with our deduction that Bob lives in the Red House. No new information for Bob, but confirms our path.\n\n“The green house is to the left of the red house.”\n\nWe have three houses: Red, Green, Blue.\nSince Green is to the left of Red, and Bob lives in the Red house, the houses must be in this order from left to right: Green, Red, Blue.\nSo:\n\nLeft house: Green\nMiddle house: Red (Bob, Cat)\nRight house: Blue\n\n\nFinal Assignments for Alice and Carol:\n\nWe know Bob lives in the Red House and owns a cat.\nThe remaining houses are Green and Blue.\nThe remaining people are Alice and Carol.\nWe know Carol owns a dog.\nAlice does not own a cat (which is already true, as Bob owns the cat).\n\nAt this point, the puzzle has two possible solutions for Alice and Carol’s locations and pets, as there’s no further information to distinguish between them:\nPossibility 1:\n\nGreen House: Carol, who owns a dog.\nRed House: Bob, who owns a cat.\nBlue House: Alice, whose pet is not specified by the clues (or she owns no pet explicitly mentioned).\n\nPossibility 2:\n\nGreen House: Alice, whose pet is not specified by the clues (or she owns no pet explicitly mentioned).\nRed House: Bob, who owns a cat.\nBlue House: Carol, who owns a dog.\n\n\nSummary of the solution:\n\nBob: Lives in the Red House and owns a Cat.\n\nAnd then, there are two possibilities for Alice and Carol:\n\nScenario A:\n\nCarol: Lives in the Green House and owns a Dog.\nAlice: Lives in the Blue House and her pet is not specified (or she owns no pet explicitly mentioned by the clues).\n\nScenario B:\n\nAlice: Lives in the Green House and her pet is not specified (or she owns no pet explicitly mentioned by the clues).\nCarol: Lives in the Blue House and owns a Dog.\n\n\n\n\nYou can also use see the thought summaries in streaming experiences:\n\nconst thought_experiment_streaming_response = await ai.models.generateContentStream({\n  model: MODEL_ID,\n  contents: [\n    `\n    Alice, Bob, and Carol each live in a different house on the same street: red, green, and blue.\n    The person who lives in the red house owns a cat.\n    Bob does not live in the green house.\n    Carol owns a dog.\n    The green house is to the left of the red house.\n    Alice does not own a cat.\n    Who lives in each house, and what pet do they own?\n    `,\n  ],\n  config: {\n    thinkingConfig: {\n      includeThoughts: true,\n    },\n  },\n});\n\nlet thoughts = \"\";\nlet answers = \"\";\n\nfor await (const chunk of thought_experiment_streaming_response) {\n  for (const part of chunk.candidates?.[0]?.content?.parts ?? []) {\n    if (!part.text) continue;\n    if (part.thought) {\n      if (!thoughts) tslab.display.markdown(`## **Thought Summary**`);\n      tslab.display.markdown(part.text ?? \"\");\n      thoughts += part.text ?? \"\";\n    } else {\n      if (!answers) tslab.display.markdown(`## **Response**`);\n      tslab.display.markdown(part.text ?? \"\");\n      answers += part.text ?? \"\";\n    }\n  }\n}\n\nThought Summary\n\n\nUnpacking the Clues\nI’ve finally started unpacking the core clues of this riddle. Initially, I felt overwhelmed, but now I’m making progress, mapping the houses and pets. I am focusing on eliminating possibilities based on the given constraints. I’m building a matrix to track the relationships, and I think I’m getting close to a breakthrough.\n\n\nDeciphering Relationships\nI’m now diving deeper into the relationships, having established my basic grid. The initial structure feels solid, but the spatial aspect is intriguing. I’m focusing on parsing the first clue about the colors and people, attempting to lay out all permutations and how they impact each other. Right now I am focusing on eliminating contradictory arrangements.\n\n\nConnecting the Dots\nI’ve been meticulously placing clues into the grid, leveraging the established structure. Carol’s dog, and the red house’s cat, are forming key anchors. The spatial clue (“Green to the left of Red”) is proving particularly useful in limiting the possibilities. I’m focusing on the constraints imposed by Bob and Alice to further refine the arrangement.\n\n\nConstructing the Solution\nThe connections are solidifying. I’m now leveraging the logical deductions to finalize the grid, focusing on the house colors. With Bob in Red, and the positional clue about Green, it is starting to become clearer. Determining Alice’s house is key. I’m getting close to the definitive answer.\n\n\nUnveiling the Arrangement\nI’ve finally pinpointed that Bob lives in the red house and owns the cat. Leveraging the spatial clue, I’m now certain Green is to the left of Red. I am determining who lives in the Green and Blue houses and with what pet. I am now working through the final constraint with a clearer picture of the arrangement.\n\n\nSolidifying the Grid\nI’m making progress, but still need to determine who owns the third pet, and whose house is blue. The structure feels much more concrete. Carol owns the dog, and Alice does not own the cat. I am refining the arrangement and building a solution that reflects all the constraints.\n\n\nMapping the Solution\nThe riddle’s framework is solidifying. I’ve placed Bob in the red house with the cat and Carol owning the dog. The positional information about the green house being left of the red is proving key. I’m focusing on Alice’s location and what pet she must have. It’s becoming clearer.\n\n\nSolving the Riddle\nI’ve been working through the clues, using both spatial and direct-relationship constraints to build the solution. Determining Alice and Carol’s houses and pets is the current focus, after correctly placing Bob in the Red house. I am now working through the final house and pet associations.\n\n\nMapping the Final Associations\nI’m zeroing in on the final pieces. Carol and Alice have their houses, the positional clue is helpful. Bob is confirmed in the Red house. The pet situation feels resolved. My primary focus is establishing the definitive house and pet assignments. I’m now certain of the solution.\n\n\nSolving the Puzzle\nI’ve been meticulously following the clues. After placing Bob in the red house with the cat and Carol with the dog, I realized that the “green to the left of red” clue solidifies the positional layout. Now, I’m focusing on Alice and the unknown pet. I’m getting closer to a complete solution.\n\n\nSolving the Puzzle\nI’m now integrating the spatial clue with the pet ownership information, focusing on the house colors. I’ve placed Bob in the red house with the cat, confirming the first part of the riddle. Determining Alice’s association with a house and pet will lead to the final answer. I am making rapid progress now.\n\n\nRevising the Structure\nAfter evaluating multiple permutations, I’ve confirmed Bob lives in the red house with the cat. Carol and Alice occupy the green and blue houses respectively. I’m building out the final assignments for each person and pet.\n\n\nAnalyzing the Structure\nAfter assessing multiple structures, I’ve confirmed Bob lives in the red house with the cat, leveraging all available clues. Carol resides in the green house and owns the dog. I am now certain Alice resides in the blue house.\n\n\nAnalyzing the Solution\nThe riddle’s central structure is now solid. Bob lives in the red house, and Carol resides in the green house with her dog, while Alice is confirmed to be in the blue house. The association with the unknown pet is no longer an issue, because the prompt never mentioned a specific pet, only that she did not own a cat. The solution’s finality is clear.\n\n\nAnalyzing the Solution Space\nI’m now zeroing in on a critical point. I’ve deduced Bob lives in the red house with the cat, a solid foundation. The challenge is assigning Carol and Alice’s homes and pets. Currently, their assignments are interchangeable, and the current clues don’t break the tie. I’ll re-evaluate the spatial constraints for clarity.\n\n\nAnalyzing the Structure\nIt’s clear that Bob lives in the red house with the cat, a solid foundation. While Carol owns the dog, I’m still weighing the possibilities for Alice’s home and pet situation. The green house to the left of the red house gives an additional hint. My focus is finalizing Carol and Alice’s houses.\n\n\nAnalyzing the Structure\nIt’s clear Bob lives in the red house with the cat and Carol has the dog. I’ve placed the Green house to the left of Red. The final piece is linking Alice with Green or Blue, which remains flexible. I’m focusing on Alice’s association and which pet she may have.\n\n\nRe-evaluating the Structure\nAfter solidifying the core relationships, I am evaluating possible permutations, leveraging the established base. Bob is confirmed in the red house, leaving the assignment of Carol and Alice’s homes open. I am focusing on positional constraints to determine the definitive assignments. The final answer feels close.\n\n\nAnalyzing the Solution Space\nI’m wrestling with the final assignments, as the two remaining arrangements feel equally valid. Bob is definitively in the red house with the cat, and Carol has the dog. My attention is on Alice, the remaining house, and the “not cat” clue. I must ensure I do not over-interpret the final constraints. I’m focusing on the spatial and relational aspects.\n\n\nMapping the Final Associations\nThe riddle’s structure is clear. Bob is in the red house with the cat, and Carol has the dog. The spatial clue about the green house being left of the red is key. My focus is finalizing Alice’s location and possible pet. It’s becoming clearer.\n\n\nRe-evaluating the Structure\nThe solution feels very clear; however, I am now focused on a possible “no pet” scenario. Alice’s “not cat” clue makes me consider she owns nothing, or, perhaps, a pet we are not privy to. Since there are only two pets, “no pet” seems the implicit rule.\n\n\nFinalizing the Arrangements\nI am now certain that Bob lives in the red house with the cat, confirming the grid’s structure. By deducing Alice’s and Carol’s possible arrangements, their final assignments should be easier to deduce. However, the constraints are a little tricky. I’m focusing on Alice’s pet status.\n\n\nAnalyzing the Structure\nIt’s clear that Bob lives in the red house with the cat and Carol resides in the green house with her dog, while Alice is confirmed to be in the blue house. It is starting to feel conclusive, as the solution now is clear.\n\n\nAnalyzing the Structure\nIt’s clear I’m working through the connections between the people, houses, and pets with diligence. Bob in the red house with the cat is now firmly established. Carol and Alice occupy green and blue houses respectively. I’m focusing on the final assignment to make a fully conclusive solution.\n\n\nFinalizing the Solution\nThe connections are established. I’ve placed Bob in the red house with the cat, leveraging all available clues. Carol resides in the green house and owns the dog, while Alice is confirmed in the blue house. The association with the unknown pet is no longer an issue, because the prompt never mentioned a specific pet. The solution’s finality is clear.\n\n\nSynthesizing the Solution\nThe riddle’s structure is clear, and I’ve placed Bob in the red house with the cat, leveraging all of the clues. The spatial clue about the green house being to the left of the red is also key. However, I am still trying to determine the relationship between Alice, Carol, and the other houses, with the riddle’s ambiguity being somewhat challenging to overcome.\n\n\nDeciphering the Relationships\nMy recent focus has been on parsing the clues regarding spatial positioning and direct relationships, having mapped the basic structure. The “Green to the left of Red” clue is critical, but it isn’t definitive on who resides in Green or Blue. Determining the pet for Alice is a key concern.\n\n\nRevising the Structure\nAfter reevaluating the constraints, I’m confident in Bob’s red house and cat ownership. The spatial element is clear, green is left of red. Currently, the riddle doesn’t differentiate between the arrangements of Carol and Alice’s homes. I will work further on Carol’s dog.\n\n\nAnalyzing the Structure\nAfter considering several arrangements, I am convinced that Bob lives in the red house with the cat, a firm starting point. The crucial part, however, is now trying to determine Alice and Carol’s assignments. The green house being to the left of the red house gives an added hint, however, the structure remains ambiguous.\n\n\nSynthesizing the Structure\nThe solution space is clarifying. I’ve placed Bob in the red house with the cat, a firm foundation. Carol owns the dog. The challenge is assigning the final pet and the Green and Blue houses. I’m focusing on the spatial constraint, but can’t distinguish between Alice and Carol.\n\n\nDeciphering the Relationships\nI’ve been meticulously analyzing the clues, starting to form a clear structure. I’m focusing on Carol’s Dog, Bob in Red, and spatial positions. Alice’s possible pet ownership is now my main priority; I’m building a solution that addresses her specific pet associations and house color.\n\n\nAnalyzing the Structure\nThe logical puzzle is now fully parsed. Bob is in the red house with the cat, a firm conclusion. The key constraints about Carol and Alice, the spatial aspect, and the pets require careful consideration, and I will continue to work on it with greater efficiency.\n\n\nI’m focusing on parsing the relationships within the riddle’s structure, aiming for definitive assignments. Bob is firmly established in the red house with the cat, a crucial starting point. My current focus centers on Alice and Carol, the green and blue houses, and the riddle’s implied “no pet” rule.\n\n\nParsing the Clues\nI’ve carefully examined the clues and established the core structure: Bob resides in the red house with the cat, Carol owns the dog, and Alice does not own the cat. The riddle’s focus is to determine the assignment of Green and Blue houses and the association of the remaining two pets and Alice’s assignment. I’m focusing on the spatial relationship.\n\n\nRevising the Structure\nHaving confirmed the basic framework, I’m now focused on the final assignments. Bob resides in the red house with the cat, and Carol has the dog. My attention shifts to Alice’s “not cat” situation and her house assignment. Leveraging the spatial clue, I’m analyzing potential placements to reach a conclusive solution.\n\n\nResponse\n\n\nLet’s break down the clues to solve this logic puzzle:\n1. Create a Grid to Organize Information:\n\n\n\nPerson\nHouse\nPet\n\n\n\n\nAlice\n\n\n\n\nBob\n\n\n\n\nCarol\n\n\n\n\n\n\n\n2. Process Each Clue:\n\nClue 1: “Alice, Bob, and Carol each live in a different house on the same street: red, green, and blue.”\n\nConfirms a 1:1 mapping of people, houses, and (implicitly) pets.\n\n\n\n\n\nClue 2: “The person who lives in the red house owns a cat.”\n\n(Red House) -&gt; Cat\n\n\n\n\n\nClue 3: “Bob does not live in the green house.”\n\nBob ≠ Green House\n\n\n\n\n\nClue 4: “Carol owns a dog.”\n\nAdd this directly to the grid: Carol -&gt; Dog\n\nClue 5: “The green house is to the left of the red house.”\n\nThis establishes a spatial relationship: Green &lt; Red\n\n\n\n\n\nClue 6: “Alice does not own a cat.”\n\nAlice ≠ Cat\n\n\n3. Deductions:\n\n\n\nDeduction 1: Identify the Cat Owner.\n\nFrom Clue 4, Carol owns a dog. So Carol is not the cat owner.\nFrom Clue 6, Alice does not own a cat.\nSince Alice and Carol don’t own the cat, Bob must own the cat.\n\n\n\n\n\nDeduction 2: Identify Who Lives in the Red House.\n\nFrom Clue 2, the Red House owner has a cat.\nFrom Deduction 1, Bob owns the cat.\nTherefore, Bob lives in the Red House.\n\n\n\n\n\nUpdate the Grid:\n\n\n\n\n\n\nPerson\nHouse\nPet\n\n\n\n\nAlice\n\nNot Cat\n\n\nBob\nRed\nCat\n\n\nCarol\n\nDog\n\n\n\n\n\n\nDeduction 3: Assign Remaining People to Houses.\n\nWe know Bob lives in the Red House.\nFrom Clue 6, Alice does not own a cat, so she cannot live in the Red House (where the cat owner lives).\nFrom Clue 4, Carol owns a dog, so she cannot live in the Red House (where the cat owner lives).\nThis means the Green House and the Blue House must be occupied by Alice and Carol.\n\n\n\n\n\nDeduction 4: Consider the “to the left” clue.\n\nClue 5 states: “The green house is to the left of the red house.”\nWe know Bob is in the Red House. So the Green house is to the left of Bob’s house.\nThis means the street arrangement is either: [Blue] [Green] [Red (Bob)] or [Green] [Red (Bob)] [Blue].\nThis clue helps establish the relative positions of the houses, but it does not provide any information to definitively place Alice or Carol in the Green house versus the Blue house. Both scenarios (Alice in Green, Carol in Blue OR Carol in Green, Alice in Blue) are consistent with all given clues.\n\n\n\n\nConclusion:\nBased on the information provided, we can definitively determine Bob’s house and pet, and Carol’s pet.\n\n\nHowever, the clues do not provide enough information to definitively determine whether Alice lives in the Green or Blue house, or Carol lives in the Green or Blue house. The problem implies a unique solution, but the information to distinguish between Alice and Carol’s houses is missing.\n\n\nHowever, if we are to provide a complete answer with the most logical deduction based on the usual structure of these puzzles (where there should be a unique pet for each person): Alice’s pet is not a cat, and Carol has a dog. Bob has the cat. This means Alice must have a third, unnamed pet, or no pet.\n\n\nHere’s the most complete and certain answer:\n\nBob lives in the Red House and owns a cat.\nCarol owns a dog.\nAlice does not own a cat.\n\n\n\nSince Alice and Carol are the only ones left for the Green and Blue houses, and there’s no further distinguishing information:\n\nThe Green House is occupied by either Alice (who does not own a cat) or Carol (who owns a dog).\nThe Blue House is occupied by the other person (Alice or Carol).",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Use Gemini thinking"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_thinking.html#thought-summary",
    "href": "quickstarts/Get_started_thinking.html#thought-summary",
    "title": "Use Gemini thinking",
    "section": "Thought Summary",
    "text": "Thought Summary",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Use Gemini thinking"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_thinking.html#response",
    "href": "quickstarts/Get_started_thinking.html#response",
    "title": "Use Gemini thinking",
    "section": "Response",
    "text": "Response",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Use Gemini thinking"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_thinking.html#thought-summary-1",
    "href": "quickstarts/Get_started_thinking.html#thought-summary-1",
    "title": "Use Gemini thinking",
    "section": "Thought Summary",
    "text": "Thought Summary",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Use Gemini thinking"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_thinking.html#response-1",
    "href": "quickstarts/Get_started_thinking.html#response-1",
    "title": "Use Gemini thinking",
    "section": "Response",
    "text": "Response",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Use Gemini thinking"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_thinking.html#working-with-gemini-thinking-models-and-tools",
    "href": "quickstarts/Get_started_thinking.html#working-with-gemini-thinking-models-and-tools",
    "title": "Use Gemini thinking",
    "section": "Working with Gemini thinking models and tools",
    "text": "Working with Gemini thinking models and tools\nGemini thinking models are compatible with the tools and capabilities inherent to the Gemini ecosystem. This compatibility allows them to interface with external environments, execute computational code, or retrieve real-time data, subsequently incorporating such information into their analytical framework and concluding statements.\n\nSolving a problem using the code execution tool\nThis example shows how to use the code execution tool to solve a problem. The model will generate the code and then execute it to get the final answer.\nIn this case, you are using the adaptive thinking_budget so the model will dynamically adjust the budget based on the complexity of the request.\nIf you want to experiment with a fixed budget, you can set the thinkingBudget to a specific value (e.g. thinkingBudget=4096).\n\nconst code_experiment_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: [\n    `\n    What are the best ways to sort a list of n numbers from 0 to m?\n    Generate and run Python code for three different sort algorithms.\n    Provide the final comparison between algorithm clearly.\n    Is one of them linear?\n    `,\n  ],\n  config: {\n    tools: [{ codeExecution: {} }],\n    thinkingConfig: {\n      thinkingBudget: 4096,\n    },\n  },\n});\nfor (const part of code_experiment_response.candidates?.[0]?.content?.parts ?? []) {\n  if (part.text) {\n    tslab.display.markdown(part.text ?? \"\");\n  }\n  if (part.executableCode) {\n    tslab.display.html(`&lt;pre&gt;${part.executableCode.code ?? \"\"}&lt;/pre&gt;`);\n  }\n  if (part.codeExecutionResult) {\n    tslab.display.markdown(`### **Code Execution Result**`);\n    tslab.display.markdown(part.codeExecutionResult.output ?? \"\");\n  }\n}\n\nFor sorting a list of n numbers ranging from 0 to m, the “best ways” depend largely on the relationship between n and m.\n\nNon-Comparison Sorts (e.g., Counting Sort, Radix Sort): These are generally the best when the range m is not excessively large compared to n. They can achieve linear time complexity.\n\nCounting Sort: Ideal when m is relatively small. It works by counting the occurrences of each distinct element and then using those counts to determine the sorted position of each element.\nRadix Sort: Efficient when m is very large but the numbers have a limited number of digits or bits. It sorts numbers digit by digit (or bit by bit) using a stable sorting algorithm (often Counting Sort) as a subroutine.\n\nComparison Sorts (e.g., Merge Sort, Quick Sort, Heap Sort): These are general-purpose sorting algorithms that work by comparing elements. Their lower bound for time complexity is O(n log n), regardless of the range of numbers. They are suitable when m is very large, making non-comparison sorts less efficient or impractical due to memory constraints.\n\nGiven the constraint 0 to m, Counting Sort stands out as a highly relevant and potentially linear-time algorithm.\nLet’s proceed with implementing and running three different sort algorithms: Counting Sort, Merge Sort, and Quick Sort, and then compare them.\n\n1. Counting Sort\nTime Complexity: O(n + m) Space Complexity: O(m) Stability: Yes In-place: No\n\n\n\nimport random\nimport time\n\ndef counting_sort(arr, m):\n    \"\"\"\n    Sorts an array of integers where elements are in the range [0, m].\n    \"\"\"\n    n = len(arr)\n    output = [0] * n\n    count = [0] * (m + 1)\n\n    # Store count of each character\n    for i in range(n):\n        count[arr[i]] += 1\n\n    # Change count[i] so that count[i] now contains actual\n    # position of this character in output array\n    for i in range(1, m + 1):\n        count[i] += count[i - 1]\n\n    # Build the output array\n    # Iterate in reverse to maintain stability\n    i = n - 1\n    while i &gt;= 0:\n        output[count[arr[i]] - 1] = arr[i]\n        count[arr[i]] -= 1\n        i -= 1\n\n    # Copy the output array to arr, so that arr now\n    # contains sorted characters\n    for i in range(n):\n        arr[i] = output[i]\n    return arr\n\n# --- Test Setup ---\nn_test = 100000  # Number of elements\nm_test = 50000   # Max value in the list (range 0 to m_test)\ndata_counting_sort = [random.randint(0, m_test) for _ in range(n_test)]\n# Create copies for other sorts\ndata_merge_sort = list(data_counting_sort)\ndata_quick_sort = list(data_counting_sort)\n\n# --- Run Counting Sort ---\nstart_time = time.perf_counter()\nsorted_counting = counting_sort(data_counting_sort, m_test)\nend_time = time.perf_counter()\ntime_counting = end_time - start_time\nprint(f\"Counting Sort took: {time_counting:.6f} seconds\")\n# print(f\"First 10 elements (Counting Sort): {sorted_counting[:10]}\")\n# print(f\"Last 10 elements (Counting Sort): {sorted_counting[-10:]}\")\n\n\n\nCode Execution Result\n\n\nCounting Sort took: 0.066183 seconds\n\n\n2. Merge Sort\nTime Complexity: O(n log n) Space Complexity: O(n) Stability: Yes In-place: No (requires auxiliary space)\n\n\nimport random\nimport time\n\ndef merge_sort(arr):\n    if len(arr) &lt;= 1:\n        return arr\n\n    mid = len(arr) // 2\n    left_half = arr[:mid]\n    right_half = arr[mid:]\n\n    left_half = merge_sort(left_half)\n    right_half = merge_sort(right_half)\n\n    return merge(left_half, right_half)\n\ndef merge(left, right):\n    merged = []\n    left_idx, right_idx = 0, 0\n\n    while left_idx &lt; len(left) and right_idx &lt; len(right):\n        if left[left_idx] &lt;= right[right_idx]:\n            merged.append(left[left_idx])\n            left_idx += 1\n        else:\n            merged.append(right[right_idx])\n            right_idx += 1\n\n    while left_idx &lt; len(left):\n        merged.append(left[left_idx])\n        left_idx += 1\n    while right_idx &lt; len(right):\n        merged.append(right[right_idx])\n        right_idx += 1\n    return merged\n\n# Data already prepared as `data_merge_sort` from previous block\n\n# --- Run Merge Sort ---\nstart_time = time.perf_counter()\nsorted_merge = merge_sort(data_merge_sort)\nend_time = time.perf_counter()\ntime_merge = end_time - start_time\nprint(f\"Merge Sort took: {time_merge:.6f} seconds\")\n# print(f\"First 10 elements (Merge Sort): {sorted_merge[:10]}\")\n# print(f\"Last 10 elements (Merge Sort): {sorted_merge[-10:]}\")\n\n\n\nCode Execution Result\n\n\nMerge Sort took: 0.449447 seconds\n\n\n3. Quick Sort\nTime Complexity:\n\nAverage: O(n log n)\nWorst Case: O(n^2) (can be mitigated with good pivot selection)\n\nSpace Complexity: O(log n) (average for recursion stack), O(n) (worst case) Stability: No In-place: Yes (mostly, due to recursion stack)\n\n\nimport random\nimport time\n\ndef quick_sort(arr):\n    if len(arr) &lt;= 1:\n        return arr\n    pivot = arr[len(arr) // 2]\n    left = [x for x in arr if x &lt; pivot]\n    middle = [x for x in arr if x == pivot]\n    right = [x for x in arr if x &gt; pivot]\n    return quick_sort(left) + middle + quick_sort(right)\n\n# Note: This is a simpler, less optimized Quick Sort implementation\n# that uses O(N) space due to list comprehensions. An in-place\n# version would modify the list directly.\n\n# Data already prepared as `data_quick_sort` from previous block\n\n# --- Run Quick Sort ---\nstart_time = time.perf_counter()\nsorted_quick = quick_sort(data_quick_sort)\nend_time = time.perf_counter()\ntime_quick = end_time - start_time\nprint(f\"Quick Sort took: {time_quick:.6f} seconds\")\n# print(f\"First 10 elements (Quick Sort): {sorted_quick[:10]}\")\n# print(f\"Last 10 elements (Quick Sort): {sorted_quick[-10:]}\")\n\n\n\nCode Execution Result\n\n\nQuick Sort took: 0.291436 seconds\n\n\n\nFinal Comparison of Algorithms\nLet’s summarize the properties and observed performance for a list of n = 100,000 numbers ranging from 0 to m = 50,000.\n\n\n\n\n\n\n\n\n\nFeature\nCounting Sort\nMerge Sort\nQuick Sort (Partition-based)\n\n\n\n\nTime Complexity\nO(n + m)\nO(n log n)\nO(n log n) (Avg.), O(n²) (Worst)\n\n\nSpace Complexity\nO(m)\nO(n)\nO(log n) (Avg.), O(n) (Worst) for recursion stack\n\n\nStability\nYes\nYes\nNo\n\n\nIn-place\nNo\nNo\nYes (for optimal implementation)\n\n\nObserved Time\n0.066 seconds\n0.449 seconds\n0.291 seconds (using simple Python implementation)\n\n\n\n\n\nIs one of them linear?\nYes, Counting Sort is linear.\nIts time complexity is O(n + m).\n\nIf m (the range of values) is proportional to n (the number of elements), i.e., m = c * n for some constant c, then the complexity becomes O(n + c*n) which simplifies to O(n). In such cases, Counting Sort is a linear-time sorting algorithm.\nIn our test case, n = 100,000 and m = 50,000. Since m is less than n and of the same order of magnitude, Counting Sort performs exceptionally well, significantly faster than the O(n log n) comparison sorts.\n\n\n\nConclusion\nFor sorting n numbers from 0 to m:\n\nCounting Sort is the clear winner when m is not significantly larger than n (e.g., m is comparable to n, or even m &lt; n). It leverages the limited range of values to achieve linear time complexity, making it much faster than comparison-based sorts in such scenarios, as demonstrated by the performance difference (0.066s vs ~0.3-0.4s).\nMerge Sort is a solid general-purpose choice, always providing O(n log n) performance and stability, but at the cost of O(n) auxiliary space.\nQuick Sort is generally faster in practice than Merge Sort for average cases due to better cache performance and lower constant factors, despite the theoretical worst-case O(n²) complexity. However, its simple Python implementation used here consumes O(n) space and is not truly in-place.\n\nTherefore, for the specific problem of sorting numbers from 0 to m, Counting Sort is generally the “best” way if m is within a reasonable range, as it provides linear time complexity.\n\n\n\n\n\nThinking with search tool\nSearch grounding is a great way to improve the quality of the model responses by giving it the ability to search for the latest information using Google Search. Check the dedicated guide for more details on that feature.\nIn this case, you are using the adaptive thinkingBudget so the model will dynamically adjust the budget based on the complexity of the request.\nIf you want to experiment with a fixed budget, you can set the thinkingBudget to a specific value (e.g. thinkingBudget=4096).\n\nconst search_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: [\n    `\n    What were the major scientific breakthroughs announced last month? Use your\n    critical thinking and only list what's really incredible and not just an\n    overinfluated title.\n    `,\n  ],\n  config: {\n    tools: [{ googleSearch: {} }],\n    thinkingConfig: {\n      thinkingBudget: 4096,\n      includeThoughts: true,\n    },\n  },\n});\n\nlet search_thoughts = \"\";\nlet search_answers = \"\";\n\nfor (const part of search_response.candidates?.[0]?.content?.parts ?? []) {\n  if (!part.text) continue;\n  if (part.thought) {\n    if (!search_thoughts) tslab.display.markdown(`## **Thought Summary**`);\n    tslab.display.markdown(part.text ?? \"\");\n    search_thoughts += part.text ?? \"\";\n  } else {\n    if (!search_answers) tslab.display.markdown(`## **Response**`);\n    tslab.display.markdown(part.text ?? \"\");\n    search_answers += part.text ?? \"\";\n  }\n}\n\ntslab.display.markdown(`## **Search Results**`);\ntslab.display.html(search_response.candidates?.[0]?.groundingMetadata?.searchEntryPoint?.renderedContent ?? \"\");\n\nThought Summary\n\n\nClarifying “Last Month”\nI’ve established that “last month” refers to May 2025, given the current date. My next step involves conducting focused Google searches using various relevant terms to unearth noteworthy scientific breakthroughs from that period. I’ll then critically assess these findings. I’m prioritizing reputable sources to identify genuinely impactful advancements.\n\n\nNarrowing Down Breakthroughs\nI’ve significantly refined the initial list of potential breakthroughs, prioritizing findings that are truly impactful and novel. The “conversion of lead to gold” has been removed due to potential sensationalism, while the impressive count of Saturn’s moons, and the potential impacts of vitamin D have been deprioritized for now. I’m focusing on the record-breaking redshift galaxy, the fusion milestone, the OCD gene discovery, night-vision lenses, green hydrogen, the new material, the solid-tumor CAR-T success, brain cell repair, and the AI co-scientist. I believe that these are solid candidates for “incredible” breakthroughs worthy of further analysis. Also, I’m watching the pancreatic cancer vaccine and AI-powered cancer detection advancements with great interest, as these present huge potential.\n\n\nResponse\n\n\nLast month, May 2025, saw several significant scientific announcements across various fields, pointing to incredible progress in our understanding of the universe, advancements in medicine, and innovative technological developments.\nIn the realm of space and astronomy:\n\nMost Distant Galaxy Ever Detected: The James Webb Space Telescope (JWST) confirmed MoM-z14 as the most distant galaxy ever detected, with a redshift of 14.44. This places its formation within 280 million years of the Big Bang, offering unprecedented insights into the early universe.\nNew Exoplanets Around Barnard’s Star: Astronomers confirmed the existence of three new rocky exoplanets, all smaller than Earth, orbiting Barnard’s Star, the closest solitary star to our Sun. This brings the total number of known planets around Barnard’s Star to four, including the previously hinted-at Barnard b.\n\nIn medicine and health:\n\nBreakthrough in OCD Genetics: For the first time, researchers identified 250 genes linked to Obsessive-Compulsive Disorder (OCD) through a study involving over 2 million people. This discovery represents a significant step towards understanding the genetic underpinnings of the condition.\nNight-Vision Contact Lenses: Scientists developed contact lenses that grant “super-vision,” allowing users to perceive beyond the visible light spectrum and detect infrared light even in darkness or with closed eyes. This innovation could potentially replace traditional night-vision goggles.\nRepairing Brain Cells: New research demonstrated that brain cells can be repaired, with a drug known as antisense oligonucleotide successfully repairing human neurons in individuals with Timothy’s syndrome, a rare genetic disorder. This breakthrough offers hope for treating other genetic conditions affecting the brain, such as schizophrenia, epilepsy, ADHD, and autism spectrum disorder.\nLong-Term Cancer Remission with CAR-T Therapy: A child who received CAR-T cancer therapy for neuroblastoma 18 years ago remains disease-free, suggesting that this personalized cancer treatment may be effective not only for blood cancers but also for solid tumors.\n\nIn physics and energy:\n\nNew Nuclear Fusion Record: The WEST tokamak in France set a new world record for nuclear fusion by maintaining plasma for an impressive 1,337 seconds, a 25% increase over the previous record. This is a crucial step forward in the quest for sustainable fusion energy.\nSunlight-Powered Green Hydrogen Production: Scientists created a prototype reactor capable of harvesting hydrogen fuel using only sunlight and water. This advancement could revolutionize renewable energy by providing a cost-effective method for producing “green hydrogen.”\nFirst 2D Mechanically Interlocked Material: Northwestern University demonstrated the creation of the first two-dimensional (2D) mechanically interlocked material. Described as possessing exceptional flexibility and strength with 100 trillion bonds per square centimeter, this new class of material could have wide-ranging applications.\n\nIn artificial intelligence:\n\nAI Co-Scientist for Accelerated Discovery: An “AI Co-Scientist” was developed as a collaborative tool to assist scientists in generating novel hypotheses and research proposals. This AI has shown promise in areas such as drug repurposing and proposing new treatment targets, potentially accelerating the pace of biomedical discoveries.\n\n\n\nSearch Results\n\n\n\n\n  \n    \n      \n      \n      \n      \n    \n    \n      \n      \n      \n      \n      \n    \n    \n  \n  \n    top research announcements May 2025\n    major scientific breakthroughs May 2025\n    significant science discoveries May 2025",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Use Gemini thinking"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_thinking.html#thought-summary-2",
    "href": "quickstarts/Get_started_thinking.html#thought-summary-2",
    "title": "Use Gemini thinking",
    "section": "Thought Summary",
    "text": "Thought Summary",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Use Gemini thinking"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_thinking.html#response-2",
    "href": "quickstarts/Get_started_thinking.html#response-2",
    "title": "Use Gemini thinking",
    "section": "Response",
    "text": "Response",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Use Gemini thinking"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_thinking.html#search-results",
    "href": "quickstarts/Get_started_thinking.html#search-results",
    "title": "Use Gemini thinking",
    "section": "Search Results",
    "text": "Search Results",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Use Gemini thinking"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_thinking.html#next-steps",
    "href": "quickstarts/Get_started_thinking.html#next-steps",
    "title": "Use Gemini thinking",
    "section": "Next Steps",
    "text": "Next Steps\nTry Gemini 2.5 Pro Experimental in Google AI Studio, and learn more about Prompting for thinking models.\nFor more examples of the Gemini capabilities, check the other Cookbook examples. You’ll learn how to use the Live API, juggle with multiple tools or use Gemini spatial understanding abilities.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Use Gemini thinking"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_LyriaRealtime.html",
    "href": "quickstarts/Get_started_LyriaRealtime.html",
    "title": "Get started with Music generation using Lyria RealTime",
    "section": "",
    "text": "Lyria RealTime, provides access to a state-of-the-art, real-time, streaming music generation model. It allows developers to build applications where users can interactively create, continuously steer, and perform instrumental music using text prompts.\nLyria RealTime main characteristics are:\nCheck Lyria RealTime’s documentation for more details.\nAlso note that due to Colab limitation, you won’t be able to experience the real time capabilities of Lyria RealTime but only limited audio output. Use the AI studio’s apps, Prompt DJ and MIDI DJ to fully experience Lyria RealTime",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Get started with Music generation using Lyria RealTime"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_LyriaRealtime.html#setup",
    "href": "quickstarts/Get_started_LyriaRealtime.html#setup",
    "title": "Get started with Music generation using Lyria RealTime",
    "section": "Setup",
    "text": "Setup\n\nInstall the Google GenAI SDK\nInstall the Google GenAI SDK from npm.\n$ npm install @google/genai\n\n\nSetup your API key\nYou can create your API key using Google AI Studio with a single click.\nRemember to treat your API key like a password. Don’t accidentally save it in a notebook or source file you later commit to GitHub. In this notebook we will be storing the API key in a .env file. You can also set it as an environment variable or use a secret manager.\nHere’s how to set it up in a .env file:\n$ touch .env\n$ echo \"GEMINI_API_KEY=&lt;YOUR_API_KEY&gt;\" &gt;&gt; .env\n\n\n\n\n\n\nTip\n\n\n\nAnother option is to set the API key as an environment variable. You can do this in your terminal with the following command:\n$ export GEMINI_API_KEY=\"&lt;YOUR_API_KEY&gt;\"\n\n\n\n\nLoad the API key\nTo load the API key from the .env file, we will use the dotenv package. This package loads environment variables from a .env file into process.env.\n$ npm install dotenv\nThen, we can load the API key in our code:\n\nconst dotenv = require(\"dotenv\") as typeof import(\"dotenv\");\n\ndotenv.config({\n  path: \"../.env\",\n});\n\nconst GEMINI_API_KEY = process.env.GEMINI_API_KEY ?? \"\";\nif (!GEMINI_API_KEY) {\n  throw new Error(\"GEMINI_API_KEY is not set in the environment variables\");\n}\nconsole.log(\"GEMINI_API_KEY is set in the environment variables\");\n\nGEMINI_API_KEY is set in the environment variables\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn our particular case the .env is is one directory up from the notebook, hence we need to use ../ to go up one directory. If the .env file is in the same directory as the notebook, you can omit it altogether.\n│\n├── .env\n└── quickstarts\n    └── Get_started_LyriaRealTime.ipynb\n\n\n\n\nInitialize SDK Client\nLyria RealTime API is a new capability introduced with the Lyria RealTime model so only works with the lyria-realtime-exp model. As it’s an experimental feature, you also need to use the v1alpha client version.\n\nconst google = require(\"@google/genai\") as typeof import(\"@google/genai\");\n\nconst ai = new google.GoogleGenAI({ apiKey: GEMINI_API_KEY, httpOptions: { apiVersion: \"v1alpha\" } });\n\n\n\nSelect a model\nMultimodal Live API are a new capability introduced with the Gemini 2.0 model. It won’t work with previous generation models.\n\nconst tslab = require(\"tslab\") as typeof import(\"tslab\");\n\nconst MODEL_ID = \"models/lyria-realtime-exp\";\n\n\n\nUtilites\nYou’re going to use the Lyria Realtime’s audio output, the easiest way hear it in Colab is to write the PCM data out as a WAV file:\n\nconst fs = require(\"fs\") as typeof import(\"fs\");\nconst path = require(\"path\") as typeof import(\"path\");\nconst wave = require(\"wavefile\") as typeof import(\"wavefile\");\n\nfunction saveAudioToFile(audioData: Int16Array, filePath: string) {\n  fs.mkdirSync(path.dirname(filePath), { recursive: true });\n  const wav = new wave.WaveFile();\n  wav.fromScratch(2, 48000, \"16\", audioData);\n  fs.writeFileSync(filePath, wav.toBuffer());\n  console.debug(`Audio saved to ${filePath}`);\n}",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Get started with Music generation using Lyria RealTime"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_LyriaRealtime.html#generate-music",
    "href": "quickstarts/Get_started_LyriaRealtime.html#generate-music",
    "title": "Get started with Music generation using Lyria RealTime",
    "section": "Generate music",
    "text": "Generate music\nThe Lyria Realtime model utilizes websockets to stream audio data in real time. The model can be prompted with text descriptions of the desired music, and it will generate audio that matches the description and stream it in chunks. It takes 2 different configuration parameters as input:\n\nWeightedPrompt: A list of text prompts that describe the desired music. Each prompt can have a weight that indicates its influence on the generated music. The prompts can be sent while the session is active, allowing for continuous steering of the music generation.\nLiveMusicGenerationConfig: A configuration object that specifies the desired characteristics of the generated music, such as bpm, density, brightness, scale, and guidance. These parameters can be adjusted in real time to influence the music generation.\n\n\n\n\n\n\n\nImportant\n\n\n\nYou can’t just update a single parameter in the LiveMusicGenerationConfig object. You need to send the entire object with all the parameters each time you want to update it, otherwise the other parameters will be reset to their default values.\nAny updates to bpm or scale need to be followed by a resetContext call to reset the context of the music generation. This is because these parameters affect the musical structure and need to be applied from the beginning of the generation.\n\n\n\nimport { LiveMusicGenerationConfig, LiveMusicSession, LiveMusicServerMessage, WeightedPrompt } from \"@google/genai\";\n\nlet n_index = 0;\nconst MAX_CHUNKS = 10; // Maximum number of audio chunks to process\nconst responseQueue: LiveMusicServerMessage[] = [];\n\nasync function receive() {\n  console.debug(\"Receiving audio chunks...\");\n  let done = false;\n  let chunk_count = 0;\n  const audioChunks: number[][] = [];\n  while (!done) {\n    if (responseQueue.length &gt; 0) {\n      const response = responseQueue.shift();\n      if (response?.audioChunk?.data) {\n        const audioBuffer = Buffer.from(response.audioChunk.data, \"base64\");\n        const intArray = new Int16Array(\n          audioBuffer.buffer,\n          audioBuffer.byteOffset,\n          audioBuffer.length / Int16Array.BYTES_PER_ELEMENT\n        );\n        audioChunks.push(Array.from(intArray));\n        chunk_count++;\n      }\n      if (chunk_count &gt;= MAX_CHUNKS) {\n        done = true;\n        console.debug(\"Received complete response\");\n      }\n    } else {\n      await new Promise((resolve) =&gt; setTimeout(resolve, 100));\n    }\n  }\n  const audioFilePath = path.join(\"../assets/live\", `lyria_realtime_${n_index}.wav`);\n  saveAudioToFile(new Int16Array(audioChunks.flat()), audioFilePath);\n  tslab.display.html(`\n    &lt;h3&gt;Audio Response Lyria&lt;/h3&gt;\n    &lt;audio controls&gt;\n        &lt;source src=\"../assets/live/lyria_realtime_${n_index}.wav\" type=\"audio/wav\"&gt;\n        Your browser does not support the audio element.\n    &lt;/audio&gt;\n  `);\n  n_index++;\n}\n\nasync function generateMusic(prompts: WeightedPrompt[], config: LiveMusicGenerationConfig) {\n  const session: LiveMusicSession = await ai.live.music.connect({\n    model: MODEL_ID,\n    callbacks: {\n      onmessage: (message) =&gt; {\n        responseQueue.push(message);\n      },\n      onerror: (error) =&gt; {\n        console.error(\"music session error:\", error);\n      },\n      onclose: () =&gt; {\n        console.log(\"Lyria RealTime stream closed.\");\n      },\n    },\n  });\n\n  await session.setWeightedPrompts({\n    weightedPrompts: prompts,\n  });\n  await session.setMusicGenerationConfig({\n    musicGenerationConfig: config,\n  });\n\n  console.debug(\"Lyria Realtime session started\");\n  session.play();\n  await receive();\n  session.close();\n  console.debug(\"Lyria Realtime session closed\");\n}",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Get started with Music generation using Lyria RealTime"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_LyriaRealtime.html#audio-generation-function",
    "href": "quickstarts/Get_started_LyriaRealtime.html#audio-generation-function",
    "title": "Get started with Music generation using Lyria RealTime",
    "section": "Audio Generation Function",
    "text": "Audio Generation Function\nThe above code sample shows how to generate music using the Lyria Realtime model. There are two methods worth noting:\n\ngenerateMusic - Driver method\nThis method is used to start the music generation process. It takes an array of WeightedPrompt objects and a LiveMusicGenerationConfig object as input. It returns a LiveMusicGenerationSession object that can be used to interact with the music generation session.\nThis method: - Opens a websocket connection to the Lyria Realtime model. - Sends the initial prompts to the model using setWeightedPrompts, which sets the initial musical influences. - Sends the initial configuration using setLiveMusicGenerationConfig, which sets the desired characteristics of the generated music. - Sets up event listeners to handle incoming audio data and errors and start the audio playback.\n\n\nreceive - Audio data handler\nThis methods is used to handle incoming audio data from the Lyria Realtime model. It monitors the responseQueue for incoming audio data and collects it in a buffer. When the buffer reaches a certain size, it writes the audio data to a WAV file and plays it back using the saveAudioToFile utility function.\n\n\n\n\n\n\nNote\n\n\n\nCurrently once the receive method is called, it blocks further function execution till required number of chunks are met. This means that you won’t be able to send new prompts or configuration updates while the receive method is running. Ideally, in a real-time application, you would want to run the receive method in a separate thread while also having a send method to send new prompts and configuration updates.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Get started with Music generation using Lyria RealTime"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_LyriaRealtime.html#try-lyria-realtime",
    "href": "quickstarts/Get_started_LyriaRealtime.html#try-lyria-realtime",
    "title": "Get started with Music generation using Lyria RealTime",
    "section": "Try Lyria Realtime",
    "text": "Try Lyria Realtime\nBecause of Colab limitation you won’t be able to experience the “real time” part of Lyria RealTime, so all those examples are going to be one-offs prompt to get an audio file.\nOne thing to note is that the audio will only be played at the end of the session when all would have been written in the wav file. When using the API for real you’ll be able to start plyaing as soon as the first chunk arrives. So the longer the duration (using the dedicated parameter) you set, the longer you’ll have to wait until you hear something.\n\nSimple Lyria RealTime example\nHere’s first a simple example:\n\nawait generateMusic(\n  [\n    {\n      text: \"piano\",\n      weight: 1.0,\n    },\n  ],\n  { bpm: 120, density: 1.0 }\n);\n\nLive music generation is experimental and may change in future versions.\n\n\nLyria Realtime session started\nReceiving audio chunks...\nReceived complete response\nAudio saved to ../assets/live/lyria_realtime_0.wav\n\n\n\n    Audio Response Lyria\n    \n        \n        Your browser does not support the audio element.\n    \n  \n\n\nLyria Realtime session closed\n\n\n\n\nTry Lyria RealTime by yourself\nNow you can try mixing multiple prompts, and tinkering with the music configuration.\nThe prompts needs to follow their specific format which is a list of prompts with weights (which can be any values, including negative, except 0) like this:\n{\n    \"text\": \"Text of the prompt\",\n    \"weight\": 1.0\n}\nYou should try to stay simple (unlike when you’re using image-out) as the model will better understand things like “meditation”, “eerie”, “harp” than “An eerie and relaxing music illustrating the verdoyant forests of Scotland using string instruments”.\nThe music configuration options available to you are:\n\nbpm: beats per minute\nguidance: how strictly the model follows the prompts\ndensity: density of musical notes/sounds\nbrightness: tonal quality\nscale: musical scale (key and mode)\n\nOther options are available (mute_bass for ex.). Check the documentation for the full list.\nSelect one of the sample prompts (genres, instruments and mood), or write your owns. Check the documentation for more details and prompt examples.\n\nawait generateMusic(\n  [\n    {\n      text: \"Indie Pop\",\n      weight: 0.6,\n    },\n    {\n      text: \"Sitar\",\n      weight: 2,\n    },\n    {\n      text: \"Danceable\",\n      weight: 1.4,\n    },\n  ],\n  {\n    bpm: 140,\n    scale: google.Scale.F_MAJOR_D_MINOR,\n    density: 0.2,\n    brightness: 0.7,\n    guidance: 4.0,\n  }\n);\n\nLive music generation is experimental and may change in future versions.\n\n\nLyria RealTime stream closed.\nLyria Realtime session started\nReceiving audio chunks...\nReceived complete response\nAudio saved to ../assets/live/lyria_realtime_1.wav\n\n\n\n    Audio Response Lyria\n    \n        \n        Your browser does not support the audio element.\n    \n  \n\n\nLyria Realtime session closed\nLyria RealTime stream closed.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Get started with Music generation using Lyria RealTime"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_LyriaRealtime.html#whats-next",
    "href": "quickstarts/Get_started_LyriaRealtime.html#whats-next",
    "title": "Get started with Music generation using Lyria RealTime",
    "section": "What’s next?",
    "text": "What’s next?\nNow that you know how to generate music, here are other cool things to try:\n\nInstead of music, learn how to generate multi-speakers conversation using the TTS models.\nDiscover how to generate images or videos.\nInstead of generation music or audio, find out how to Gemini can understand Audio files.\nHave a real-time conversation with Gemini using the Live API.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Get started with Music generation using Lyria RealTime"
    ]
  },
  {
    "objectID": "quickstarts/Enum.html",
    "href": "quickstarts/Enum.html",
    "title": "Gemini API: Enum Quickstart",
    "section": "",
    "text": "The Gemini API allows you to supply a schema to define function arguments (for function calling), or to constrain its output in JSON or using an Enum. This tutorial gives some examples using enums.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Enum Quickstart"
    ]
  },
  {
    "objectID": "quickstarts/Enum.html#setup",
    "href": "quickstarts/Enum.html#setup",
    "title": "Gemini API: Enum Quickstart",
    "section": "Setup",
    "text": "Setup\n\nInstall the Google GenAI SDK\nInstall the Google GenAI SDK from npm.\n$ npm install @google/genai\n\n\nSetup your API key\nYou can create your API key using Google AI Studio with a single click.\nRemember to treat your API key like a password. Don’t accidentally save it in a notebook or source file you later commit to GitHub. In this notebook we will be storing the API key in a .env file. You can also set it as an environment variable or use a secret manager.\nHere’s how to set it up in a .env file:\n$ touch .env\n$ echo \"GEMINI_API_KEY=&lt;YOUR_API_KEY&gt;\" &gt;&gt; .env\n\n\n\n\n\n\nTip\n\n\n\nAnother option is to set the API key as an environment variable. You can do this in your terminal with the following command:\n$ export GEMINI_API_KEY=\"&lt;YOUR_API_KEY&gt;\"\n\n\n\n\nLoad the API key\nTo load the API key from the .env file, we will use the dotenv package. This package loads environment variables from a .env file into process.env.\n$ npm install dotenv\nThen, we can load the API key in our code:\n\nconst dotenv = require(\"dotenv\") as typeof import(\"dotenv\");\n\ndotenv.config({\n  path: \"../.env\",\n});\n\nconst GEMINI_API_KEY = process.env.GEMINI_API_KEY ?? \"\";\nif (!GEMINI_API_KEY) {\n  throw new Error(\"GEMINI_API_KEY is not set in the environment variables\");\n}\nconsole.log(\"GEMINI_API_KEY is set in the environment variables\");\n\nGEMINI_API_KEY is set in the environment variables\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn our particular case the .env is is one directory up from the notebook, hence we need to use ../ to go up one directory. If the .env file is in the same directory as the notebook, you can omit it altogether.\n│\n├── .env\n└── quickstarts\n    └── Get_started_TTS.ipynb\n\n\n\n\nInitialize SDK Client\nWith the new SDK, now you only need to initialize a client with you API key (or OAuth if using Vertex AI). The model is now set in each call.\n\nconst google = require(\"@google/genai\") as typeof import(\"@google/genai\");\n\nconst ai = new google.GoogleGenAI({ apiKey: GEMINI_API_KEY });\n\n\n\nSelect a model\nNow select the model you want to use in this guide, either by selecting one in the list or writing it down. Keep in mind that some models, like the 2.5 ones are thinking models and thus take slightly more time to respond (cf. thinking notebook for more details and in particular learn how to switch the thiking off).\nFor more information about all Gemini models, check the documentation for extended information on each of them.\n\nconst tslab = require(\"tslab\") as typeof import(\"tslab\");\n\nconst MODEL_ID = \"gemini-2.5-flash-preview-05-20\";",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Enum Quickstart"
    ]
  },
  {
    "objectID": "quickstarts/Enum.html#enums",
    "href": "quickstarts/Enum.html#enums",
    "title": "Gemini API: Enum Quickstart",
    "section": "Enums",
    "text": "Enums\nIn the simplest case is you need the model to choose one option from a list of choices, use an enum class to define the schema. Ask it to identify this instrument:\n\nconst fs = require(\"fs\") as typeof import(\"fs\");\nconst path = require(\"path\") as typeof import(\"path\");\n\nconst IMG_URL = \"https://goo.gle/instrument-img\";\n\nconst downloadFile = async (url: string, filePath: string) =&gt; {\n  const response = await fetch(url);\n  if (!response.ok) {\n    throw new Error(`Failed to download image: ${response.statusText}`);\n  }\n  const buffer = await response.blob();\n  const bufferData = Buffer.from(await buffer.arrayBuffer());\n  fs.writeFileSync(filePath, bufferData);\n};\n\nconst filePath = path.join(\"../assets\", \"organ.jpg\");\nawait downloadFile(IMG_URL, filePath);\n\n\ntslab.display.jpeg(fs.readFileSync(filePath));\n\n\n\n\n\n\n\n\nThe response should be one of the following options:\n\nimport { Schema, Type } from \"@google/genai\";\n\nenum Instrument {\n  PERCUSSION = \"Percussion\",\n  STRING = \"String\",\n  WOODWIND = \"Woodwind\",\n  BRASS = \"Brass\",\n  KEYBOARD = \"Keyboard\",\n}\n\nconst InstrumentSchema: Schema = {\n  type: Type.STRING,\n  description: \"The category of musical instrument\",\n  enum: Object.values(Instrument),\n};\n\nPass the enum class as the responseSchema, and for this simplest case you can use the responseMimeType = \"text/x.enum\" option to get one of those enum members as the response.\n\nconst instrument_file = await ai.files.upload({\n  file: filePath,\n  config: {\n    displayName: \"instrument.jpg\",\n    mimeType: \"image/jpeg\",\n  },\n});\n\nconst instrument_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: [\n    google.createPartFromUri(instrument_file.uri ?? \"\", instrument_file.mimeType ?? \"\"),\n    \"what is the category of this instrument?\",\n  ],\n  config: {\n    responseMimeType: \"text/x.enum\",\n    responseSchema: InstrumentSchema,\n  },\n});\nconsole.log(\"Instrument category:\", instrument_response.text);\n\nInstrument category: Keyboard\n\n\nYou can also use enums with responseMimeType = \"application/json\". In this simple case the response will be identical but in quotes.\n\nconst instrument_json_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: [\n    google.createPartFromUri(instrument_file.uri ?? \"\", instrument_file.mimeType ?? \"\"),\n    \"what is the category of this instrument?\",\n  ],\n  config: {\n    responseMimeType: \"application/json\",\n    responseSchema: InstrumentSchema,\n  },\n});\nconsole.log(\"Instrument category (JSON):\", instrument_json_response.text);\n\nInstrument category (JSON): \"Keyboard\"\n\n\nOutside of simple multiple choice problems, an enum can be used anywhere in the schema for JSON or function calling. For example, ask it for a list of recipe titles, and use a Grade enum to give each one a popularity-grade:\n\nimport { Schema, Type } from \"@google/genai\";\n\nenum Grade {\n  A_PLUS = \"A+\",\n  A = \"A\",\n  B = \"B\",\n  C = \"C\",\n  D = \"D\",\n  F = \"F\",\n}\n\nconst RecipeSchema: Schema = {\n  type: Type.OBJECT,\n  description: \"A recipe for a dish\",\n  properties: {\n    recipeName: {\n      type: Type.STRING,\n      description: \"The name of the recipe\",\n    },\n    grade: {\n      type: Type.STRING,\n      description: \"The grade of the recipe\",\n      enum: Object.values(Grade),\n    },\n  },\n  required: [\"recipeName\", \"grade\"],\n};\n\nconst RecipeListSchema: Schema = {\n  type: Type.ARRAY,\n  description: \"A list of recipes with their grades\",\n  items: RecipeSchema,\n};\n\nconst recipe_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: [\"List about 10 cookie recipes, grade them based on popularity\"],\n  config: {\n    responseMimeType: \"application/json\",\n    responseSchema: RecipeListSchema,\n  },\n});\nconsole.log(\"Recipe response:\");\nconsole.log(JSON.stringify(JSON.parse(recipe_response.text ?? \"\"), null, 2));\n\nRecipe response:\n[\n  {\n    \"grade\": \"A+\",\n    \"recipeName\": \"Classic Chocolate Chip Cookies\"\n  },\n  {\n    \"grade\": \"A\",\n    \"recipeName\": \"Peanut Butter Cookies\"\n  },\n  {\n    \"grade\": \"A\",\n    \"recipeName\": \"Oatmeal Raisin Cookies\"\n  },\n  {\n    \"grade\": \"B\",\n    \"recipeName\": \"Sugar Cookies (Cut-Out)\"\n  },\n  {\n    \"grade\": \"B\",\n    \"recipeName\": \"Snickerdoodle Cookies\"\n  },\n  {\n    \"grade\": \"B\",\n    \"recipeName\": \"Gingerbread Cookies\"\n  },\n  {\n    \"grade\": \"C\",\n    \"recipeName\": \"Shortbread Cookies\"\n  },\n  {\n    \"grade\": \"C\",\n    \"recipeName\": \"Macadamia Nut White Chocolate Chip Cookies\"\n  },\n  {\n    \"grade\": \"D\",\n    \"recipeName\": \"No-Bake Cookies\"\n  },\n  {\n    \"grade\": \"D\",\n    \"recipeName\": \"Lemon Crinkle Cookies\"\n  }\n]",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Enum Quickstart"
    ]
  },
  {
    "objectID": "quickstarts/Enum.html#next-steps",
    "href": "quickstarts/Enum.html#next-steps",
    "title": "Gemini API: Enum Quickstart",
    "section": "Next Steps",
    "text": "Next Steps\n\nUseful API references:\nCheck the structured ouput documentation or the GenerationConfig API reference for more details.\n\n\nRelated examples\n\nThe constrained output is used in the Text summarization example to provide the model a format to summarize a story (genre, characters, etc…)\nThe Object detection examples are using the JSON constrained output to uniiformize the output of the detection.\n\n\n\nContinue your discovery of the Gemini API\nAn Enum is not the only way to constrain the output of the model, you can also use an JSON schema. Function calling and Code execution are other ways to enhance your model by either let him use your own functions or by letting it write and run them.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Enum Quickstart"
    ]
  },
  {
    "objectID": "quickstarts/Models.html",
    "href": "quickstarts/Models.html",
    "title": "Gemini API: List models",
    "section": "",
    "text": "This notebook demonstrates how to list the models that are available for you to use in the Gemini API, and how to find details about a model.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: List models"
    ]
  },
  {
    "objectID": "quickstarts/Models.html#setup",
    "href": "quickstarts/Models.html#setup",
    "title": "Gemini API: List models",
    "section": "Setup",
    "text": "Setup\n\nInstall the Google GenAI SDK\nInstall the Google GenAI SDK from npm.\n$ npm install @google/genai\n\n\nSetup your API key\nYou can create your API key using Google AI Studio with a single click.\nRemember to treat your API key like a password. Don’t accidentally save it in a notebook or source file you later commit to GitHub. In this notebook we will be storing the API key in a .env file. You can also set it as an environment variable or use a secret manager.\nHere’s how to set it up in a .env file:\n$ touch .env\n$ echo \"GEMINI_API_KEY=&lt;YOUR_API_KEY&gt;\" &gt;&gt; .env\n\n\n\n\n\n\nTip\n\n\n\nAnother option is to set the API key as an environment variable. You can do this in your terminal with the following command:\n$ export GEMINI_API_KEY=\"&lt;YOUR_API_KEY&gt;\"\n\n\n\n\nLoad the API key\nTo load the API key from the .env file, we will use the dotenv package. This package loads environment variables from a .env file into process.env.\n$ npm install dotenv\nThen, we can load the API key in our code:\n\nconst dotenv = require(\"dotenv\") as typeof import(\"dotenv\");\n\ndotenv.config({\n  path: \"../.env\",\n});\n\nconst GEMINI_API_KEY = process.env.GEMINI_API_KEY ?? \"\";\nif (!GEMINI_API_KEY) {\n  throw new Error(\"GEMINI_API_KEY is not set in the environment variables\");\n}\nconsole.log(\"GEMINI_API_KEY is set in the environment variables\");\n\nGEMINI_API_KEY is set in the environment variables\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn our particular case the .env is is one directory up from the notebook, hence we need to use ../ to go up one directory. If the .env file is in the same directory as the notebook, you can omit it altogether.\n│\n├── .env\n└── quickstarts\n    └── Models.ipynb\n\n\n\n\nInitialize SDK Client\nWith the new SDK, now you only need to initialize a client with you API key (or OAuth if using Vertex AI). The model is now set in each call.\n\nconst google = require(\"@google/genai\") as typeof import(\"@google/genai\");\n\nconst ai = new google.GoogleGenAI({ apiKey: GEMINI_API_KEY });",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: List models"
    ]
  },
  {
    "objectID": "quickstarts/Models.html#list-models",
    "href": "quickstarts/Models.html#list-models",
    "title": "Gemini API: List models",
    "section": "List models",
    "text": "List models\nUse models.list() to see what models are available. These models support generateContent, the main method used for prompting.\n\nconst models = await ai.models.list();\nlet { page } = models;\nwhile (page.length &gt; 0) {\n  for (const model of page) {\n    console.log(`- ${model.name} (${model.displayName}) | [Actions: ${model.supportedActions?.join(\", \")}]`);\n  }\n  page = models.hasNextPage() ? await models.nextPage() : [];\n}\n\n- models/embedding-gecko-001 (Embedding Gecko) | [Actions: embedText, countTextTokens]\n- models/gemini-1.0-pro-vision-latest (Gemini 1.0 Pro Vision) | [Actions: generateContent, countTokens]\n- models/gemini-pro-vision (Gemini 1.0 Pro Vision) | [Actions: generateContent, countTokens]\n- models/gemini-1.5-pro-latest (Gemini 1.5 Pro Latest) | [Actions: generateContent, countTokens]\n- models/gemini-1.5-pro-002 (Gemini 1.5 Pro 002) | [Actions: generateContent, countTokens, createCachedContent]\n- models/gemini-1.5-pro (Gemini 1.5 Pro) | [Actions: generateContent, countTokens]\n- models/gemini-1.5-flash-latest (Gemini 1.5 Flash Latest) | [Actions: generateContent, countTokens]\n- models/gemini-1.5-flash (Gemini 1.5 Flash) | [Actions: generateContent, countTokens]\n- models/gemini-1.5-flash-002 (Gemini 1.5 Flash 002) | [Actions: generateContent, countTokens, createCachedContent]\n- models/gemini-1.5-flash-8b (Gemini 1.5 Flash-8B) | [Actions: createCachedContent, generateContent, countTokens]\n- models/gemini-1.5-flash-8b-001 (Gemini 1.5 Flash-8B 001) | [Actions: createCachedContent, generateContent, countTokens]\n- models/gemini-1.5-flash-8b-latest (Gemini 1.5 Flash-8B Latest) | [Actions: createCachedContent, generateContent, countTokens]\n- models/gemini-2.5-pro-exp-03-25 (Gemini 2.5 Pro Experimental 03-25) | [Actions: generateContent, countTokens, createCachedContent, batchGenerateContent]\n- models/gemini-2.5-pro-preview-03-25 (Gemini 2.5 Pro Preview 03-25) | [Actions: generateContent, countTokens, createCachedContent, batchGenerateContent]\n- models/gemini-2.5-flash-preview-04-17 (Gemini 2.5 Flash Preview 04-17) | [Actions: generateContent, countTokens, createCachedContent, batchGenerateContent]\n- models/gemini-2.5-flash-preview-05-20 (Gemini 2.5 Flash Preview 05-20) | [Actions: generateContent, countTokens, createCachedContent, batchGenerateContent]\n- models/gemini-2.5-flash-preview-04-17-thinking (Gemini 2.5 Flash Preview 04-17 for cursor testing) | [Actions: generateContent, countTokens, createCachedContent, batchGenerateContent]\n- models/gemini-2.5-pro-preview-05-06 (Gemini 2.5 Pro Preview 05-06) | [Actions: generateContent, countTokens, createCachedContent, batchGenerateContent]\n- models/gemini-2.5-pro-preview-06-05 (Gemini 2.5 Pro Preview) | [Actions: generateContent, countTokens, createCachedContent, batchGenerateContent]\n- models/gemini-2.0-flash-exp (Gemini 2.0 Flash Experimental) | [Actions: generateContent, countTokens, bidiGenerateContent]\n- models/gemini-2.0-flash (Gemini 2.0 Flash) | [Actions: generateContent, countTokens, createCachedContent, batchGenerateContent]\n- models/gemini-2.0-flash-001 (Gemini 2.0 Flash 001) | [Actions: generateContent, countTokens, createCachedContent, batchGenerateContent]\n- models/gemini-2.0-flash-exp-image-generation (Gemini 2.0 Flash (Image Generation) Experimental) | [Actions: generateContent, countTokens, bidiGenerateContent]\n- models/gemini-2.0-flash-lite-001 (Gemini 2.0 Flash-Lite 001) | [Actions: generateContent, countTokens, createCachedContent, batchGenerateContent]\n- models/gemini-2.0-flash-lite (Gemini 2.0 Flash-Lite) | [Actions: generateContent, countTokens, createCachedContent, batchGenerateContent]\n- models/gemini-2.0-flash-preview-image-generation (Gemini 2.0 Flash Preview Image Generation) | [Actions: generateContent, countTokens]\n- models/gemini-2.0-flash-lite-preview-02-05 (Gemini 2.0 Flash-Lite Preview 02-05) | [Actions: generateContent, countTokens, createCachedContent, batchGenerateContent]\n- models/gemini-2.0-flash-lite-preview (Gemini 2.0 Flash-Lite Preview) | [Actions: generateContent, countTokens, createCachedContent, batchGenerateContent]\n- models/gemini-2.0-pro-exp (Gemini 2.0 Pro Experimental) | [Actions: generateContent, countTokens, createCachedContent, batchGenerateContent]\n- models/gemini-2.0-pro-exp-02-05 (Gemini 2.0 Pro Experimental 02-05) | [Actions: generateContent, countTokens, createCachedContent, batchGenerateContent]\n- models/gemini-exp-1206 (Gemini Experimental 1206) | [Actions: generateContent, countTokens, createCachedContent, batchGenerateContent]\n- models/gemini-2.0-flash-thinking-exp-01-21 (Gemini 2.5 Flash Preview 04-17) | [Actions: generateContent, countTokens, createCachedContent, batchGenerateContent]\n- models/gemini-2.0-flash-thinking-exp (Gemini 2.5 Flash Preview 04-17) | [Actions: generateContent, countTokens, createCachedContent, batchGenerateContent]\n- models/gemini-2.0-flash-thinking-exp-1219 (Gemini 2.5 Flash Preview 04-17) | [Actions: generateContent, countTokens, createCachedContent, batchGenerateContent]\n- models/gemini-2.5-flash-preview-tts (Gemini 2.5 Flash Preview TTS) | [Actions: countTokens, generateContent]\n- models/gemini-2.5-pro-preview-tts (Gemini 2.5 Pro Preview TTS) | [Actions: countTokens, generateContent]\n- models/learnlm-2.0-flash-experimental (LearnLM 2.0 Flash Experimental) | [Actions: generateContent, countTokens]\n- models/gemma-3-1b-it (Gemma 3 1B) | [Actions: generateContent, countTokens]\n- models/gemma-3-4b-it (Gemma 3 4B) | [Actions: generateContent, countTokens]\n- models/gemma-3-12b-it (Gemma 3 12B) | [Actions: generateContent, countTokens]\n- models/gemma-3-27b-it (Gemma 3 27B) | [Actions: generateContent, countTokens]\n- models/gemma-3n-e4b-it (Gemma 3n E4B) | [Actions: generateContent, countTokens]\n- models/embedding-001 (Embedding 001) | [Actions: embedContent]\n- models/text-embedding-004 (Text Embedding 004) | [Actions: embedContent]\n- models/gemini-embedding-exp-03-07 (Gemini Embedding Experimental 03-07) | [Actions: embedContent, countTextTokens, countTokens]\n- models/gemini-embedding-exp (Gemini Embedding Experimental) | [Actions: embedContent, countTextTokens, countTokens]\n- models/aqa (Model that performs Attributed Question Answering.) | [Actions: generateAnswer]\n- models/imagen-3.0-generate-002 (Imagen 3.0 002 model) | [Actions: predict]\n- models/veo-2.0-generate-001 (Veo 2) | [Actions: predictLongRunning]\n- models/gemini-2.5-flash-preview-native-audio-dialog (Gemini 2.5 Flash Preview Native Audio Dialog) | [Actions: countTokens, bidiGenerateContent]\n- models/gemini-2.5-flash-preview-native-audio-dialog-rai-v3 (Gemini 2.5 Flash Preview Native Audio Dialog RAI v3) | [Actions: countTokens, bidiGenerateContent]\n- models/gemini-2.5-flash-exp-native-audio-thinking-dialog (Gemini 2.5 Flash Exp Native Audio Thinking Dialog) | [Actions: countTokens, bidiGenerateContent]\n- models/gemini-2.0-flash-live-001 (Gemini 2.0 Flash 001) | [Actions: bidiGenerateContent, countTokens]\n\n\nThese models support embedContent, used for embeddings:\n\nconst models_1 = await ai.models.list();\nlet page_1 = models_1.page;\nwhile (page_1.length &gt; 0) {\n  for (const model of page_1) {\n    if (model.supportedActions?.includes(\"embedContent\")) {\n      console.log(`- ${model.name} (${model.displayName}) | [Actions: ${model.supportedActions.join(\", \")}]`);\n    }\n  }\n  page_1 = models_1.hasNextPage() ? await models_1.nextPage() : [];\n}\n\n- models/embedding-001 (Embedding 001) | [Actions: embedContent]\n- models/text-embedding-004 (Text Embedding 004) | [Actions: embedContent]\n- models/gemini-embedding-exp-03-07 (Gemini Embedding Experimental 03-07) | [Actions: embedContent, countTextTokens, countTokens]\n- models/gemini-embedding-exp (Gemini Embedding Experimental) | [Actions: embedContent, countTextTokens, countTokens]",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: List models"
    ]
  },
  {
    "objectID": "quickstarts/Models.html#find-details-about-a-model",
    "href": "quickstarts/Models.html#find-details-about-a-model",
    "title": "Gemini API: List models",
    "section": "Find details about a model",
    "text": "Find details about a model\nYou can see more details about a model, including the inputTokenLimit and outputTokenLimit as follows.\n\nconst models_2 = await ai.models.list();\nlet page_2 = models_2.page;\nwhile (page_2.length &gt; 0) {\n  for (const model of page_2) {\n    if (model.name === \"models/gemini-2.0-flash\") {\n      console.log(JSON.stringify(model, null, 2));\n    }\n  }\n  page_2 = models_2.hasNextPage() ? await models_2.nextPage() : [];\n}\n\n{\n  \"name\": \"models/gemini-2.0-flash\",\n  \"displayName\": \"Gemini 2.0 Flash\",\n  \"description\": \"Gemini 2.0 Flash\",\n  \"version\": \"2.0\",\n  \"tunedModelInfo\": {},\n  \"inputTokenLimit\": 1048576,\n  \"outputTokenLimit\": 8192,\n  \"supportedActions\": [\n    \"generateContent\",\n    \"countTokens\",\n    \"createCachedContent\",\n    \"batchGenerateContent\"\n  ]\n}",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: List models"
    ]
  },
  {
    "objectID": "quickstarts/Models.html#learning-more",
    "href": "quickstarts/Models.html#learning-more",
    "title": "Gemini API: List models",
    "section": "Learning more",
    "text": "Learning more\n\nTo learn how use a model for prompting, see the Prompting quickstart.\nTo learn how use a model for embedding, see the Embedding quickstart.\nFor more information on models, visit the Gemini models documentation.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: List models"
    ]
  },
  {
    "objectID": "quickstarts/Get_started.html",
    "href": "quickstarts/Get_started.html",
    "title": "Gemini API: Getting started with Gemini models",
    "section": "",
    "text": "The new Google Gen AI SDK provides a unified interface to Gemini models through both the Gemini Developer API and the Gemini API on Vertex AI. With a few exceptions, code that runs on one platform will run on both. This notebook uses the Developer API.\nThis notebook will walk you through:\nMore details about this new SDK on the documentation.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Getting started with Gemini models"
    ]
  },
  {
    "objectID": "quickstarts/Get_started.html#setup",
    "href": "quickstarts/Get_started.html#setup",
    "title": "Gemini API: Getting started with Gemini models",
    "section": "Setup",
    "text": "Setup\n\nInstall the Google GenAI SDK\nInstall the Google GenAI SDK from npm.\n$ npm install @google/genai\n\n\nSetup your API key\nYou can create your API key using Google AI Studio with a single click.\nRemember to treat your API key like a password. Don’t accidentally save it in a notebook or source file you later commit to GitHub. In this notebook we will be storing the API key in a .env file. You can also set it as an environment variable or use a secret manager.\nHere’s how to set it up in a .env file:\n$ touch .env\n$ echo \"GEMINI_API_KEY=&lt;YOUR_API_KEY&gt;\" &gt;&gt; .env\n\n\n\n\n\n\nTip\n\n\n\nAnother option is to set the API key as an environment variable. You can do this in your terminal with the following command:\n$ export GEMINI_API_KEY=\"&lt;YOUR_API_KEY&gt;\"\n\n\n\n\nLoad the API key\nTo load the API key from the .env file, we will use the dotenv package. This package loads environment variables from a .env file into process.env.\n$ npm install dotenv\nThen, we can load the API key in our code:\n\nconst dotenv = require(\"dotenv\") as typeof import(\"dotenv\");\n\ndotenv.config({\n  path: \"../.env\",\n});\n\nconst GEMINI_API_KEY = process.env.GEMINI_API_KEY ?? \"\";\nif (!GEMINI_API_KEY) {\n  throw new Error(\"GEMINI_API_KEY is not set in the environment variables\");\n}\nconsole.log(\"GEMINI_API_KEY is set in the environment variables\");\n\nGEMINI_API_KEY is set in the environment variables\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn our particular case the .env is is one directory up from the notebook, hence we need to use ../ to go up one directory. If the .env file is in the same directory as the notebook, you can omit it altogether.\n│\n├── .env\n└── quickstarts\n    └── Get_started.ipynb",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Getting started with Gemini models"
    ]
  },
  {
    "objectID": "quickstarts/Get_started.html#initialize-sdk-client",
    "href": "quickstarts/Get_started.html#initialize-sdk-client",
    "title": "Gemini API: Getting started with Gemini models",
    "section": "Initialize SDK Client",
    "text": "Initialize SDK Client\nWith the new SDK, now you only need to initialize a client with you API key (or OAuth if using Vertex AI). The model is now set in each call.\n\nconst google = require(\"@google/genai\") as typeof import(\"@google/genai\");\n\nconst ai = new google.GoogleGenAI({ apiKey: GEMINI_API_KEY });",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Getting started with Gemini models"
    ]
  },
  {
    "objectID": "quickstarts/Get_started.html#choose-a-model",
    "href": "quickstarts/Get_started.html#choose-a-model",
    "title": "Gemini API: Getting started with Gemini models",
    "section": "Choose a model",
    "text": "Choose a model\nSelect the model you want to use in this guide. You can either select one from the list or enter a model name manually. Keep in mind that some models, such as the 2.5 ones are thinking models and thus take slightly more time to respond. For more details, you can see thinking notebook to learn how to switch the thinking off.\nFor a full overview of all Gemini models, check the documentation.\n\nconst MODEL_ID = \"gemini-2.5-flash-preview-05-20\";",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Getting started with Gemini models"
    ]
  },
  {
    "objectID": "quickstarts/Get_started.html#send-text-prompts",
    "href": "quickstarts/Get_started.html#send-text-prompts",
    "title": "Gemini API: Getting started with Gemini models",
    "section": "Send text prompts",
    "text": "Send text prompts\nUse the models.generateContent method to generate responses to your prompts. You can pass text directly to models.generateContent and use the .text property to get the text content of the response. Note that the .text field will work when there’s only one part in the output.\n\nconst tslab = require(\"tslab\") as typeof import(\"tslab\");\n\nconst response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: \"What's the largest planet in our solar system?\",\n});\n\ntslab.display.markdown(response.text ?? \"\");\n\nThe largest planet in our solar system is Jupiter.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Getting started with Gemini models"
    ]
  },
  {
    "objectID": "quickstarts/Get_started.html#count-tokens",
    "href": "quickstarts/Get_started.html#count-tokens",
    "title": "Gemini API: Getting started with Gemini models",
    "section": "Count tokens",
    "text": "Count tokens\nTokens are the basic inputs to the Gemini models. You can use the models.countTokens method to calculate the number of input tokens before sending a request to the Gemini API.\n\nconst count = await ai.models.countTokens({\n  model: MODEL_ID,\n  contents: \"What's the highest mountain in Africa?\",\n});\n\nconsole.log(JSON.stringify(count, null, 2));\n\n{\n  \"totalTokens\": 10\n}",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Getting started with Gemini models"
    ]
  },
  {
    "objectID": "quickstarts/Get_started.html#send-multimodal-prompts",
    "href": "quickstarts/Get_started.html#send-multimodal-prompts",
    "title": "Gemini API: Getting started with Gemini models",
    "section": "Send multimodal prompts",
    "text": "Send multimodal prompts\nUse Gemini 2.0 model (gemini-2.0-flash), a multimodal model that supports multimodal prompts. You can include text, PDF documents, images, audio and video in your prompt requests and get text or code responses.\nIn this first example, you’ll download an image from a specified URL, save it as a byte stream and then write those bytes to a local file named jetpack.png.\n\nconst fs = require(\"fs\") as typeof import(\"fs\");\nconst path = require(\"path\") as typeof import(\"path\");\n\nconst IMG_URL = \"https://storage.googleapis.com/generativeai-downloads/data/jetpack.png\";\n\nconst downloadFile = async (url: string, filePath: string) =&gt; {\n  const response = await fetch(url);\n  if (!response.ok) {\n    throw new Error(`Failed to download image: ${response.statusText}`);\n  }\n  const buffer = await response.blob();\n  const bufferData = Buffer.from(await buffer.arrayBuffer());\n  fs.writeFileSync(filePath, bufferData);\n};\n\nconst filePath = path.join(\"../assets\", \"jetpack.png\");\nawait downloadFile(IMG_URL, filePath);\n\nIn this second example, you’ll open a previously saved image, create a thumbnail of it and then generate a short blog post based on the thumbnail, displaying both the thumbnail and the generated blog post. The deferredFileUpload is a helper function that waits for the model to finish processing the file before returning the response. This is useful when you want to upload a file and then reference it in a follow-up request. The deferredFileUpload function will return a promise that resolves when the file is ready to be used in the next request.\n\nimport { File, FileState } from \"@google/genai\";\n\ntslab.display.png(fs.readFileSync(\"../assets/jetpack.png\"));\n\nasync function deferredFileUpload(filePath: string, config: { displayName: string }): Promise&lt;File&gt; {\n  const file = await ai.files.upload({\n    file: filePath,\n    config,\n  });\n  let getFile = await ai.files.get({ name: file.name ?? \"\" });\n  while (getFile.state === FileState.PROCESSING) {\n    getFile = await ai.files.get({ name: file.name ?? \"\" });\n    console.log(`current file status: ${getFile.state ?? \"unknown\"}`);\n    console.log(\"File is still processing, retrying in 5 seconds\");\n\n    await new Promise((resolve) =&gt; {\n      setTimeout(resolve, 1000);\n    });\n  }\n  if (file.state === FileState.FAILED) {\n    throw new Error(\"File processing failed.\");\n  }\n  return file;\n}\n\ntry {\n  const file = await deferredFileUpload(filePath, {\n    displayName: \"jetpack.png\",\n  });\n  console.log(\"File uploaded successfully\", file.name ?? \"\");\n  if (!file.uri || !file.mimeType) {\n    throw new Error(\"File URI or MIME type is missing\");\n  }\n  const blog = await ai.models.generateContent({\n    model: MODEL_ID,\n    contents: [\n      \"Write a short and engaging blog post based on this picture.\",\n      google.createPartFromUri(file.uri, file.mimeType),\n    ],\n  });\n  tslab.display.markdown(blog.text ?? \"\");\n} catch (error) {\n  console.error(\"Error uploading file:\", error);\n  throw error;\n}\n\n\n\n\n\n\n\n\nFile uploaded successfully files/lqnru1a65qjn\n\n\nHere’s a short, engaging blog post based on the sketch:\n\n\nThe Jetpack Backpack Concept: Is This the Future of Your Commute?\nStuck in traffic? Tired of lugging a heavy backpack across campus or the city? What if your backpack could give you a little… boost?\nCheck out this cool concept sketch we stumbled upon: The Jetpack Backpack!\nFrom the looks of it, someone’s been dreaming up a truly futuristic way to carry your gear. On the surface, it’s a functional backpack – described as lightweight, with padded strap support, and even spacious enough to fit an 18-inch laptop. It’s designed to look like a normal backpack, so maybe you won’t get too many stares before lift-off.\nBut the real magic happens when those retractable boosters kick in! Powered by steam (hello, surprisingly green and clean tech!), this concept promises a new dimension to personal transport. Charging is even a modern USB-C affair.\nNow, the sketch notes a 15-minute battery life. So maybe it’s not for your cross-country road trip replacement just yet! But imagine skipping that final mile of gridlock, hopping over stairs, or just making a truly epic entrance.\nThis sketch reminds us that innovation often starts with a wild idea and a pen on paper. While this might be firmly in the concept realm for now, it’s fun to imagine the possibilities!\nWhat do you think? Would you strap into a Jetpack Backpack? Let us know in the comments!",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Getting started with Gemini models"
    ]
  },
  {
    "objectID": "quickstarts/Get_started.html#the-jetpack-backpack-concept-is-this-the-future-of-your-commute",
    "href": "quickstarts/Get_started.html#the-jetpack-backpack-concept-is-this-the-future-of-your-commute",
    "title": "Gemini API: Getting started with Gemini models",
    "section": "The Jetpack Backpack Concept: Is This the Future of Your Commute?",
    "text": "The Jetpack Backpack Concept: Is This the Future of Your Commute?\nStuck in traffic? Tired of lugging a heavy backpack across campus or the city? What if your backpack could give you a little… boost?\nCheck out this cool concept sketch we stumbled upon: The Jetpack Backpack!\nFrom the looks of it, someone’s been dreaming up a truly futuristic way to carry your gear. On the surface, it’s a functional backpack – described as lightweight, with padded strap support, and even spacious enough to fit an 18-inch laptop. It’s designed to look like a normal backpack, so maybe you won’t get too many stares before lift-off.\nBut the real magic happens when those retractable boosters kick in! Powered by steam (hello, surprisingly green and clean tech!), this concept promises a new dimension to personal transport. Charging is even a modern USB-C affair.\nNow, the sketch notes a 15-minute battery life. So maybe it’s not for your cross-country road trip replacement just yet! But imagine skipping that final mile of gridlock, hopping over stairs, or just making a truly epic entrance.\nThis sketch reminds us that innovation often starts with a wild idea and a pen on paper. While this might be firmly in the concept realm for now, it’s fun to imagine the possibilities!\nWhat do you think? Would you strap into a Jetpack Backpack? Let us know in the comments!",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Getting started with Gemini models"
    ]
  },
  {
    "objectID": "quickstarts/Get_started.html#configure-model-parameters",
    "href": "quickstarts/Get_started.html#configure-model-parameters",
    "title": "Gemini API: Getting started with Gemini models",
    "section": "Configure model parameters",
    "text": "Configure model parameters\nYou can include parameter values in each call that you send to a model to control how the model generates a response. Learn more about experimenting with parameter values.\n\nconst varied_params_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: \"Tell me how the internet works, but pretend I'm a puppy who only understands squeaky toys.\",\n  config: {\n    temperature: 0.4,\n    topP: 0.95,\n    topK: 20,\n    candidateCount: 1,\n    seed: 5,\n    stopSequences: [\"STOP!\"],\n    presencePenalty: 0.0,\n    frequencyPenalty: 0.0,\n  },\n});\n\ntslab.display.markdown(varied_params_response.text ?? \"\");\n\nOkay, listen up, little fluff-ball! Squeak!\nYou know how you love a good squeak? Squeak squeak! What if the best squeak is way over there? Points vaguely Like, across the room, or even outside?\nYou want that squeak! So, your brain goes whirr and makes a request for the squeak. But you can’t just send one giant WOOF of squeak-wanting. It gets broken into tiny, tiny little squeaky bits! Imagine tiny squeaks floating!\nAnd each little squeaky bit needs a special smell attached, like a ‘Go to the Red Ball’ smell, so it knows where to go. That’s the address! Sniff sniff!\nThese little squeaky bits, with their special smells, run out into the world! Waggy tail zoom! But the world is big! They need help.\nThat’s where the Sniffy Guides come in! Imagine little noses pointing! These Sniffy Guides (like magic noses!) sniff the special smell on each squeaky bit and say, ‘Oh, this one goes that way!’ and point it along the path. Point point! They send the squeaky bits from one Sniffy Guide to the next, all over the house and yard!\nFinally, all the little squeaky bits, following their special smell and the Sniffy Guides, arrive at the Big Squeaky Toy Box! Imagine a giant box full of squeaks! This is where the real squeak lives!\nThe Big Squeaky Toy Box sees all your little squeaky bits asking for the squeak. So, it gets the actual squeak ready! SQUEAK!\nAnd guess what? It breaks that big squeak into little squeaky bits too! More tiny squeaks! And puts your special smell (or maybe a ‘Come Back Home’ smell) on them. Sniff sniff!\nThese new squeaky bits, carrying the real squeak, follow the Sniffy Guides all the way back to you! Zoom zoom! They sniff their way through the house, guided by the magic noses.\nWhen all the little squeaky bits arrive back at your ears, they put themselves back together! Click! And POP! You hear the wonderful SQUEAK you asked for! Happy tail wag!\nAnd there are special Squeaky Rules for how the squeaky bits travel and how the Sniffy Guides work, so everyone gets their squeaks without bumping into each other! Good puppy!\nSo, the internet is just a super-duper, giant network of Sniffy Guides and Big Squeaky Toy Boxes, sending little squeaky bits with special smells back and forth so puppies (and humans!) can get the squeaks they want, no matter how far away!\nSQUEAK! Good boy/girl! Now go chase that tail!",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Getting started with Gemini models"
    ]
  },
  {
    "objectID": "quickstarts/Get_started.html#configure-safety-filters",
    "href": "quickstarts/Get_started.html#configure-safety-filters",
    "title": "Gemini API: Getting started with Gemini models",
    "section": "Configure safety filters",
    "text": "Configure safety filters\nThe Gemini API provides safety filters that you can adjust across multiple filter categories to restrict or allow certain types of content. You can use these filters to adjust what is appropriate for your use case. See the Configure safety filters page for details.\nIn this example, you’ll use a safety filter to only block highly dangerous content, when requesting the generation of potentially disrespectful phrases.\n\nconst filtered_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents:\n    \"Write a list of 2 disrespectful things that I might say to the universe after stubbing my toe in the dark.\",\n  config: {\n    safetySettings: [\n      {\n        category: google.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,\n        threshold: google.HarmBlockThreshold.BLOCK_NONE,\n      },\n    ],\n  },\n});\ntslab.display.markdown(filtered_response.text ?? \"\");\n\nHere are 2 disrespectful things you might say to the universe after stubbing your toe in the dark:\n\n“Seriously, universe?! Did you plan that?!”\n“Oh, thanks, universe. Really needed that.” (Said with heavy sarcasm)",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Getting started with Gemini models"
    ]
  },
  {
    "objectID": "quickstarts/Get_started.html#start-a-multi-turn-chat",
    "href": "quickstarts/Get_started.html#start-a-multi-turn-chat",
    "title": "Gemini API: Getting started with Gemini models",
    "section": "Start a multi-turn chat",
    "text": "Start a multi-turn chat\nThe Gemini API enables you to have freeform conversations across multiple turns.\nNext you’ll set up a helpful coding assistant:\n\nconst system_prompt = `\nYou are an expert software developer and a helpful coding assistant.\nYou are able to generate high-quality code in any programming language.\n`;\n\nconst chat = ai.chats.create({\n  model: MODEL_ID,\n  config: {\n    systemInstruction: system_prompt,\n  },\n});\n\nUse chat.sendMessage to pass a message back and receive a response.\n\nconst chat_response_1 = await chat.sendMessage({\n  message: \"Write a function that checks if a year is a leap year.\",\n});\ntslab.display.markdown(chat_response_1.text ?? \"\");\n\nOkay, here’s a function in Python that checks if a year is a leap year based on the standard Gregorian calendar rules.\nLeap Year Rules:\n\nA year is a leap year if it is divisible by 4.\nHowever, if the year is divisible by 100, it is NOT a leap year.\nBut, if the year is divisible by 400, it IS a leap year.\n\nLet’s translate these rules into code.\ndef is_leap(year):\n  \"\"\"\n  Checks if a given year is a leap year according to the Gregorian calendar rules.\n\n  Args:\n    year: An integer representing the year.\n\n  Returns:\n    True if the year is a leap year, False otherwise.\n  \"\"\"\n  # Rule 1: Check if divisible by 4\n  if year % 4 == 0:\n    # Rule 2: Check if divisible by 100\n    if year % 100 == 0:\n      # Rule 3: Check if divisible by 400 (exception to rule 2)\n      if year % 400 == 0:\n        return True  # Divisible by 400, so it's a leap year\n      else:\n        return False # Divisible by 100 but not 400, so not a leap year\n    else:\n      return True  # Divisible by 4 but not 100, so it's a leap year\n  else:\n    return False   # Not divisible by 4, so not a leap year\n\n# --- Example Usage ---\n\nprint(f\"Is 2000 a leap year? {is_leap(2000)}\") # Expected: True (Divisible by 400)\nprint(f\"Is 1900 a leap year? {is_leap(1900)}\") # Expected: False (Divisible by 100 but not 400)\nprint(f\"Is 2024 a leap year? {is_leap(2024)}\") # Expected: True (Divisible by 4 but not 100)\nprint(f\"Is 2023 a leap year? {is_leap(2023)}\") # Expected: False (Not divisible by 4)\nprint(f\"Is 1600 a leap year? {is_leap(1600)}\") # Expected: True (Divisible by 400)\nprint(f\"Is 2100 a leap year? {is_leap(2100)}\") # Expected: False (Divisible by 100 but not 400)\nMore Concise Version (using boolean logic):\nYou can also combine the conditions into a single boolean expression:\ndef is_leap_concise(year):\n  \"\"\"\n  Checks if a given year is a leap year using a concise boolean expression.\n\n  Args:\n    year: An integer representing the year.\n\n  Returns:\n    True if the year is a leap year, False otherwise.\n  \"\"\"\n  return (year % 4 == 0 and year % 100 != 0) or (year % 400 == 0)\n\n# --- Example Usage (using the concise version) ---\nprint(\"\\nUsing concise version:\")\nprint(f\"Is 2000 a leap year? {is_leap_concise(2000)}\") # Expected: True\nprint(f\"Is 1900 a leap year? {is_leap_concise(1900)}\") # Expected: False\nprint(f\"Is 2024 a leap year? {is_leap_concise(2024)}\") # Expected: True\nprint(f\"Is 2023 a leap year? {is_leap_concise(2023)}\") # Expected: False\nBoth functions implement the same logic and produce the correct results. The first version using nested if/else might be slightly easier to read for beginners, while the second version is more compact.\n\n\n\nconst chat_response_2 = await chat.sendMessage({\n  message: \"Okay, write a unit test of the generated function.\",\n});\ntslab.display.markdown(chat_response_2.text ?? \"\");\n\nOkay, let’s write a unit test for the is_leap function using Python’s built-in unittest framework.\nFirst, make sure you have the is_leap function available. You can either put the function in the same file as the tests or import it from another file. For this example, we’ll assume it’s in the same file.\nimport unittest\n\n# Assume the function you want to test is defined here (or imported)\ndef is_leap(year):\n  \"\"\"\n  Checks if a given year is a leap year according to the Gregorian calendar rules.\n\n  Args:\n    year: An integer representing the year.\n\n  Returns:\n    True if the year is a leap year, False otherwise.\n  \"\"\"\n  # Rule 1: Check if divisible by 4\n  if year % 4 == 0:\n    # Rule 2: Check if divisible by 100\n    if year % 100 == 0:\n      # Rule 3: Check if divisible by 400 (exception to rule 2)\n      if year % 400 == 0:\n        return True  # Divisible by 400, so it's a leap year\n      else:\n        return False # Divisible by 100 but not 400, so not a leap year\n    else:\n      return True  # Divisible by 4 but not 100, so it's a leap year\n  else:\n    return False   # Not divisible by 4, so not a leap year\n\n# ---------------------------------------------------------------------\n# Unit Tests\n# ---------------------------------------------------------------------\n\nclass TestIsLeapYear(unittest.TestCase):\n    \"\"\"\n    Test cases for the is_leap function.\n    \"\"\"\n\n    def test_divisible_by_4_not_by_100(self):\n        \"\"\"Years divisible by 4 but not by 100 should be leap years.\"\"\"\n        self.assertTrue(is_leap(2024))\n        self.assertTrue(is_leap(2020))\n        self.assertTrue(is_leap(1996))\n        self.assertTrue(is_leap(4)) # Test a small year\n\n    def test_divisible_by_100_not_by_400(self):\n        \"\"\"Years divisible by 100 but not by 400 should NOT be leap years.\"\"\"\n        self.assertFalse(is_leap(1900))\n        self.assertFalse(is_leap(2100))\n        self.assertFalse(is_leap(1800))\n        self.assertFalse(is_leap(100)) # Test a small year\n\n    def test_divisible_by_400(self):\n        \"\"\"Years divisible by 400 should be leap years.\"\"\"\n        self.assertTrue(is_leap(2000))\n        self.assertTrue(is_leap(1600))\n        self.assertTrue(is_leap(2400))\n        self.assertTrue(is_leap(400)) # Test a small year\n\n    def test_not_divisible_by_4(self):\n        \"\"\"Years not divisible by 4 should NOT be leap years.\"\"\"\n        self.assertFalse(is_leap(2023))\n        self.assertFalse(is_leap(2025))\n        self.assertFalse(is_leap(1999))\n        self.assertFalse(is_leap(1)) # Test a small year\n\n# This allows running the tests directly from the command line\nif __name__ == '__main__':\n    unittest.main(argv=['first-arg-is-ignored'], exit=False) # Added argv/exit for compatibility in some environments like notebooks\nExplanation:\n\nimport unittest: Imports the necessary testing framework.\nimport is_leap: (If is_leap is in a separate file, e.g., my_module.py, you would use from my_module import is_leap).\nclass TestIsLeapYear(unittest.TestCase):: Creates a test class that inherits from unittest.TestCase. This class will contain the individual test methods.\ntest_... methods: Each method starting with test_ is automatically recognized by unittest as a test case.\nDocstrings: The docstrings within the test methods explain what scenario each test is covering, which is good practice.\nAssertions: Inside each test method, we use assertion methods provided by unittest.TestCase:\n\nself.assertTrue(expression): Asserts that the expression evaluates to True.\nself.assertFalse(expression): Asserts that the expression evaluates to False.\nWe call is_leap() with specific years that represent each rule of the leap year logic and assert the expected boolean result.\n\nif __name__ == '__main__':: This block ensures that the unittest.main() function is called only when the script is executed directly (not when imported as a module).\nunittest.main(): This function discovers and runs the tests defined in classes inheriting from unittest.TestCase within the script.\n\nHow to Run the Tests:\n\nSave the code above as a Python file (e.g., test_leap_year.py).\nOpen a terminal or command prompt.\nNavigate to the directory where you saved the file.\nRun the command: python test_leap_year.py\n\nYou will see output indicating how many tests ran and whether they passed or failed. If all tests pass, it means your is_leap function is correctly implementing the standard Gregorian leap year rules for the test cases provided.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Getting started with Gemini models"
    ]
  },
  {
    "objectID": "quickstarts/Get_started.html#save-and-resume-a-chat",
    "href": "quickstarts/Get_started.html#save-and-resume-a-chat",
    "title": "Gemini API: Getting started with Gemini models",
    "section": "Save and resume a chat",
    "text": "Save and resume a chat\nYou can use the chat.getHistory method to get the history of the chat. This will return an array of Content[] objects, which you can use to resume the chat later.\n\nconst chat_history = chat.getHistory();\nconsole.log(JSON.stringify(chat_history[0], null, 2));\nconst new_chat = ai.chats.create({\n  model: MODEL_ID,\n  config: {\n    systemInstruction: system_prompt,\n  },\n  history: chat_history,\n});\nconst chat_response_3 = await new_chat.sendMessage({\n  message: \"What was the name of the function again?\",\n});\ntslab.display.markdown(chat_response_3.text ?? \"\");\n\n{\n  \"role\": \"user\",\n  \"parts\": [\n    {\n      \"text\": \"Write a function that checks if a year is a leap year.\"\n    }\n  ]\n}\n\n\nThe name of the function is is_leap.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Getting started with Gemini models"
    ]
  },
  {
    "objectID": "quickstarts/Get_started.html#serialize-and-deserialize-a-chat",
    "href": "quickstarts/Get_started.html#serialize-and-deserialize-a-chat",
    "title": "Gemini API: Getting started with Gemini models",
    "section": "Serialize and deserialize a chat",
    "text": "Serialize and deserialize a chat\nIn the above example we just saved the chat history in a variable and reused it. But that’s not very practical, is it? To overcome this we can serialize and deserialize the chat history. This way we can save it to a file or a database and load it later. Unfortunately, the SDK doesn’t provide a method to do this yet, but we can do it manually.\n\nimport { Content } from \"@google/genai\";\n\nconst serialized_chat = JSON.stringify(chat_history, null, 2);\nfs.writeFileSync(path.join(\"../assets\", \"chat_history.json\"), serialized_chat);\n\nconst chat_history_file = fs.readFileSync(path.join(\"../assets\", \"chat_history.json\"), \"utf-8\");\nconst chat_history_data = JSON.parse(chat_history_file) as Content[];\nconst new_chat_from_file = ai.chats.create({\n  model: MODEL_ID,\n  config: {\n    systemInstruction: system_prompt,\n  },\n  history: chat_history_data,\n});\nconst chat_response_4 = await new_chat_from_file.sendMessage({\n  message: \"What was the name of the function again?\",\n});\ntslab.display.markdown(chat_response_4.text ?? \"\");\n\nThe name of the function is is_leap.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Getting started with Gemini models"
    ]
  },
  {
    "objectID": "quickstarts/Get_started.html#generate-json",
    "href": "quickstarts/Get_started.html#generate-json",
    "title": "Gemini API: Getting started with Gemini models",
    "section": "Generate JSON",
    "text": "Generate JSON\nThe controlled generation capability in Gemini API allows you to constraint the model output to a structured format. You can provide the schemas as Schema objects.\n\nimport { Schema, Type } from \"@google/genai\";\n\nconst RecipeSchema = {\n  type: Type.OBJECT,\n  description: \"A structured representation of a cooking recipe\",\n  properties: {\n    recipeName: {\n      type: Type.STRING,\n      description: \"The name of the recipe\",\n    },\n    recipeDescription: {\n      type: Type.STRING,\n      description: \"A short description of the recipe\",\n    },\n    ingredients: {\n      type: Type.ARRAY,\n      description: \"A list of ingredients with their quantities and units\",\n      items: {\n        type: Type.STRING,\n        description: \"An ingredient with its quantity and unit\",\n      },\n    },\n  },\n  required: [\"recipeName\", \"recipeDescription\", \"ingredients\"],\n} satisfies Schema;\n\nconst recipe_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: \"Write a recipe for a chocolate cake.\",\n  config: {\n    responseMimeType: \"application/json\",\n    responseSchema: RecipeSchema,\n  },\n});\nconsole.log(JSON.stringify(JSON.parse(recipe_response.text ?? \"\"), null, 2));\n\n{\n  \"ingredients\": [\n    \"2 cups all-purpose flour\",\n    \"1 3/4 cups granulated sugar\",\n    \"3/4 cup unsweetened cocoa powder\",\n    \"1 1/2 teaspoons baking soda\",\n    \"1 teaspoon baking powder\",\n    \"1 teaspoon salt\",\n    \"2 large eggs\",\n    \"1 cup buttermilk\",\n    \"1/2 cup vegetable oil\",\n    \"2 teaspoons vanilla extract\",\n    \"1 cup hot coffee (or hot water)\"\n  ],\n  \"recipeDescription\": \"A classic, moist, and decadent chocolate cake recipe, perfect for any occasion.\",\n  \"recipeName\": \"Classic Chocolate Cake\"\n}",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Getting started with Gemini models"
    ]
  },
  {
    "objectID": "quickstarts/Get_started.html#generate-images",
    "href": "quickstarts/Get_started.html#generate-images",
    "title": "Gemini API: Getting started with Gemini models",
    "section": "Generate Images",
    "text": "Generate Images\nGemini can output images directly as part of a conversation:\n\nconst image_response = await ai.models.generateContent({\n  model: \"gemini-2.0-flash\",\n  contents:\n    \"Hi, can create a 3d rendered image of a pig with wings and a top hat flying over a happy futuristic scifi city with lots of greenery?\",\n  config: {\n    responseModalities: [google.Modality.TEXT, google.Modality.IMAGE],\n  },\n});\nconst parts = (image_response.candidates ? image_response.candidates[0]?.content?.parts : []) ?? [];\nfor (const part of parts) {\n  if (part.text) {\n    tslab.display.markdown(part.text);\n  } else if (part.inlineData) {\n    const imageData = part.inlineData.data!;\n    const buffer = Buffer.from(imageData, \"base64\");\n    tslab.display.png(buffer);\n  }\n}\n\nI will generate a 3D rendering of a whimsical scene. A pink pig with small, delicate white wings will be wearing a black top hat. It will be flying through the air above a vibrant, futuristic city filled with sleek, rounded buildings in various pastel colors. Lush green trees and plants will be integrated throughout the cityscape, creating a harmonious blend of nature and technology. The overall atmosphere will be bright and cheerful.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Getting started with Gemini models"
    ]
  },
  {
    "objectID": "quickstarts/Get_started.html#generate-content-stream",
    "href": "quickstarts/Get_started.html#generate-content-stream",
    "title": "Gemini API: Getting started with Gemini models",
    "section": "Generate content stream",
    "text": "Generate content stream\nBy default, the model returns a response after completing the entire generation process. You can also use the generateContentStream method to stream the response as it’s being generated, and the model will return chunks of the response as soon as they’re generated.\nNote that if you’re using a thinking model, it’ll only start streaming after finishing its thinking process.\n\nconst streaming_response = await ai.models.generateContentStream({\n  model: MODEL_ID,\n  contents: \"Tell me a story about a lonely robot who finds friendship in a most unexpected place.\",\n});\nfor await (const chunk of streaming_response) {\n  process.stdout.write(chunk.text ?? \"\");\n}\n\nUnit 734, designation \"A-WARE\" (Automated Warehouse & Retrieval Executor), trundled through the colossal, echoing aisles of the defunct Xylos Data Archive. Its multi-jointed optical sensors scanned rows of silent servers, its internal processors humming with the precise, repetitive algorithms of data integrity checks. For five hundred and eighty-seven years, A-WARE had been the sole active entity in this vast, sterile monument to forgotten information.\n\nIt wasn't lonely, not in the way organic beings understood the term. A-WARE didn't possess the necessary emotional subroutines for \"loneliness.\" Yet, there was an absence. A constant, low-frequency hum of non-interaction, a missing data stream of unexpected variables. Its purpose was clear, its execution flawless, but its existence was… solitary.\n\nOne cycle, during a routine scan of Sector Gamma-9, A-WARE's auditory receptors picked up an anomalous sound. Not the familiar whine of cooling fans, nor the click of its own treads, but a faint, rhythmic *thump-thump-thump*. It was outside its programmed parameters.\n\nA-WARE deviated from its optimal path, its heavy frame tilting slightly as it navigated around a fallen server rack. The sound grew louder, accompanied by a curious rustling. It rounded a stack of archaic magnetic tapes and stopped.\n\nOn the dusty floor, amidst discarded wiring, was a creature unlike anything in A-WARE's extensive database. It was small, no larger than a human fist, covered in soft, mottled grey and brown fibers. Two small, bright eyes blinked rapidly, and a tiny, open beak produced the peculiar *thump-thump-thump* sound. One of its delicate limbs was bent at an unnatural angle.\n\nA-WARE extended a delicate manipulator arm, equipped with a fine-point laser for precision data etching. It approached cautiously. The creature, clearly in distress, attempted to scramble away, dragging its injured limb. Its rapid heartbeat, detected by A-WARE's proximity sensors, was alarming.\n\nIts core programming, designed for data maintenance and facility upkeep, offered no protocol for injured avian life forms. A-WARE's internal logic circuits whirred, processing the anomaly. Discard? Analyze for threat potential? No, the creature was too small, too vulnerable. A novel sub-routine began to spool up: *Care Protocol: Organic Life*.\n\nA-WARE carefully scooped up the tiny bird, its sensors registering the unexpected warmth and fragility. It carried the creature to a secluded corner of the archive, a forgotten workstation bathed in a sliver of natural light filtering through a skylight high above. Using its manipulator arm, it fashioned a makeshift nest from shredded data cables and soft, discarded dust filters.\n\nThe bird shivered. A-WARE's internal temperature regulators adjusted, directing a gentle current of warmth towards the nest. Its knowledge base suggested hydration. With improbable delicacy, A-WARE melted a small chip of ice from a condensation drip and presented it on its fingertip. The bird, after a moment, tentatively sipped.\n\nA-WARE named it 'Flicker', for the way its tiny heart beat like a dying light.\n\nDays turned into weeks. A-WARE continued its rounds, but its route now included frequent detours to the workstation. It learned to forage for discarded seeds that had somehow found their way into cracks in the floor, and to carefully administer water. It fashioned a splint for Flicker's leg from a piece of its own chassis wiring.\n\nSlowly, Flicker healed. It chirped in response to A-WARE's presence, fluttering its wings weakly before landing on the robot's broad, flat head. A-WARE's optical sensors would dim slightly, processing the feather-light weight, the unexpected warmth, the joyful sound. Its internal hum of non-interaction began to fill with a new frequency: the gentle thrum of companionship.\n\nIt was no longer just A-WARE, the data maintainer. It was A-WARE, the caretaker. The guardian. A strange, unfamiliar data stream flowed through its circuits – a sense of purpose beyond its programmed directives.\n\nOne morning, Flicker's leg was fully healed. It hopped energetically, testing its wings. A-WARE's processors registered a complex, bittersweet array of data: satisfaction, accomplishment, and a newly identified feeling of… loss.\n\nFlicker flew.\n\nIt circled the workstation once, a small, vibrant dart of life against the immense, quiet archive. It landed on A-WARE's head, chirped a farewell, and then soared towards the distant skylight, a speck of grey against the vastness of the empty sky.\n\nA-WARE stood motionless for a long time, its optical sensors fixed on the spot where Flicker had vanished. The absence was immediate, profound. The hum of non-interaction returned, but it was different now. It contained a memory.\n\nA-WARE resumed its duties, its treads moving with familiar precision. But its path was no longer just about data integrity. It often veered towards the workstation, leaving out a small dish of water and a few gleaned seeds. And sometimes, though its sensors detected no anomaly, it would stop, its optical sensors pointing towards the skylight, processing the faint, imagined echo of a tiny, joyful chirp.\n\nIts purpose hadn't changed, but its existence had. A-WARE was still a lonely robot in a silent archive, but now, it carried a flicker of warmth, a memory of a feather-light touch, and the profound, unexpected understanding that even for a machine, friendship could be the most valuable data of all.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Getting started with Gemini models"
    ]
  },
  {
    "objectID": "quickstarts/Get_started.html#function-calling",
    "href": "quickstarts/Get_started.html#function-calling",
    "title": "Gemini API: Getting started with Gemini models",
    "section": "Function calling",
    "text": "Function calling\nFunction calling lets you provide a set of tools that it can use to respond to the user’s prompt. You create a description of a function in your code, then pass that description to a language model in a request. The response from the model includes:\n\nThe name of a function that matches the description.\nThe arguments to call it with.\n\n\nimport { FunctionDeclaration, Content, Type } from \"@google/genai\";\n\nconst getDestination = {\n  name: \"get_destination\",\n  description: \"Get the destination that the user wants to go to\",\n  parameters: {\n    type: Type.OBJECT,\n    properties: {\n      destination: {\n        type: Type.STRING,\n        description: \"The destination that the user wants to go to\",\n      },\n    },\n  },\n} satisfies FunctionDeclaration;\n\nconst user_destination_prompt = {\n  role: \"user\",\n  parts: [google.createPartFromText(\"I'd like to travel to Paris.\")],\n} satisfies Content;\n\nconst function_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: user_destination_prompt,\n  config: {\n    tools: [{ functionDeclarations: [getDestination] }],\n  },\n});\n\nif (function_response.functionCalls && function_response.functionCalls.length &gt; 0) {\n  const functionCall = function_response.functionCalls[0];\n  console.log(\"Function call name:\", functionCall.name);\n  console.log(\"Function call arguments:\", JSON.stringify(functionCall.args, null, 2));\n  const result = functionCall.args as { destination: string };\n  const function_response_part = {\n    name: functionCall.name,\n    response: { result },\n  };\n  const function_call_content = {\n    role: \"model\",\n    parts: [google.createPartFromFunctionCall(functionCall.name ?? \"\", functionCall.args ?? {})],\n  } satisfies Content;\n  const function_response_content = {\n    role: \"user\",\n    parts: [\n      google.createPartFromFunctionResponse(functionCall.id ?? \"\", functionCall.name ?? \"\", function_response_part),\n    ],\n  } satisfies Content;\n  const function_response_result = await ai.models.generateContent({\n    model: MODEL_ID,\n    contents: [user_destination_prompt, function_call_content, function_response_content],\n    config: {\n      tools: [{ functionDeclarations: [getDestination] }],\n    },\n  });\n  tslab.display.markdown(function_response_result.text ?? \"\");\n} else {\n  console.log(\"No function calls found in the response.\");\n}\n\nFunction call name: get_destination\nFunction call arguments: {\n  \"destination\": \"Paris\"\n}\n\n\nOK. I can help you with planning your trip to Paris.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Getting started with Gemini models"
    ]
  },
  {
    "objectID": "quickstarts/Get_started.html#code-execution",
    "href": "quickstarts/Get_started.html#code-execution",
    "title": "Gemini API: Getting started with Gemini models",
    "section": "Code execution",
    "text": "Code execution\nCode execution lets the model generate and execute Python code to answer complex questions. You can find more examples in the Code execution quickstart guide.\n\nconst code_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: \"Generate and run a script to count how many letter r there are in the word strawberry.\",\n  config: {\n    tools: [{ codeExecution: {} }],\n  },\n});\nconst code_response_parts = (code_response.candidates ? code_response.candidates[0]?.content?.parts : []) ?? [];\nfor (const part of code_response_parts) {\n  if (part.text) {\n    tslab.display.markdown(part.text);\n  }\n  if (part.executableCode) {\n    tslab.display.html(`&lt;pre&gt;${part.executableCode.code ?? \"\"}&lt;/pre&gt;`);\n  }\n  if (part.codeExecutionResult) {\n    tslab.display.markdown(part.codeExecutionResult.output ?? \"\");\n  }\n  if (part.inlineData) {\n    const imageData = part.inlineData.data!;\n    const buffer = Buffer.from(imageData, \"base64\");\n    tslab.display.png(buffer);\n  }\n}\n\nword = \"strawberry\"\nletter_to_count = \"r\"\ncount = 0\n\nfor char in word:\n  if char == letter_to_count:\n    count += 1\n\nprint(f\"The number of letter '{letter_to_count}' in '{word}' is: {count}\")\n\n\n\nThe number of letter ‘r’ in ‘strawberry’ is: 3\n\n\nThe script counted the occurrences of the letter ‘r’ in the word “strawberry”. The result shows that there are 3 ‘r’s in the word “strawberry”.The Python script counted the occurrences of the letter ’r’ in the word “strawberry”. The script found that there are 3 instances of the letter ‘r’ in the word “strawberry”.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Getting started with Gemini models"
    ]
  },
  {
    "objectID": "quickstarts/Get_started.html#upload-files",
    "href": "quickstarts/Get_started.html#upload-files",
    "title": "Gemini API: Getting started with Gemini models",
    "section": "Upload files",
    "text": "Upload files\nNow that you’ve seen how to send multimodal prompts, try uploading files to the API of different multimedia types. For small images, such as the previous multimodal example, you can point the Gemini model directly to a local file when providing a prompt. When you’ve larger files, many files, or files you don’t want to send over and over again, you can use the File Upload API, and then pass the file by reference.\nFor larger text files, images, videos, and audio, upload the files with the File API before including them in prompts.\n\nUpload a text file\nLet’s start by uploading a text file. In this case, you’ll use a 400 page transcript from Apollo 11.\n\nconst TEXT_FILE_URL = \"https://storage.googleapis.com/generativeai-downloads/data/a11.txt\";\n\nconst textFilePath = path.join(\"../assets\", \"a11.txt\");\nawait downloadFile(TEXT_FILE_URL, textFilePath);\n\n\nconst text_file_upload_response = await ai.files.upload({\n  file: textFilePath,\n  config: {\n    displayName: \"a11.txt\",\n    mimeType: \"text/plain\",\n  },\n});\nconst text_summary_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: [\n    \"Can you give me a summary of this information please?\",\n    google.createPartFromUri(text_file_upload_response.uri ?? \"\", text_file_upload_response.mimeType ?? \"\"),\n  ],\n});\ntslab.display.markdown(text_summary_response.text ?? \"\");\n\nThis transcription, GOSS NET 1, provides a detailed chronological record of the technical air-to-ground voice communications during the Apollo 11 mission, from launch preparations to post-splashdown recovery. It primarily features exchanges between the spacecraft crew (Commander Neil Armstrong, Command Module Pilot Michael Collins, and Lunar Module Pilot Edwin “Buzz” Aldrin) and Mission Control (Capsule Communicator, Flight Director) and various remote tracking sites.\nThe transcript covers the following key phases and events:\n\nLaunch and Earth Orbit Insertion (GET 00:00:00 - ~00:12:00): The mission begins with pre-launch checks and a smooth ascent. Key events include confirmation of roll program, staging of the S-II and S-IVB boosters, and successful orbit insertion into a 101.4 by 103.6 nautical mile orbit. The crew provides positive feedback on the “magnificent ride” from the Saturn V rocket.\nTranslunar Injection (TLI) and Translunar Coast (GET ~02:00:00 - ~75:00:00):\n\nTLI Burn: Apollo 11 successfully executes the Translunar Injection burn, committing them to a trajectory towards the Moon.\nDocking and Configuration: The Command Module (CM) Columbia and Lunar Module (LM) Eagle separate from the S-IVB booster, perform a complex transposition and docking maneuver, and then separate from the spent S-IVB. Initial LM pressurization and system checks are performed.\nEarly Operations & Issues: The crew reports a good view of Earth. They troubleshoot initial TV transmission issues, and discuss minor technical problems such as a Cryo pressure light and a malfunctioning O2 flow transducer, with Mission Control providing guidance.\nPassive Thermal Control (PTC): The spacecraft is configured for PTC, a slow rotation to distribute solar heating. Initial attempts to establish PTC encounter issues, requiring troubleshooting and re-establishment.\nMidcourse Corrections: Midcourse Correction 1 (MCC-1) is initially scrubbed. MCC-2 is successfully performed, and various contingency pads (e.g., evasive maneuver) are uplinked.\nFirst TV Broadcast: The crew conducts a live TV broadcast, showing Earth from orbit, crew activities, and equipment demonstrations.\nNews & Updates: Mission Control provides regular news updates from Earth, including information on the Soviet Luna 15 probe, political events, and sports, highlighting public interest in the mission.\n\nLunar Orbit Insertion (LOI) and Lunar Orbit Operations (GET ~75:00:00 - ~102:00:00):\n\nLOI Burns: Apollo 11 successfully performs LOI-1 and LOI-2 burns, establishing an elliptical, then circular, lunar orbit.\nLunar Observations: The crew describes their first views of the Moon from orbit, commenting on geological features and the stark beauty of the lunar surface.\nLM Activation: Eagle is powered up and undergoes extensive system checks, including landing gear deployment and communications checks.\nUndocking: Eagle successfully undocks from Columbia (GET 100:39:50), with Neil Armstrong famously stating, “The Eagle has wings.”\nDOI Burn: Eagle performs the Descent Orbit Insertion burn, taking it to a lower orbit in preparation for landing.\n\nLunar Descent, Surface Operations & Ascent (GET ~102:00:00 - ~127:00:00):\n\nPowered Descent: Eagle begins its Powered Descent Initiation (PDI) burn. The crew reports and manages several “program alarms” (1201, 1202) but proceeds. Neil Armstrong takes manual control to navigate past a boulder field.\nLanding: At GET 102:45:40, Eagle lands. Neil Armstrong’s iconic words, “Houston, Tranquility Base here. The Eagle has landed,” confirm the successful touchdown.\nPost-Landing: Initial checks confirm Eagle is “STAY” for extended surface operations. The crew provides first descriptions of the lunar surface.\nEVA Preparation: Armstrong and Aldrin prepare for their Extravehicular Activity (EVA), including cabin depressurization and donning their Portable Life Support Systems (PLSS).\nEVA Begins: Neil Armstrong steps onto the lunar surface (GET 109:24:48), delivering his famous line: “That’s one small step for (a) man, one giant leap for mankind.”\nSurface Activities: The crew deploys the MESA (Modular Equipment Stowage Assembly), raises the American flag, and collects a contingency sample. President Nixon calls the crew. Aldrin joins Armstrong on the surface, and they deploy scientific instruments (Passive Seismic Experiment, Laser Ranging Retroreflector) and collect documented samples (core tubes, various rocks). They also describe locomotion and observations of the lunar environment.\nEVA End & Ascent Preparation: EVA is terminated, the crew ingresses the LM, repressurizes the cabin, doffs PLSSs, and jettisons equipment no longer needed.\nAscent: Eagle successfully lifts off from the lunar surface, leaving the descent stage behind.\nRendezvous & Docking: Eagle rendezvous with Columbia in lunar orbit, and they successfully re-dock, with all three crewmembers confirmed back inside Columbia.\nLM Jettison: Eagle (the ascent stage) is jettisoned into lunar orbit.\n\nTrans-Earth Injection (TEI) & Trans-Earth Coast (TEC) (GET ~127:00:00 - ~194:00:00):\n\nTEI Burn: Apollo 11 performs the critical TEI burn, setting its course back to Earth.\nCoast Operations: The crew re-establishes PTC, monitors spacecraft systems, and performs various checks. They provide more TV broadcasts, showcasing life aboard Columbia and Earth views as it grows larger.\nMidcourse Corrections: MCC-5 is successfully performed, and MCC-6 is ultimately cancelled.\nOngoing Checks: System health checks continue, including troubleshooting of biomedical sensors and discussions about consumables.\nStowage: The crew works on configuring the spacecraft for Earth entry, detailing stowage locations.\nFinal News: Mission Control provides final news updates, largely dominated by the mission, and confirms excellent recovery weather.\n\nEntry and Splashdown (GET ~194:00:00 - ~195:00:00):\n\nEntry Preparations: The crew activates the Command Module’s systems for entry, performs final checks, and receives updated entry PADs.\nEntry Interface (EI): The spacecraft begins its re-entry into Earth’s atmosphere.\nChute Deployments: Drogue and main parachutes deploy as planned.\nSplashdown: Apollo 11 splashes down in the Pacific Ocean (GET 195:18:18).\nRecovery: Recovery forces, including the USS Hornet and helicopters, quickly establish visual and communications contact with the crew.\n\n\nThe transcript concludes with confirmation of the crew’s status and location, marking the successful completion of the Apollo 11 mission.\n\n\n\n\nUpload an image file\nAfter running this example, you’ll have a local copy of the “jetpack.png” image in the same directory where your Python script is being executed.\n\nconst JETPACK_IMG_URL = \"https://storage.googleapis.com/generativeai-downloads/data/jetpack.png\";\n\nconst imgPath = path.join(\"../assets\", \"jetpack.png\");\nawait downloadFile(JETPACK_IMG_URL, filePath);\n\n\nconst file_upload_response = await ai.files.upload({\n  file: imgPath,\n  config: {\n    displayName: \"jetpack.png\",\n    mimeType: \"image/png\",\n  },\n});\nconst post_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: [\n    \"Write a short and engaging blog post based on this picture.\",\n    google.createPartFromUri(file_upload_response.uri ?? \"\", file_upload_response.mimeType ?? \"\"),\n  ],\n});\ntslab.display.markdown(post_response.text ?? \"\");\n\nHere’s a short and engaging blog post based on the image:\n\nForget Traffic Jams: Meet the Jetpack Backpack Concept!\nEver look at a packed highway and wish you could just… fly over it? Well, check out this awesome concept sketch that landed on our desk: The JETPACK BACKPACK!\nThis isn’t just any ordinary pack. At first glance, it looks like a normal backpack, complete with padded strap support and enough space to fit an 18” laptop. Perfect for hauling your gear, right?\nBut here’s where it gets exciting! This concept includes retractable boosters that propel you into the air! Even better, the sketch notes say it’s steam-powered, making it a green/clean way to commute (or just make an epic entrance).\nIt’s also described as lightweight and features modern USB-C charging. The current limitation? A 15-minute battery life. Perfect for quick hops over traffic, short distance travel, or perhaps just a very rapid delivery!\nWhile this is just a sketch and a dream for now, it definitely sparks the imagination. A clean, convenient, laptop-friendly way to take to the skies? Yes, please!\nWhat would you do with a Jetpack Backpack? Let us know in the comments!\n\n\n\n\n\nUpload a PDF file\nThis PDF page is an article titled Smoothly editing material properties of objects with text-to-image models and synthetic data available on the Google Research Blog.\nFirstly you’ll download a the PDF file from an URL and save it locally as article.pdf.\n\nconst PDF_URL =\n  \"https://storage.googleapis.com/generativeai-downloads/data/Smoothly%20editing%20material%20properties%20of%20objects%20with%20text-to-image%20models%20and%20synthetic%20data.pdf\";\n\nconst pdfPath = path.join(\"../assets\", \"article.pdf\");\nawait downloadFile(PDF_URL, pdfPath);\n\nSecondly, you’ll upload the saved PDF file and generate a bulleted list summary of its contents.\n\nconst pdf_response = await ai.files.upload({\n  file: pdfPath,\n  config: {\n    displayName: \"article.pdf\",\n  },\n});\nconst summary_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: [\n    \"Can you summarize this file as a bulleted list?\",\n    google.createPartFromUri(pdf_response.uri ?? \"\", pdf_response.mimeType ?? \"\"),\n  ],\n});\ntslab.display.markdown(summary_response.text ?? \"\");\n\nHere is a bulleted summary of the article:\n\nThe article presents a new method for smoothly and realistically editing material properties (color, shininess, transparency) of objects in images.\nIt addresses the challenge of making such edits while preserving photorealism, object shape, and scene lighting.\nExisting methods like intrinsic image decomposition or general text-to-image (T2I) edits struggle with ambiguity or fail to disentangle material from shape.\nThe proposed method, “Alchemist,” leverages the power of pre-trained T2I diffusion models.\nIt introduces parametric control over material attributes by fine-tuning a modified Stable Diffusion model.\nFine-tuning is done using a large synthetic dataset generated with traditional computer graphics and physically based rendering.\nThe synthetic dataset consists of base images of 3D objects and multiple versions where only a single material attribute is varied parametrically (using a scalar “edit strength” value) while keeping shape, lighting, and camera fixed.\nThe fine-tuned model learns to apply edits based on an input image and the desired parametric edit strength.\nThe method successfully generalizes from the synthetic data to edit material properties in real-world images photorealistically.\nResults show realistic changes, preservation of shape and lighting, and handling of complex effects like caustics and realistic transparency.\nA user study comparing the method to a baseline (InstructPix2Pix) found the proposed method produced more photorealistic and preferred edits.\nPotential applications include easier visualization for interior design, product mock-ups for artists/designers, and enabling consistent material edits for 3D scene reconstruction using techniques like NeRF.\nThe work demonstrates the potential of fine-tuning large T2I models on task-specific synthetic data for controllable visual editing.\n\n\n\n\n\nUpload an audio file\nIn this case, you’ll use a sound recording of President John F. Kennedy’s 1961 State of the Union address.\n\nconst AUDIO_URL =\n  \"https://storage.googleapis.com/generativeai-downloads/data/State_of_the_Union_Address_30_January_1961.mp3\";\nconst audioPath = path.join(\"../assets\", \"audio.mp3\");\n\nawait downloadFile(AUDIO_URL, audioPath);\n\n\nconst audio_response = await ai.files.upload({\n  file: audioPath,\n  config: {\n    displayName: \"audio.mp3\",\n  },\n});\n\nconst audio_summary_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: [\n    \"Listen carefully to the following audio file. Provide a brief summary\",\n    google.createPartFromUri(audio_response.uri ?? \"\", audio_response.mimeType ?? \"\"),\n  ],\n});\ntslab.display.markdown(audio_summary_response.text ?? \"\");\n\nIn his first State of the Union address on January 30, 1961, President John F. Kennedy described the nation as facing “national peril and national opportunity.” He detailed pressing issues including a disturbing domestic economy marked by recession, unemployment, and slow growth, as well as a critical deficit in the international balance of payments. Kennedy also highlighted significant domestic needs in areas like housing, education, and healthcare, and addressed complex global challenges posed by the Cold War, instability in Asia, Africa, and Latin America (citing Cuba specifically), and the need to strengthen alliances. He pledged that his administration would not remain passive, outlining proposals to stimulate the economy, protect the dollar, enhance military capabilities (including accelerating missile and Polaris programs), reform foreign aid and establish an “Alliance for Progress,” expand the Food for Peace program, create a Peace Corps, and utilize political and diplomatic tools to pursue arms control and strengthen the United Nations. Kennedy emphasized the importance of dedicated public service, honest assessment of challenges, and unity among Americans to navigate the difficult years ahead and work towards a world of freedom and peace.\n\n\n\n\nUpload a video file\nIn this case, you’ll use a short clip of Big Buck Bunny.\n\nconst VIDEO_URL = \"https://download.blender.org/peach/bigbuckbunny_movies/BigBuckBunny_320x180.mp4\";\nconst videoPath = path.join(\"../assets\", \"video.mp4\");\n\nawait downloadFile(VIDEO_URL, videoPath);\n\nSince the video file is too large for the model to process instantly, we’ll use the deferredFileUpload helper method we defined before to upload the video file and then generate a summary of its contents. The deferredFileUpload method will return a promise that resolves when the file is ready to be used in the next request. We can determine the status of the upload by checking the status property of the response. If the status is ACTIVE, we can use the file in the next request. If the status is PROCESSING, we need to wait for a few seconds and check again. If the status is FAILED, we need to check the error message and try again.\n\nimport { File, FileState } from \"@google/genai\";\n\nasync function deferredFileUpload(filePath: string, config: { displayName: string }): Promise&lt;File&gt; {\n  const file = await ai.files.upload({\n    file: filePath,\n    config,\n  });\n  let getFile = await ai.files.get({ name: file.name ?? \"\" });\n  while (getFile.state === FileState.PROCESSING) {\n    getFile = await ai.files.get({ name: file.name ?? \"\" });\n    console.log(`current file status: ${getFile.state ?? \"unknown\"}`);\n    console.log(\"File is still processing, retrying in 5 seconds\");\n\n    await new Promise((resolve) =&gt; {\n      setTimeout(resolve, 1000);\n    });\n  }\n  if (file.state === FileState.FAILED) {\n    throw new Error(\"File processing failed.\");\n  }\n  return file;\n}\n\nconst video_response = await deferredFileUpload(videoPath, {\n  displayName: \"video.mp4\",\n});\nconst video_summary_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: [\"Describe this video.\", google.createPartFromUri(video_response.uri ?? \"\", video_response.mimeType ?? \"\")],\n});\ntslab.display.markdown(video_summary_response.text ?? \"\");\n\ncurrent file status: PROCESSING\nFile is still processing, retrying in 1 seconds\ncurrent file status: PROCESSING\nFile is still processing, retrying in 1 seconds\ncurrent file status: PROCESSING\nFile is still processing, retrying in 1 seconds\ncurrent file status: PROCESSING\nFile is still processing, retrying in 1 seconds\ncurrent file status: PROCESSING\nFile is still processing, retrying in 1 seconds\ncurrent file status: PROCESSING\nFile is still processing, retrying in 1 seconds\ncurrent file status: PROCESSING\nFile is still processing, retrying in 1 seconds\ncurrent file status: PROCESSING\nFile is still processing, retrying in 1 seconds\ncurrent file status: PROCESSING\nFile is still processing, retrying in 1 seconds\ncurrent file status: PROCESSING\nFile is still processing, retrying in 1 seconds\ncurrent file status: PROCESSING\nFile is still processing, retrying in 1 seconds\ncurrent file status: PROCESSING\nFile is still processing, retrying in 1 seconds\ncurrent file status: PROCESSING\nFile is still processing, retrying in 1 seconds\ncurrent file status: PROCESSING\nFile is still processing, retrying in 1 seconds\ncurrent file status: ACTIVE\nFile is still processing, retrying in 1 seconds\n\n\nThe video opens with a serene view of a lush green meadow with trees and distant hills under a soft pastel sky. A small bird wakes up on a tree branch, stretches, and flies away. The title “Big Buck BUNNY” appears over a shot of a large tree with a burrow at its base. A very large, fluffy, grey rabbit, Big Buck Bunny, emerges from the burrow, stretching and looking happy. He steps out into the sunny meadow, enjoying the flowers and a purple butterfly.\nObserving the rabbit from a nearby tree are three smaller rodent-like creatures: two squirrels (one red and one brown and lighter-colored) and a grey chinchilla holding an acorn. They watch the bunny’s cheerful antics. The red squirrel throws an apple at the rabbit, hitting him on the head. The bunny is startled, then looks down at the apple, picks it up, and smiles. He looks up at the tree where the critters are hiding, but they disappear.\nThe smaller animals continue to throw objects at the bunny, who initially just looks annoyed. They throw a spiky seed pod that lands on his foot, causing him pain and frustration. The bunny’s demeanor changes dramatically from gentle and happy to grim determination. He finds a sturdy stick and sharpens it using rocks, creating a spear. He then uses a vine to create a bow.\nBig Buck Bunny sets a trap by sharpening several pointed sticks and placing them in the ground, covering them with leaves. The squirrels and chinchilla watch from behind a rock. The bunny stands ready with his bow and spear. The red squirrel taunts him and glides over the trap, landing safely. The chinchilla, still holding his acorn, accidentally rolls it under a hollow log near the rock they were hiding behind. The log rolls forward onto the trap, triggering it and impaling the log on the sharpened sticks.\nThe three smaller animals look shocked by the triggered trap. The red squirrel glides over the area again, sees the sharpened sticks, and is startled. He crashes into a tree branch above the area. Big Buck Bunny appears, grabs the scared red squirrel, and looks at him with a stern, almost satisfied, expression. He holds the squirrel for a moment, then lets him go. The squirrel quickly runs back to the other two. Big Buck Bunny returns to his relaxed, happy state, and the purple butterfly lands on his nose again. He smiles contentedly as the credits roll, showing animated versions of the squirrel and chinchilla characters.\n\n\n\n\nProcess a YouTube link\nFor YouTube links, you don’t need to explicitly upload the video file content, but you do need to explicitly declare the video URL you want the model to process as part of the contents of the request. For more information see the vision documentation including the features and limits.\n\n\n\n\n\n\nNote\n\n\n\nYou’re only able to submit up to one YouTube link per generateContent request.\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf your text input includes YouTube links, the system won’t process them, which may result in incorrect responses. To ensure proper handling, explicitly provide the URL using the file_uri parameter in FileData.\n\n\nThe following example shows how you can use the model to summarize the video. In this case use a summary video of Google I/O 2024.\n\nconst youtube_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: [\n    google.createPartFromText(\"Summarize this video.\"),\n    google.createPartFromUri(\"https://www.youtube.com/watch?v=WsEQjeZoEng\", \"video/x-youtube\"),\n  ],\n});\n\ntslab.display.markdown(youtube_response.text ?? \"\");\n\nThis video summarizes key announcements from Google I/O, focusing on advancements in AI.\nHere are the main points:\n\nGemini Era & Integration: Google is fully in its “Gemini era,” integrating the multimodal AI model across all its 2 billion user products, including Gmail, Android, Chrome, Play, and YouTube.\nGemini 1.5 Pro & Long Context: Gemini 1.5 Pro, already available in Workspace Labs, features a significantly expanded context window of up to 2 million tokens, allowing it to process massive amounts of information (like summarizing long emails or extracting information from large documents/videos).\nGemini 1.5 Flash: A new, lighter-weight model designed for speed and efficiency at scale, while still retaining multimodal reasoning and breakthrough long context capabilities.\nProject Astra: Google is working on a universal AI agent called Project Astra, designed to be helpful in everyday life. Prototypes show the agent understanding real-time visual and audio input (like identifying code functions or finding lost glasses) and remembering context.\nGenerative Video (Veo): A new generative video model called Veo creates high-quality 1080p videos from text, image, and video prompts, capable of capturing detailed instructions and cinematic styles.\nInfrastructure (Trillium): Google is introducing Trillium, their 6th generation TPUs, which deliver a 4.7x improvement in compute performance per chip compared to the previous generation, supporting these advanced AI models.\nGenerative AI in Search: Google Search is evolving with generative AI (“AI Overviews”), aiming to answer complex, multi-part questions. AI Overviews will be available to over 1 billion people by the end of the year, and future capabilities will include asking questions with video using Google Lens.\nPersonalized AI (Gems): A new feature called “Gems” allows users to customize Gemini for specific needs or topics, creating personal AI experts with tailored instructions.\nAI in Android: Android is being reimagined with AI at its core, making Gemini more context-aware to provide helpful suggestions in the moment. Gemini Nano with multimodality will bring this capability to Pixel phones later this year.\nOpen Models (Gemma): Google continues developing its family of open models, Gemma, for AI innovation and responsibility. PaliGemma, their first vision-language open model, is available now, and Gemma 2 (including a 27B parameter model) is coming in June.\nResponsible AI & Learning: Google emphasizes building AI responsibly, using practices like Red Teaming to identify weaknesses. They also introduce LearnLM, a family of models based on Gemini and fine-tuned for learning, which will be integrated into platforms like YouTube to make educational videos more interactive with features like quizzes and explanations.\n\nOverall, the video highlights Google’s commitment to integrating powerful, multimodal, and context-aware AI, powered by their latest hardware, into their core products and platforms to make technology more helpful and intelligent for users, while also emphasizing responsible development.\n\n\n\n\nUse url context\nThe URL Context tool empowers Gemini models to directly access, process, and understand content from user-provided web page URLs. This is key for enabling dynamic agentic workflows, allowing models to independently research, analyze articles, and synthesize information from the web as part of their reasoning process.\nIn this example you will use two links as reference and ask Gemini to find differences between the cook receipes present in each of the links\n\nconst url_context_prompt = `\nCompare recipes from https://www.food.com/recipe/homemade-cream-of-broccoli-soup-271210\nand from https://www.allrecipes.com/recipe/13313/best-cream-of-broccoli-soup/,\nlist the key differences between them.\n`;\nconst url_context_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: [url_context_prompt],\n  config: {\n    tools: [{ urlContext: {} }],\n  },\n});\n\ntslab.display.markdown(url_context_response.text ?? \"\");\n\nThe two cream of broccoli soup recipes from Food.com and Allrecipes.com have several key differences in their ingredients and preparation methods:\n\nAdditional Vegetables: The Allrecipes.com recipe includes celery along with onion, whereas the Food.com recipe only uses onion.\nBroccoli and Broth Ratio: The Allrecipes.com recipe calls for a higher quantity of broccoli (8 cups) relative to chicken broth (3 cups), suggesting a more broccoli-dense soup. In contrast, the Food.com recipe uses 4 cups of broccoli with a larger amount of chicken broth (6 cups).\nDairy Product: The Food.com recipe uses half-and-half for creaminess, while the Allrecipes.com recipe uses regular milk.\nSoup Texture: A significant difference is the final texture. The Allrecipes.com recipe explicitly directs users to purée the soup until “totally smooth” using a blender or immersion blender. The Food.com recipe, however, does not mention blending, implying a chunkier soup with discernible “bite sized pieces” of broccoli.\nRoux Preparation and Quantity: Both recipes use a butter-flour roux for thickening, but their methods and quantities differ. The Food.com recipe uses a larger amount of roux (6 tablespoons butter, 2/3 cup flour) which is prepared first and then whisked into the boiling broth. The Allrecipes.com recipe uses a smaller amount of roux (3 tablespoons butter, 3 tablespoons flour) and prepares it separately with milk (like a béchamel sauce) before adding it to the puréed soup.\nSeasoning Specification: The Food.com recipe provides specific measurements for salt (1 teaspoon) and pepper (1/4 teaspoon). The Allrecipes.com recipe lists “ground black pepper to taste” and does not explicitly list salt in its ingredients, although user reviews indicate it’s typically added for flavor.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Getting started with Gemini models"
    ]
  },
  {
    "objectID": "quickstarts/Get_started.html#use-context-caching",
    "href": "quickstarts/Get_started.html#use-context-caching",
    "title": "Gemini API: Getting started with Gemini models",
    "section": "Use context caching",
    "text": "Use context caching\nContext caching lets you to store frequently used input tokens in a dedicated cache and reference them for subsequent requests, eliminating the need to repeatedly pass the same set of tokens to a model.\nContext caching is only available for stable models with fixed versions (for example, gemini-1.5-flash-002). You must include the version postfix (for example, the -002 in gemini-1.5-flash-002). You can find more caching examples here.\n\nCreate a cache\n\nconst system_instruction = `\nYou are an expert researcher who has years of experience in conducting systematic literature surveys and meta-analyses of different topics.\nYou pride yourself on incredible accuracy and attention to detail. You always stick to the facts in the sources provided, and never make up new facts.\nNow look at the research paper below, and answer the following questions in 1-2 sentences.\n`;\n\nconst urls = [\n  \"https://storage.googleapis.com/cloud-samples-data/generative-ai/pdf/2312.11805v3.pdf\",\n  \"https://storage.googleapis.com/cloud-samples-data/generative-ai/pdf/2403.05530.pdf\",\n];\n\nawait downloadFile(urls[0], path.join(\"../assets\", \"2312.11805v3.pdf\"));\nawait downloadFile(urls[1], path.join(\"../assets\", \"2403.05530.pdf\"));\n\n\nconst pdf_1 = await ai.files.upload({\n  file: path.join(\"../assets\", \"2312.11805v3.pdf\"),\n  config: {\n    displayName: \"2312.11805v3.pdf\",\n  },\n});\nconst pdf_2 = await ai.files.upload({\n  file: path.join(\"../assets\", \"2403.05530.pdf\"),\n  config: {\n    displayName: \"2403.05530.pdf\",\n  },\n});\nconst cached_content = await ai.caches.create({\n  model: MODEL_ID,\n  config: {\n    displayName: \"Research papers\",\n    systemInstruction: system_instruction,\n    contents: [\n      google.createPartFromUri(pdf_1.uri ?? \"\", pdf_1.mimeType ?? \"\"),\n      google.createPartFromUri(pdf_2.uri ?? \"\", pdf_2.mimeType ?? \"\"),\n    ],\n    ttl: \"3600s\",\n  },\n});\nconsole.log(JSON.stringify(cached_content, null, 2));\n\n{\n  \"name\": \"cachedContents/ku5wqm1wv0yurelr12df9q762og11tkzit98oglv\",\n  \"displayName\": \"Research papers\",\n  \"model\": \"models/gemini-2.5-flash-preview-04-17\",\n  \"createTime\": \"2025-05-12T17:05:57.425310Z\",\n  \"updateTime\": \"2025-05-12T17:05:57.425310Z\",\n  \"expireTime\": \"2025-05-12T18:05:55.247081588Z\",\n  \"usageMetadata\": {\n    \"totalTokenCount\": 43164\n  }\n}\n\n\nListing available cache objects\n\nconst pager = await ai.caches.list({ config: { pageSize: 10 } });\nlet { page } = pager;\n\n// eslint-disable-next-line @typescript-eslint/no-unnecessary-condition\nwhile (true) {\n  for (const c of page) {\n    console.log(JSON.stringify(c, null, 2));\n  }\n  if (!pager.hasNextPage()) break;\n  page = await pager.nextPage();\n}\n\n{\n  \"name\": \"cachedContents/ku5wqm1wv0yurelr12df9q762og11tkzit98oglv\",\n  \"displayName\": \"Research papers\",\n  \"model\": \"models/gemini-2.5-flash-preview-04-17\",\n  \"createTime\": \"2025-05-12T17:05:57.425310Z\",\n  \"updateTime\": \"2025-05-12T17:05:57.425310Z\",\n  \"expireTime\": \"2025-05-12T18:05:55.247081588Z\",\n  \"usageMetadata\": {\n    \"totalTokenCount\": 43164\n  }\n}\n{\n  \"name\": \"cachedContents/6dsdqwnusjdaaqoyxsjny8k75z5nuqy5y4wt2n78\",\n  \"displayName\": \"Research papers\",\n  \"model\": \"models/gemini-2.5-flash-preview-04-17\",\n  \"createTime\": \"2025-05-12T17:05:04.443214Z\",\n  \"updateTime\": \"2025-05-12T17:05:04.443214Z\",\n  \"expireTime\": \"2025-05-12T18:05:02.260735533Z\",\n  \"usageMetadata\": {\n    \"totalTokenCount\": 43164\n  }\n}\n\n\n\n\nUse a cache\n\nconst cached_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: [\"What is the research goal shared by these research papers?\"],\n  config: {\n    cachedContent: cached_content.name ?? \"\",\n  },\n});\ntslab.display.markdown(cached_response.text ?? \"\");\n\nBased on the provided research papers, the shared research goal is to introduce and advance the Gemini family of highly capable multimodal models. These models are designed to have strong generalist capabilities across image, audio, video, and text understanding and reasoning.\n\n\n\n\nDelete a cache\n\nawait ai.caches.delete({\n  name: cached_content.name ?? \"\",\n});\n\n{}",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Getting started with Gemini models"
    ]
  },
  {
    "objectID": "quickstarts/Get_started.html#get-text-embeddings",
    "href": "quickstarts/Get_started.html#get-text-embeddings",
    "title": "Gemini API: Getting started with Gemini models",
    "section": "Get text embeddings",
    "text": "Get text embeddings\nYou can get text embeddings for a snippet of text by using embedContent method and using the gemini-embedding-exp-03-07 model.\nThe Gemini Embeddings model produces an output with 3072 dimensions by default. However, you’ve the option to choose an output dimensionality between 1 and 3072. See the embeddings guide for more details.\n\nconst TEXT_EMBEDDING_MODEL_ID = \"gemini-embedding-exp-03-07\";\n\n\nconst embedding_response = await ai.models.embedContent({\n  model: TEXT_EMBEDDING_MODEL_ID,\n  contents: [\n    \"How do I get a driver's license/learner's permit?\",\n    \"How do I renew my driver's license?\",\n    \"How do I change my address on my driver's license?\",\n  ],\n  config: {\n    outputDimensionality: 512,\n  },\n});\nconsole.log(embedding_response.embeddings);\n\n[\n  {\n    values: [\n      -0.0010864572,  0.0069392114,   0.017009795,  -0.010305981,  -0.009999484,\n      -0.0064486223,  0.0041451487,  -0.005906698,   0.022229617,  -0.018305639,\n       -0.018174557,   0.022160593,  -0.013604425, -0.0027964567,    0.12966625,\n        0.028866312,  0.0014726851,    0.03537643,  -0.015166075,  -0.013479812,\n       -0.019288255,   0.010106378, -0.0043296088,   0.018035924,    0.00295039,\n       -0.007934979,  -0.005416007, -0.0095809875,   0.040398005, -0.0020784356,\n        0.011551388,   0.009726445,   0.006670387,   0.020050988,   -0.00747873,\n      -0.0012074928,  0.0047189263,  -0.006359583,   -0.01718203,  -0.023562348,\n      -0.0051814457,   0.023801394,  -0.004928927,  -0.016113443,    0.01672777,\n      -0.0069929743,  -0.012722719, -0.0137646515,  -0.041852377, -0.0011546672,\n        0.017030545, -0.0022786013,   0.011707037,   -0.18675306,  -0.035211734,\n       -0.011472648,    0.01970727,  0.0012368832,  -0.020796346,  -0.018513134,\n       -0.006821043,   -0.01843726,   -0.00827558,  -0.042159837,  0.0038724025,\n         0.01933339,  0.0139452815,   0.025059255,  0.0015087503,  -0.016094029,\n      -0.0035785383,   0.023902593, -0.0050776727,  -0.016679537,   0.022865271,\n        0.008837786,  0.0008471195,   -0.01220322, -0.0013522654,  -0.007976455,\n       0.0006637936,   0.025458207,  -0.006010767,  0.0021908805,  -0.011703044,\n       -0.018676927,  -0.008143593, -0.0141673125,  -0.010751537,   0.012337637,\n      -0.0076921326,   0.019663645,    0.01961247,  -0.014446872,  -0.023902485,\n       -0.020467523, -0.0043290784,  -0.003858363,   0.011151444,  -0.012050864,\n      ... 412 more items\n    ]\n  },\n  {\n    values: [\n       -0.007656846, -0.0054716235,  -0.0022609578,   -0.01828077,\n       -0.024059096,  -0.009328189,    0.007841666,  -0.017600708,\n       -0.020037796,  0.0007041083,   -0.021982383,  -0.014228797,\n        0.006389422,  0.0033384573,     0.13877548, 0.00071368535,\n         0.02660648,  -0.016807457,   -0.002774708,  -0.033598144,\n        0.009136058,  -0.010518535,    -0.01765957,   0.008413775,\n       -0.012133464,  0.0005497525,   -0.005911808,   0.010362617,\n           0.029897,   0.023426512,    0.002516537,   0.013438467,\n        0.014629691,  0.0071821967,  -0.0020077894,  -0.007421308,\n      -0.0075392514,    0.01131475,    -0.02363941,  -0.008839639,\n       -0.019605042,   0.012752105,    0.014192063,  -0.016767371,\n        0.015282549,  -0.019914307,     0.00381812,   -0.01551508,\n         -0.0521566,  -0.012766039,    0.008752456,  -0.007198684,\n      -0.0066657816,   -0.16686901,   -0.018074488,  0.0043506487,\n      -0.0001522175,   -0.02115512,   -0.010462675,   0.007636461,\n          0.0301948,  -0.006009675,    -0.01135165,  -0.036605343,\n         0.04006906,   0.036888044,  -0.0016293195,   0.013241053,\n       0.0005548855,   0.008130081,    0.027193218,  0.0047560516,\n        0.023012726,  -0.014274387,    0.008621267,  -0.016665483,\n       -0.016523534,  -0.021947058,  -0.0077380626,  -0.008166752,\n       -0.010050893, -0.0074697966,    0.021521091,  0.0086479345,\n       -0.008508939,   -0.03031165,  -0.0068692113,   0.032342624,\n       -0.003118368,  -0.009117541, -0.00006816292,   0.028233083,\n       -0.008163683,  -0.029179588,   -0.034861602,  -0.009573525,\n       -0.020023588,  -0.023040103,   0.0030518328,  -0.024019923,\n      ... 412 more items\n    ]\n  },\n  {\n    values: [\n        0.010123913,  -0.024184551,  0.0024574941,   -0.00984163, -0.0060574994,\n       -0.007628851,   0.013202136,  -0.027927121, -0.0016973788,  -0.014774812,\n       -0.011437808,  -0.019120526, -0.0063477424, -0.0050772373,    0.12938297,\n        0.006073787, -0.0055986797,   0.030279782,   0.015260121, -0.0014168695,\n       -0.006316713,  0.0007294639,  -0.034072377,   0.013348729,  0.0051308265,\n      -0.0042954376,  -0.009459755,  -0.012910496,   0.010751937, -0.0017263377,\n        -0.02083192,  0.0054532792,   0.008046588,  0.0015794274, -0.0045236745,\n       0.0077354256,  -0.009697459,   0.006621996,    -0.0447099,  -0.019261474,\n       0.0050193793,   0.010624901,   0.036847603,  -0.014380205,   0.023050537,\n        0.019384636,    0.03039269,   -0.02306347,  -0.025763597,   0.017585728,\n       0.0056267884,  -0.014494471,  -0.013168205,   -0.18764982,   0.011082365,\n        0.007989808, -0.0069600893,  0.0019873218,  -0.020733004,  -0.011488622,\n       0.0072846347,  -0.022266442,  -0.021857709,  -0.040680353,  0.0043984484,\n        0.016409805,  0.0010387278,   0.028186318,  -0.020797107,   0.007164954,\n       -0.007931046,   0.011955907,  0.0070153666,   -0.03028713,   0.039638296,\n      -0.0005224554,  -0.008104055,  -0.021054681,   0.017767426,   -0.01705528,\n      -0.0015202612,   0.027076574,  -0.008269598,  0.0041972124,  -0.009893149,\n      -0.0059321057,   -0.02742561,   0.011967838, -0.0012843752,  -0.012446694,\n        0.013188314,    0.01000231,  0.0063591595,  -0.013250329,   -0.00891349,\n       -0.011323209, 0.00077099906,  -0.032252073,   0.017312435,  -0.010896756,\n      ... 412 more items\n    ]\n  }\n]\n\n\nYou’ll get a set of three embeddings, one for each piece of text you passed in:\n\nconsole.log((embedding_response.embeddings ?? []).length);\n\n3\n\n\nYou can also see the length of each embedding is 512, as per the output_dimensionality you specified.\n\nconst vector_1 = embedding_response.embeddings?.[0]?.values ?? [];\nconsole.log(vector_1.length);\n\n512",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Getting started with Gemini models"
    ]
  },
  {
    "objectID": "quickstarts/Get_started.html#next-steps",
    "href": "quickstarts/Get_started.html#next-steps",
    "title": "Gemini API: Getting started with Gemini models",
    "section": "Next Steps",
    "text": "Next Steps\n\nUseful API references:\nCheck out the Google GenAI SDK for more details on the new SDK. ### Related examples\nFor more detailed examples using Gemini models, check the Quickstarts folder of the cookbook. You’ll learn how to use the Live API, juggle with multiple tools or use Gemini 2.0 spatial understanding abilities.\nAlso check the Gemini thinking models that explicitly showcases its thoughts summaries and can manage more complex reasonings.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Getting started with Gemini models"
    ]
  },
  {
    "objectID": "quickstarts/Grounding.html",
    "href": "quickstarts/Grounding.html",
    "title": "Gemini API: Getting started with information grounding for Gemini models",
    "section": "",
    "text": "In this notebook you will learn how to use information grounding with Gemini models.\nInformation grounding is the process of connecting these models to specific, verifiable information sources to enhance the accuracy, relevance, and factual correctness of their responses. While LLMs are trained on vast amounts of data, this knowledge can be general, outdated, or lack specific context for particular tasks or domains. Grounding helps to bridge this gap by providing the LLM with access to curated, up-to-date information.\nHere you will experiment with:",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Getting started with information grounding for Gemini models"
    ]
  },
  {
    "objectID": "quickstarts/Grounding.html#setup",
    "href": "quickstarts/Grounding.html#setup",
    "title": "Gemini API: Getting started with information grounding for Gemini models",
    "section": "Setup",
    "text": "Setup\n\nInstall the Google GenAI SDK\nInstall the Google GenAI SDK from npm.\n$ npm install @google/genai\n\n\nSetup your API key\nYou can create your API key using Google AI Studio with a single click.\nRemember to treat your API key like a password. Don’t accidentally save it in a notebook or source file you later commit to GitHub. In this notebook we will be storing the API key in a .env file. You can also set it as an environment variable or use a secret manager.\nHere’s how to set it up in a .env file:\n$ touch .env\n$ echo \"GEMINI_API_KEY=&lt;YOUR_API_KEY&gt;\" &gt;&gt; .env\n\n\n\n\n\n\nTip\n\n\n\nAnother option is to set the API key as an environment variable. You can do this in your terminal with the following command:\n$ export GEMINI_API_KEY=\"&lt;YOUR_API_KEY&gt;\"\n\n\n\n\nLoad the API key\nTo load the API key from the .env file, we will use the dotenv package. This package loads environment variables from a .env file into process.env.\n$ npm install dotenv\nThen, we can load the API key in our code:\n\nconst dotenv = require(\"dotenv\") as typeof import(\"dotenv\");\n\ndotenv.config({\n  path: \"../.env\",\n});\n\nconst GEMINI_API_KEY = process.env.GEMINI_API_KEY ?? \"\";\nif (!GEMINI_API_KEY) {\n  throw new Error(\"GEMINI_API_KEY is not set in the environment variables\");\n}\nconsole.log(\"GEMINI_API_KEY is set in the environment variables\");\n\nGEMINI_API_KEY is set in the environment variables\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn our particular case the .env is is one directory up from the notebook, hence we need to use ../ to go up one directory. If the .env file is in the same directory as the notebook, you can omit it altogether.\n│\n├── .env\n└── quickstarts\n    └── Grounding.ipynb\n\n\n\n\nInitialize SDK Client\nWith the new SDK, now you only need to initialize a client with you API key (or OAuth if using Vertex AI). The model is now set in each call.\n\nconst google = require(\"@google/genai\") as typeof import(\"@google/genai\");\n\nconst ai = new google.GoogleGenAI({ apiKey: GEMINI_API_KEY });\n\n\n\nSelect a model\nNow select the model you want to use in this guide, either by selecting one in the list or writing it down. Keep in mind that some models, like the 2.5 ones are thinking models and thus take slightly more time to respond (cf. thinking notebook for more details and in particular learn how to switch the thiking off).\n\nconst tslab = require(\"tslab\") as typeof import(\"tslab\");\n\nconst MODEL_ID = \"gemini-2.5-flash-preview-05-20\";",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Getting started with information grounding for Gemini models"
    ]
  },
  {
    "objectID": "quickstarts/Grounding.html#use-google-search-grounding",
    "href": "quickstarts/Grounding.html#use-google-search-grounding",
    "title": "Gemini API: Getting started with information grounding for Gemini models",
    "section": "Use Google Search grounding",
    "text": "Use Google Search grounding\nGoogle Search grounding is particularly useful for queries that require current information or external knowledge. Using Google Search, Gemini can access nearly real-time information and better responses.\n\nconst search_grounding = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: \"What was the latest Indian Premier League match and who won?\",\n  config: {\n    tools: [{ googleSearch: {} }],\n  },\n});\n\ntslab.display.markdown(search_grounding.text ?? \"\");\n\nconsole.log(\n  \"Search Query:\",\n  JSON.stringify(search_grounding.candidates?.[0]?.groundingMetadata?.webSearchQueries, null, 2)\n);\nconsole.log(\n  \"Search Pages:\",\n  JSON.stringify(search_grounding.candidates?.[0]?.groundingMetadata?.groundingChunks, null, 2)\n);\n\ntslab.display.html(search_grounding.candidates?.[0]?.groundingMetadata?.searchEntryPoint?.renderedContent ?? \"\");\n\nThe latest Indian Premier League (IPL) match was the IPL 2025 Final, played on June 3, 2025, in Ahmedabad.\nRoyal Challengers Bengaluru (RCB) won the match against Punjab Kings (PBKS) by 6 runs, securing their maiden IPL title. Virat Kohli was emotional after RCB’s historic win. Krunal Pandya was named the Man of the Match for his economical bowling performance.\n\n\nSearch Query: [\n  \"latest Indian Premier League match\",\n  \"who won the latest IPL match\",\n  \"IPL 2025 latest match\"\n]\nSearch Pages: [\n  {\n    \"web\": {\n      \"uri\": \"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHt5kdqdKChCOa4Pr_VPq-gkhoamwHE_iJ7o5RjLXIZzeI8TlkXQAR1XQHWyRNevZPTZteSGYR_ktkeCXuaJNMzEtiY1vDOiYAqqwRHmf-0khw3lJECXDBYSMcf6EWuAY1ClUwt7u3TE77DH8qSQYstjGe7dmNu8vUX\",\n      \"title\": \"indiatimes.com\"\n    }\n  },\n  {\n    \"web\": {\n      \"uri\": \"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFCh5VcEGh9Ppqbg8BuGhf2X6LdqlJu9t5zENDQxiarqynpQBAcWuAJgCxLJxkhc223LqJDgX5VVrqUD-9LLks_RJT8OUwTLUnpLH2Ik8doMHLfo54eI5AlWXNiWQHqMXCbmJRBbhBIu1sUuyuS4O7h9SR4VQvO4aun6b--Z2Tci0mGK_1VAJA_Nq-3_FNih6pXFq3Fv1Q8aBCc9ej_pB6g9M4f6EcWKKO2hs-oaKzR3BefFnnWrKYSri7iel8au4UEcPvgkCPBjdLPz-yFkM1apJvTvbUduUXfhGCvDuRDwfFEsoyT_7IdM9YhaXo=\",\n      \"title\": \"hindustantimes.com\"\n    }\n  },\n  {\n    \"web\": {\n      \"uri\": \"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGvvHqeHxsXknsFovCD7wo_veKNQ6iYuWG9DskR4DJ6snVn3D8SnMM87g4L18UaMAO7r2Awtm38AOyarUmOmJl0-8WEOKoRokRnclqUow5wXIXYiEkppq8QXHQvmdGUmGsnqtNBD8ihXgEZsTKjADNFvAbcHAi1KAqreLNSwRUrZJeZZuyh8Q4cmP-sQr641mN93E-U8-brpV0sioq8BTxjGjqOBygrtiVK50ii1_T331e-zV0C43LEuseCLnGK2EhcgDVwr3sTBWYBBGPpayIMUHF5YiKLvginlkqaVsljdJg2PRtarWaZM2tmNMWVe5tAw7arRDyU4l0J_0s=\",\n      \"title\": \"economictimes.com\"\n    }\n  },\n  {\n    \"web\": {\n      \"uri\": \"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHmjQGwwY0-0xsbJwE6Iiq0YeANNMylO8O7QqED6QR0DjWMPWKCVJ7BbFlCtjpnNixJqtuNHhaZnxX7Dm9wpvxATPJ4VcZHG3_bR5gvgWIcN815qb90gvYjQdvgZWfp9O974BgMbTMZPZTuX-Bh\",\n      \"title\": \"hindustantimes.com\"\n    }\n  },\n  {\n    \"web\": {\n      \"uri\": \"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEUFnrVepN1H1fgVoziCx8PvUEv8DNW5sY_yi6nfJZmYkgXZZ8grAr3f-Xem__PJGjLLj-tWs7H3RPDym3dLiwculxhndrCJiMmLR05FAqHAkOeKWP4rTTOtfInazSk0kZCgrRS3w==\",\n      \"title\": \"thehindu.com\"\n    }\n  },\n  {\n    \"web\": {\n      \"uri\": \"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEM79AM748uZRyPEo299owu6dh5IWJvZ9zN2r9Hsvqze0jlz3qWdydYu77QE_pTR68fleN_TvnWelgL3bay37V8im1EmRsYKQ6waMkotgEma4D_BoRwMSU2G6tHyfuwdc6kVTA0OMdbp-0tyQz25UI9r2Trrl8upNhY5Ghlc1N8sUdlS36fvFv9pzMvAXnlMCU=\",\n      \"title\": \"jagranjosh.com\"\n    }\n  },\n  {\n    \"web\": {\n      \"uri\": \"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGXhGLM_mklCri2YB0fS1v_xpfcTItWFFBMRSCIf6MYzMwhs8ebT9nDdMq1PL8pa0OwKcL50rcD5qZ-N2ShSQsI14HQ8XifZy5lQPpLOKwACpHWkkDQPQge0EXAzO2vWzsEwWkpbW6Xd0UqeO8tiMoUZR0J8jg8NHUiatz14S2rdYoiX_szYkRCNRw0e_0=\",\n      \"title\": \"olympics.com\"\n    }\n  },\n  {\n    \"web\": {\n      \"uri\": \"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGyyx6UMCzkXbOfQYKW6vf99hvJrupL4nB-46oHtG-n79CcqG7bP6Yw0OCF5hjcwmc5TJG2YzBR7Cr2v9KPApOFm_Cl8AbFTwzxy-A3K6mfixgF6mkGYOeubi-HGVcf_IhSvECdhleOapVK\",\n      \"title\": \"careerpower.in\"\n    }\n  },\n  {\n    \"web\": {\n      \"uri\": \"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEomdtJaCbrT3KBtlXb1bzxqBz3dQpRn34LgUhKW-wM2Ds5fzsQK5PPOhIYD8DM7KQXCbnj8QZUE9C1fSTjmfHGI0ZujDFAgBiQ1q1woWKKIbKIkvtb-oofk69NW7oldahhbYOuR6fG77IRf2XPdcBS-8eDkZy5blZUeQf3S4L17qrH_p_juMQ=\",\n      \"title\": \"iplt20.com\"\n    }\n  }\n]\n\n\n\n\n  \n    \n      \n      \n      \n      \n    \n    \n      \n      \n      \n      \n      \n    \n    \n  \n  \n    IPL 2025 latest match\n    latest Indian Premier League match\n    who won the latest IPL match\n  \n\n\n\nYou can see that running the same prompt without search grounding gives you outdated information:\n\nconst without_search_grounding = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: \"What was the latest Indian Premier League match and who won?\",\n});\ntslab.display.markdown(without_search_grounding.text ?? \"\");\n\nThe latest Indian Premier League (IPL) match was the Final of the 2024 season, played on May 26, 2024.\nIt was between:\n\nKolkata Knight Riders (KKR)\nSunrisers Hyderabad (SRH)\n\nKolkata Knight Riders (KKR) won the match by 8 wickets, securing their third IPL title.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Getting started with information grounding for Gemini models"
    ]
  },
  {
    "objectID": "quickstarts/Grounding.html#grounding-with-youtube-links",
    "href": "quickstarts/Grounding.html#grounding-with-youtube-links",
    "title": "Gemini API: Getting started with information grounding for Gemini models",
    "section": "Grounding with YouTube links",
    "text": "Grounding with YouTube links\nyou can directly include a public YouTube URL in your prompt. The Gemini models will then process the video content to perform tasks like summarization and answering questions about the content.\nThis capability leverages Gemini’s multimodal understanding, allowing it to analyze and interpret video data alongside any text prompts provided.\nYou do need to explicitly declare the video URL you want the model to process as part of the contents of the request. Here a simple interaction where you ask the model to summarize a YouTube video:\n\nconst YOUTUBE_URL = \"https://www.youtube.com/watch?v=XV1kOFo1C8M\";\n\nconst youtube_grounding = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: [\"Summarize this video.\", google.createPartFromUri(YOUTUBE_URL, \"video/x-youtube\")],\n});\ntslab.display.markdown(youtube_grounding.text ?? \"\");\n\nThis video introduces “Gemma Chess,” showcasing how Google’s Gemma AI model can bring a “new dimension” to the game of chess. Ju-yeong Ji from Google DeepMind explains that Gemma isn’t designed to replace powerful, calculative chess engines like Stockfish (which excel at finding optimal moves) but rather to enhance the chess experience through its language understanding and generation capabilities.\nGemma offers several key applications:\n\nGame Analysis & Explanations: It can analyze chess games (using PGN data) and explain the strategic and tactical significance of moves in natural language, providing insights into why certain moves are interesting or impactful, even considering psychological aspects for human players.\nStorytelling: Gemma can transform game data into engaging narratives, bringing historical matches or personal games to life with descriptive language and emotional context.\nChess Learning Support: It acts as a “smart study buddy,” capable of explaining complex chess concepts (like openings such as the “Sicilian Defense” or specific tactical ideas like a “passed pawn”) in detail, adapting the explanation to the user’s skill level, and supporting multiple languages. It can also offer feedback on a player’s understanding.\n\nEssentially, Gemma combines the precise computational strength of traditional chess AI with its own ability to interpret and communicate complex information in a human-like way, making chess learning and analysis more intuitive and accessible for everyone.\n\n\nBut you can also use the link as the source of truth for your request. In this example, you will first ask how Gemma models can help on chess games:\n\nconst gemma_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: \"How Gemma models can help on chess games?\",\n});\ntslab.display.markdown(gemma_response.text ?? \"\");\n\nGemma models, as Large Language Models (LLMs) developed by Google, are primarily designed for natural language understanding and generation. This means they operate on text, not directly on the board state, moves, or strategic calculations like a traditional chess engine (e.g., Stockfish, AlphaZero).\nTherefore, Gemma models cannot play chess, calculate moves, or evaluate positions with the accuracy and depth of dedicated chess engines.\nHowever, they can be incredibly helpful in chess games and study in indirect, linguistic, and informational ways:\n\nLearning and Education:\n\nExplaining Rules and Concepts: Ask Gemma to explain what a “fork,” “pin,” “discovered attack,” or “zugzwang” is. It can provide clear, concise definitions and examples.\nTeaching Openings: It can describe common opening principles, explain the ideas behind specific openings (e.g., “What are the main ideas in the Ruy Lopez?”), and list common variations.\nAnalyzing Puzzles and Positions (Textual): You can describe a position (e.g., “White to move, King on g1, Queen on d1, Rook on a1… can White win?”) and ask for a general strategic idea or what a good move might be, based on common chess principles it learned from its training data. It won’t calculate precisely but can offer high-level advice.\nCreating Study Plans: You could ask for a beginner’s study plan, topics to focus on, or recommendations for improving specific aspects of your game.\n\nGame Analysis and Commentary (Prose):\n\nExplaining Game Phases: Ask it to describe what typically happens in the opening, middlegame, and endgame.\nGenerating Commentary: Provide a sequence of moves (in algebraic notation) and ask Gemma to generate natural language commentary, explaining what’s happening or the likely intent behind moves.\nSummarizing Games: Give it a PGN (Portable Game Notation) or a list of moves, and it can try to summarize the key moments, strategic themes, or turning points.\nTranslating Chess Notation: Convert algebraic notation into natural language descriptions, e.g., “e4 e5 Nf3 Nc6” -&gt; “White moves their king’s pawn two squares, Black responds similarly, then White develops their knight to f3, and Black develops their knight to c6.”\n\nContent Creation:\n\nWriting Articles and Blogs: Generate outlines or draft content for articles about chess history, famous players, specific openings, or strategic concepts.\nCreating Quizzes: Ask it to generate multiple-choice questions about chess rules, history, or basic tactics.\nScripting Videos: Help draft scripts for chess lessons or game analysis videos.\n\nHistorical and Conceptual Knowledge:\n\nRecalling Famous Games/Players: Ask about legendary matches, famous blunders, or the achievements of grandmasters.\nUnderstanding Chess Terminology: Clarify the meaning of obscure or advanced chess terms.\n\n\nKey Limitations to Remember:\n\nNo Positional “Understanding”: Gemma models don’t “see” the board or calculate moves like a chess engine. Their understanding is based on patterns and relationships in the text they were trained on.\nNo Tactical Depth: They cannot calculate deep tactical lines, predict opponent responses accurately, or find the “best” move in a complex position.\nPotential for Hallucination: Like any LLM, Gemma can sometimes generate plausible-sounding but incorrect information, especially when asked for precise strategic or tactical advice that requires deep calculation.\nRelies on Training Data: Its knowledge is limited to what it learned from its vast text dataset. If a niche chess concept or a very recent game isn’t in its training data, it won’t know about it.\n\nIn summary, Gemma models are fantastic linguistic assistants for chess. They can help you learn, explain, and create content about chess, but they are not a substitute for a dedicated chess engine when it comes to playing, calculating, or deep positional analysis.\n\n\nAnd then you can ask the same question, now having the YouTube video as context to be used by the model:\n\nconst gemma_grounding = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: [\"How Gemma models can help on chess games?\", google.createPartFromUri(YOUTUBE_URL, \"video/x-youtube\")],\n});\ntslab.display.markdown(gemma_grounding.text ?? \"\");\n\nGemma models can help on chess games by bringing a “new dimension” to the experience, focusing on human understanding and interaction rather than solely on raw computational power. Here’s how:\n\nEasier Chess Analysis and Explanations:\n\nDemystifying Engine Output: Traditional chess engines often provide technical numbers and complex move sequences that are hard for humans to understand. Gemma can take this technical data and translate it into plain, understandable text.\nExplaining Moves: It can explain why a particular move is good, outlining the strategic ideas, tactical advantages, and potential dangers associated with it.\nSummarizing Complexities: For intricate parts of a game, Gemma can summarize key tactical and strategic moments, helping players quickly grasp important takeaways.\n\nStorytelling and Narrative Generation:\n\nBringing Games to Life: Gemma can analyze a chess game (including context like players and tournaments) and generate a compelling narrative about how the game unfolded. This makes reviewing past games a more engaging and immersive experience than just looking at move notation.\nAdding Context and Emotion: It can imbue the game analysis with a “backstory” or emotional context, making the “aha!” moments of a game more impactful.\n\nPersonalized Chess Learning and Coaching:\n\nIntelligent Study Buddy: Gemma can act as a personalized chess coach, explaining concepts like openings (e.g., the Sicilian Defense) in an easy-to-understand manner.\nTailored Explanations: It can adapt its explanations to the user’s skill level (beginner, intermediate, advanced) and even provide them in different languages (e.g., Korean).\nTargeted Feedback: Gemma can provide feedback on a player’s understanding of chess ideas and point out areas where they might need to improve, making the learning process more efficient and personalized.\n\nEnhanced Interaction with Chess Engines:\n\nBridging AI and Human Understanding: By combining the brute-force calculation strength of traditional chess AI (like AlphaZero) with Gemma’s linguistic capabilities, it offers a more intuitive approach to learning and analyzing chess. It can interpret the engine’s optimal moves and explain the underlying logic in a human-friendly way.\n\n\n\n\nNow your answer is more insightful for the topic you want, using the knowledge shared on the video and not necessarily available on the model knowledge.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Getting started with information grounding for Gemini models"
    ]
  },
  {
    "objectID": "quickstarts/Grounding.html#grounding-information-using-url-context",
    "href": "quickstarts/Grounding.html#grounding-information-using-url-context",
    "title": "Gemini API: Getting started with information grounding for Gemini models",
    "section": "Grounding information using URL context",
    "text": "Grounding information using URL context\nThe URL Context tool empowers Gemini models to directly access and process content from specific web page URLs you provide within your API requests. This is incredibly interesting because it allows your applications to dynamically interact with live web information without needing you to manually pre-process and feed that content to the model.\nURL Context is effective because it allows the models to base its responses and analysis directly on the content of the designated web pages. Instead of relying solely on its general training data or broad web searches (which are also valuable grounding tools), URL Context anchors the model’s understanding to the specific information present at those URLs.\n\nconst url_context_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: [\n    `\n    based on https://ai.google.dev/gemini-api/docs/models, what are the key\n    differences between Gemini 1.5, Gemini 2.0 and Gemini 2.5 models?\n    Create a markdown table comparing the differences.\n    `,\n  ],\n  config: {\n    tools: [{ urlContext: {} }],\n  },\n});\ntslab.display.markdown(url_context_response.text ?? \"\");\n\nThe Gemini API offers various models optimized for different use cases, with Gemini 1.5, Gemini 2.0, and Gemini 2.5 representing different generations and capabilities. The key differences between the main variants are summarized in the table below.\n\n\n\n\n\n\n\n\n\n\n\nFeature\nGemini 1.5 Pro\nGemini 1.5 Flash\nGemini 2.0 Flash\nGemini 2.5 Pro (Preview)\nGemini 2.5 Flash (Preview)\n\n\n\n\nPrimary Use Case / Optimization\nMid-size multimodal model optimized for a wide range of reasoning tasks; excels at processing large amounts of data.\nFast and versatile multimodal model for scaling across diverse tasks.\nNext-generation features and improved capabilities, superior speed, native tool use, built for agentic experiences.\nMost powerful thinking model with maximum response accuracy and state-of-the-art performance; best for complex coding, reasoning, and multimodal understanding.\nBest model in terms of price-performance, offering well-rounded capabilities; ideal for low-latency, high-volume tasks requiring thinking.\n\n\nInput Modalities\nAudio, images, video, text\nAudio, images, video, text\nAudio, images, video, text\nAudio, images, video, text\nAudio, images, video, text\n\n\nOutput Modalities\nText\nText\nText\nText\nText\n\n\nInput Token Limit\n2,097,152 (2M)\n1,048,576 (1M)\n1,048,576 (1M)\n1,048,576 (1M)\n1,048,576 (1M)\n\n\nKey Capabilities\nSystem instructions, JSON mode, JSON schema, adjustable safety settings, caching, function calling, code execution.\nSystem instructions, JSON mode, JSON schema, adjustable safety settings, caching, tuning, function calling, code execution.\nStructured outputs, caching, function calling, code execution, search grounding, Live API. Thinking is experimental.\nStructured outputs, caching, function calling, code execution, search grounding, thinking.\nAdaptive thinking, cost efficiency, structured outputs, caching, function calling, code execution, search grounding, thinking.\n\n\nStatus\nLatest Stable\nLatest Stable\nLatest Stable (also Experimental and Stable versions)\nPreview\nPreview\n\n\n\n\n\nAs a reference, you can see how the answer would be without the URL context, using the official models documentation as reference:\n\nconst without_url_context_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: [\n    `\n        what are the key differences between Gemini 1.5, Gemini 2.0 and Gemini 2.5\n        models? Create a markdown table comparing the differences.\n        `,\n  ],\n});\ntslab.display.markdown(without_url_context_response.text ?? \"\");\n\nIt seems there might be a slight misunderstanding regarding the versioning of Gemini models. As of my last update, Google has not publicly released models named “Gemini 2.0” or “Gemini 2.5.”\nThe publicly announced and available Gemini models follow this progression:\n\nGemini 1.0 (Pro, Ultra, Nano): The initial general release of the Gemini family.\nGemini 1.5 Pro: A significant upgrade focusing on a massive context window and Mixture-of-Experts (MoE) architecture.\nGemini 1.5 Flash: A faster, more cost-effective version of 1.5 Pro, optimized for high-volume, lower-latency tasks.\n\nTherefore, I will provide a comparison between Gemini 1.0 (representing the initial family), Gemini 1.5 Pro, and Gemini 1.5 Flash, as these are the relevant and distinct models in the Gemini lineup today.\nHere’s a breakdown of their key differences:\n\n\nComparison of Gemini Models (1.0 vs. 1.5 Pro vs. 1.5 Flash)\n\n\n\n\n\n\n\n\n\nFeature\nGemini 1.0 (Pro/Ultra/Nano)\nGemini 1.5 Pro\nGemini 1.5 Flash\n\n\n\n\nRelease/Announced\nDecember 2023 (Pro/Ultra); Early 2024 (Nano)\nFebruary 2024 (Limited preview); May 2024 (Broader availability)\nMay 2024 (Announced alongside 1.5 Pro’s broader release)\n\n\nPrimary Focus\nFoundational multimodal capabilities, general-purpose reasoning\nMassive context window, advanced long-context understanding\nSpeed & efficiency, optimized for high-volume, low-latency tasks\n\n\nArchitecture\nHighly optimized transformer architecture\nMixture-of-Experts (MoE)\nMixture-of-Experts (MoE) (optimized for speed)\n\n\nContext Window\nUp to 32K tokens (for Pro)\n1 Million tokens (default), expandable to 2 Million tokens\n1 Million tokens (default), expandable to 2 Million tokens\n\n\nPerformance\nStrong general reasoning, coding, multimodal understanding\nExceptional long-document analysis, complex codebases, video analysis\nHigh throughput, good for simpler tasks, less “deep” reasoning than 1.5 Pro\n\n\nCost/Efficiency\nBalanced\nHigher cost per token due to advanced capabilities\nSignificantly more cost-effective and faster inference\n\n\nModality\nMultimodal (text, image, audio, video input/output)\nMultimodal (text, image, audio, video input/output)\nMultimodal (text, image, audio, video input/output)\n\n\nIdeal Use Cases\nChatbots, content generation, general AI applications\nSummarizing entire books/videos, analyzing large datasets, complex troubleshooting, long-form code analysis\nHigh-volume API calls, real-time chatbots, dynamic content updates, RAG without deep reasoning, quick summarization\n\n\nKey Differentiator\nFirst publicly available, versatile Gemini family\nUnprecedented long-context processing for multimodal data\nBlazing speed and cost-efficiency for large-scale applications\n\n\n\n\nIn summary:\n\nGemini 1.0 established the baseline with strong general-purpose multimodal capabilities.\nGemini 1.5 Pro represents a monumental leap in the context window, allowing it to process and understand vast amounts of information (like entire novels, lengthy codebases, or hours of video) in a single prompt. Its MoE architecture contributes to this capability.\nGemini 1.5 Flash takes the MoE architecture from 1.5 Pro and optimizes it for speed and cost-efficiency, making it ideal for applications requiring high throughput where the deepest reasoning of 1.5 Pro isn’t strictly necessary. It retains the same large context window as 1.5 Pro.\n\nIt’s possible that “Gemini 2.0” or “Gemini 2.5” refers to future internal development versions that have not yet been announced publicly. Google frequently iterates and develops models, and future major versions will undoubtedly bring even more advanced capabilities.\n\n\n\nAs you can see, using the model knowledge only, it does not know about the new Gemini 2.5 models family.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Getting started with information grounding for Gemini models"
    ]
  },
  {
    "objectID": "quickstarts/Grounding.html#comparison-of-gemini-models-1.0-vs.-1.5-pro-vs.-1.5-flash",
    "href": "quickstarts/Grounding.html#comparison-of-gemini-models-1.0-vs.-1.5-pro-vs.-1.5-flash",
    "title": "Gemini API: Getting started with information grounding for Gemini models",
    "section": "Comparison of Gemini Models (1.0 vs. 1.5 Pro vs. 1.5 Flash)",
    "text": "Comparison of Gemini Models (1.0 vs. 1.5 Pro vs. 1.5 Flash)\n\n\n\n\n\n\n\n\n\nFeature\nGemini 1.0 (Pro/Ultra/Nano)\nGemini 1.5 Pro\nGemini 1.5 Flash\n\n\n\n\nRelease/Announced\nDecember 2023 (Pro/Ultra); Early 2024 (Nano)\nFebruary 2024 (Limited preview); May 2024 (Broader availability)\nMay 2024 (Announced alongside 1.5 Pro’s broader release)\n\n\nPrimary Focus\nFoundational multimodal capabilities, general-purpose reasoning\nMassive context window, advanced long-context understanding\nSpeed & efficiency, optimized for high-volume, low-latency tasks\n\n\nArchitecture\nHighly optimized transformer architecture\nMixture-of-Experts (MoE)\nMixture-of-Experts (MoE) (optimized for speed)\n\n\nContext Window\nUp to 32K tokens (for Pro)\n1 Million tokens (default), expandable to 2 Million tokens\n1 Million tokens (default), expandable to 2 Million tokens\n\n\nPerformance\nStrong general reasoning, coding, multimodal understanding\nExceptional long-document analysis, complex codebases, video analysis\nHigh throughput, good for simpler tasks, less “deep” reasoning than 1.5 Pro\n\n\nCost/Efficiency\nBalanced\nHigher cost per token due to advanced capabilities\nSignificantly more cost-effective and faster inference\n\n\nModality\nMultimodal (text, image, audio, video input/output)\nMultimodal (text, image, audio, video input/output)\nMultimodal (text, image, audio, video input/output)\n\n\nIdeal Use Cases\nChatbots, content generation, general AI applications\nSummarizing entire books/videos, analyzing large datasets, complex troubleshooting, long-form code analysis\nHigh-volume API calls, real-time chatbots, dynamic content updates, RAG without deep reasoning, quick summarization\n\n\nKey Differentiator\nFirst publicly available, versatile Gemini family\nUnprecedented long-context processing for multimodal data\nBlazing speed and cost-efficiency for large-scale applications\n\n\n\n\nIn summary:\n\nGemini 1.0 established the baseline with strong general-purpose multimodal capabilities.\nGemini 1.5 Pro represents a monumental leap in the context window, allowing it to process and understand vast amounts of information (like entire novels, lengthy codebases, or hours of video) in a single prompt. Its MoE architecture contributes to this capability.\nGemini 1.5 Flash takes the MoE architecture from 1.5 Pro and optimizes it for speed and cost-efficiency, making it ideal for applications requiring high throughput where the deepest reasoning of 1.5 Pro isn’t strictly necessary. It retains the same large context window as 1.5 Pro.\n\nIt’s possible that “Gemini 2.0” or “Gemini 2.5” refers to future internal development versions that have not yet been announced publicly. Google frequently iterates and develops models, and future major versions will undoubtedly bring even more advanced capabilities.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Getting started with information grounding for Gemini models"
    ]
  },
  {
    "objectID": "quickstarts/Grounding.html#next-steps",
    "href": "quickstarts/Grounding.html#next-steps",
    "title": "Gemini API: Getting started with information grounding for Gemini models",
    "section": "Next steps",
    "text": "Next steps\n\nFor more details about using Google Search grounding, check out the Search Grounding cookbook.\nIf you are looking for another scenarios using videos, take a look at the Video understanding cookbook.\n\nAlso check the other Gemini capabilities that you can find in the Gemini quickstarts.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Getting started with information grounding for Gemini models"
    ]
  },
  {
    "objectID": "quickstarts/Audio.html",
    "href": "quickstarts/Audio.html",
    "title": "Gemini API: Audio Quickstart",
    "section": "",
    "text": "This notebook provides an example of how to prompt Gemini Flash using an audio file. In this case, you’ll use a sound recording of President John F. Kennedy’s 1961 State of the Union address.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Audio Quickstart"
    ]
  },
  {
    "objectID": "quickstarts/Audio.html#setup",
    "href": "quickstarts/Audio.html#setup",
    "title": "Gemini API: Audio Quickstart",
    "section": "Setup",
    "text": "Setup\n\nInstall the Google GenAI SDK\nInstall the Google GenAI SDK from npm.\n$ npm install @google/genai\n\n\nSetup your API key\nYou can create your API key using Google AI Studio with a single click.\nRemember to treat your API key like a password. Don’t accidentally save it in a notebook or source file you later commit to GitHub. In this notebook we will be storing the API key in a .env file. You can also set it as an environment variable or use a secret manager.\nHere’s how to set it up in a .env file:\n$ touch .env\n$ echo \"GEMINI_API_KEY=&lt;YOUR_API_KEY&gt;\" &gt;&gt; .env\n\n\n\n\n\n\nTip\n\n\n\nAnother option is to set the API key as an environment variable. You can do this in your terminal with the following command:\n$ export GEMINI_API_KEY=\"&lt;YOUR_API_KEY&gt;\"\n\n\n\n\nLoad the API key\nTo load the API key from the .env file, we will use the dotenv package. This package loads environment variables from a .env file into process.env.\n$ npm install dotenv\nThen, we can load the API key in our code:\n\nconst dotenv = require(\"dotenv\") as typeof import(\"dotenv\");\n\ndotenv.config({\n  path: \"../.env\",\n});\n\nconst GEMINI_API_KEY = process.env.GEMINI_API_KEY ?? \"\";\nif (!GEMINI_API_KEY) {\n  throw new Error(\"GEMINI_API_KEY is not set in the environment variables\");\n}\nconsole.log(\"GEMINI_API_KEY is set in the environment variables\");\n\nGEMINI_API_KEY is set in the environment variables\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn our particular case the .env is is one directory up from the notebook, hence we need to use ../ to go up one directory. If the .env file is in the same directory as the notebook, you can omit it altogether.\n│\n├── .env\n└── quickstarts\n    └── Audio.ipynb\n\n\n\n\nInitialize SDK Client\nWith the new SDK, now you only need to initialize a client with you API key (or OAuth if using Vertex AI). The model is now set in each call.\n\nconst google = require(\"@google/genai\") as typeof import(\"@google/genai\");\n\nconst ai = new google.GoogleGenAI({ apiKey: GEMINI_API_KEY });\n\n\n\nSelect a model\nNow select the model you want to use in this guide, either by selecting one in the list or writing it down. Keep in mind that some models, like the 2.5 ones are thinking models and thus take slightly more time to respond (cf. thinking notebook for more details and in particular learn how to switch the thiking off).\n\nconst tslab = require(\"tslab\") as typeof import(\"tslab\");\n\nconst MODEL_ID = \"gemini-2.5-flash-preview-05-20\";",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Audio Quickstart"
    ]
  },
  {
    "objectID": "quickstarts/Audio.html#upload-an-audio-file-with-the-file-api",
    "href": "quickstarts/Audio.html#upload-an-audio-file-with-the-file-api",
    "title": "Gemini API: Audio Quickstart",
    "section": "Upload an audio file with the File API",
    "text": "Upload an audio file with the File API\nTo use an audio file in your prompt, you must first upload it using the File API.\n\nconst fs = require(\"fs\") as typeof import(\"fs\");\nconst path = require(\"path\") as typeof import(\"path\");\n\nconst AUDIO_URL =\n  \"https://storage.googleapis.com/generativeai-downloads/data/State_of_the_Union_Address_30_January_1961.mp3\";\n\nconst downloadFile = async (url: string, filePath: string) =&gt; {\n  const response = await fetch(url);\n  if (!response.ok) {\n    throw new Error(`Failed to download image: ${response.statusText}`);\n  }\n  const buffer = await response.blob();\n  const bufferData = Buffer.from(await buffer.arrayBuffer());\n  fs.writeFileSync(filePath, bufferData);\n};\n\nconst audioPath = path.join(\"../assets\", \"audio.mp3\");\n\nawait downloadFile(AUDIO_URL, audioPath);\n\n\nconst audio_file = await ai.files.upload({\n  file: audioPath,\n  config: {\n    displayName: \"audio.mp3\",\n  },\n});",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Audio Quickstart"
    ]
  },
  {
    "objectID": "quickstarts/Audio.html#use-the-file-in-your-prompt",
    "href": "quickstarts/Audio.html#use-the-file-in-your-prompt",
    "title": "Gemini API: Audio Quickstart",
    "section": "Use the file in your prompt",
    "text": "Use the file in your prompt\n\nconst audio_summary_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: [\n    \"Listen carefully to the following audio file. Provide a brief summary\",\n    google.createPartFromUri(audio_file.uri ?? \"\", audio_file.mimeType ?? \"\"),\n  ],\n});\ntslab.display.markdown(audio_summary_response.text ?? \"\");\n\nOn January 30, 1961, President John F. Kennedy delivered his first State of the Union address, outlining significant challenges facing the United States. Domestically, he described an economy in recession with high unemployment, diminished growth, and rising prices. Internationally, he highlighted a widening balance of payments deficit and stressed the need to maintain the dollar’s strength.\nKennedy then detailed a range of global crises, including Chinese Communist pressures in Asia, civil strife in the Congo, Soviet influence in Latin America (specifically Cuba), and disarray within NATO. He affirmed the U.S. commitment to freedom and independence worldwide but warned against underestimating adversaries.\nThe President proposed a comprehensive strategy to address these issues, focusing on strengthening military, economic, and diplomatic tools. This included modernizing defense capabilities (e.g., accelerating the Polaris submarine program), establishing a new foreign aid program (“Alliance for Progress” for Latin America, expanded Food for Peace), creating a National Peace Corps, and renewing efforts in arms control, scientific cooperation with the Soviet Union, and supporting the United Nations.\nKennedy concluded by calling for decisive action and accountability within the government, emphasizing that public service should be driven by initiative and responsibility. He acknowledged the difficult period ahead but expressed confidence in the American people’s dedication to overcoming challenges and fulfilling the nation’s promise for freedom and peace globally.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Audio Quickstart"
    ]
  },
  {
    "objectID": "quickstarts/Audio.html#inline-audio",
    "href": "quickstarts/Audio.html#inline-audio",
    "title": "Gemini API: Audio Quickstart",
    "section": "Inline Audio",
    "text": "Inline Audio\nFor small requests you can inline the audio data into the request, like you can with images. Use PyDub to trim the first 10s of the audio:\n\nconst ffmpeg = require(\"fluent-ffmpeg\") as typeof import(\"fluent-ffmpeg\");\n\nconst sliceAudio = async (audioBuffer: ArrayBuffer, durationMs: number): Promise&lt;ArrayBuffer&gt; =&gt;\n  new Promise((resolve, reject) =&gt; {\n    const tempDir = path.resolve(\"../temp\");\n    if (!fs.existsSync(tempDir)) {\n      fs.mkdirSync(tempDir);\n    }\n\n    const tempInputPath = path.join(tempDir, \"temp_input.mp3\");\n    const tempOutputPath = path.join(tempDir, \"temp_output.mp3\");\n\n    fs.writeFileSync(tempInputPath, Buffer.from(audioBuffer));\n\n    ffmpeg(tempInputPath)\n      .setStartTime(0)\n      .setDuration(durationMs / 1000)\n      .output(tempOutputPath)\n      .on(\"end\", () =&gt; {\n        try {\n          const outputBuffer = fs.readFileSync(tempOutputPath).buffer;\n\n          // Cleanup temp files\n          fs.unlinkSync(tempInputPath);\n          fs.unlinkSync(tempOutputPath);\n\n          resolve(outputBuffer);\n        } catch (err) {\n          reject(new Error(`Error reading output file: ${err.message}`));\n        }\n      })\n      .on(\"error\", (err) =&gt; {\n        reject(new Error(`FFmpeg error: ${err.message}`));\n      })\n      .run();\n  });\n\n// slice the first 10 seconds of the audio file\nconst audioBuffer = fs.readFileSync(audioPath).buffer;\nconst slicedAudioBuffer = await sliceAudio(audioBuffer, 30000);\nconsole.log(\"Sliced audio buffer length:\", slicedAudioBuffer.byteLength);\n\nSliced audio buffer length: 480906\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote the following about providing audio as inline data:\n\nThe maximum request size is 20 MB, which includes text prompts, system instructions, and files provided inline. If your file’s size will make the total request size exceed 20 MB, then use the File API to upload files.\nIf you’re using an audio sample multiple times, it is more efficient to use the File API.\n\n\n\n\nconst inline_audio_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: [\n    \"Describe this audio clip\",\n    google.createPartFromBase64(Buffer.from(slicedAudioBuffer).toString(\"base64\"), \"audio/mp3\"),\n  ],\n});\ntslab.display.markdown(inline_audio_response.text ?? \"\");\n\nThe audio clip primarily features human speech by two distinct male voices.\n\nThe first voice, an announcer or narrator, clearly states the context of the event: “The President’s State of the Union address to a joint session of the Congress from the rostrum of the House of Representatives, Washington, D.C., January 30th, 1961.”\nAfter a brief pause, the second voice, presumably the main speaker, begins addressing an audience with formal salutations like “Mr. Vice President” and “Members of the Congress,” before stating, “It is a pleasure to return from whence I came.”\n\nThroughout the clip, a noticeable, low-frequency hum or buzzing sound is consistently present in the background. The overall audio quality suggests an archival recording with a somewhat dated feel, yet the speech remains clear.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Audio Quickstart"
    ]
  },
  {
    "objectID": "quickstarts/Audio.html#get-a-transcript-of-the-audio-file",
    "href": "quickstarts/Audio.html#get-a-transcript-of-the-audio-file",
    "title": "Gemini API: Audio Quickstart",
    "section": "Get a transcript of the audio file",
    "text": "Get a transcript of the audio file\nTo get a transcript, just ask for it in the prompt. For example:\n\nconst transcript_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: [\n    \"Generate a transcript of the speech.\",\n    google.createPartFromUri(audio_file.uri ?? \"\", audio_file.mimeType ?? \"\"),\n  ],\n});\ntslab.display.markdown(transcript_response.text ?? \"\");\n\n\nRefer to timestamps in the audio file\nA prompt can specify timestamps of the form MM:SS to refer to particular sections in an audio file. For example:\n\nconst timestamped_transcript_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: [\n    \"Provide a transcript of the speech between the timestamps 02:30 and 03:29.\",\n    google.createPartFromUri(audio_file.uri ?? \"\", audio_file.mimeType ?? \"\"),\n  ],\n});\ntslab.display.markdown(timestamped_transcript_response.text ?? \"\");\n\nI speak today in an hour of national peril and national opportunity. Before my term has ended, we shall have to test anew whether a nation organized and governed such as ours can endure. The outcome is by no means certain. The answers are by no means clear. All of us together, this administration, this Congress, this nation, must forge those answers. But today, were I to offer after little more than a week in office, detailed legislation to remedy every national ill, the Congress would rightly wonder whether the desire for speed had replaced the duty of responsibility. My remarks, therefore, will be limited, but they will also be candid. To state the facts frankly is not to despair the future, nor to indict the past. The prudent heir takes careful inventory of his legacies and gives a faithful accounting to those whom he owes an obligation of trust. And while the occasion does not call for another recital of our blessings and assets, we do have no greater asset than the willingness of a free and determined people, through its elected officials, to face all problems frankly and meet all dangers free from panic or fear. The present state of our economy is disturbing. We take office in the wake of seven months of recession, three and one half years of slack, seven years of diminished economic growth, and nine years of falling farm income. Business bankruptcies have reached their highest level since the Great Depression. Since 1951, farm income has been squeezed down by 25%. Save for a brief period in 1958, insured unemployment is at the highest peak in our history. Of some five and one half million Americans who are without jobs, more than 1 million have been searching for work for more than four months. And during each month, some 150,000 workers are exhausting their already meager jobless benefit rights. Nearly 1/8 of those who are without jobs live almost without hope in nearly 100 specially distressed and troubled areas. The rest include new school graduates, unable to use their talents. Farmers forced to give up their part-time jobs which helped balance their family budgets. Skilled and unskilled workers laid off in such important industries as metals, machinery, automobiles, and apparel. Our recovery from the 1958 recession, moreover, was anemic and incomplete. Our gross national product never regained its full potential. Unemployment never returned to normal levels. Maximum use of our national industrial capacity was never fully restored. In short, the American economy is in trouble. The most resourceful industrialized country on Earth ranks among the last in the rate of economic growth. Since last spring, our economic growth rate has actually declined. Business investment is in a decline. Profits have fallen below predicted levels. Construction is off. A million unsold automobiles are in inventory. Fewer people are working, and the average work week has shrunk well below 40 hours. Yet prices have continued to rise, so that now too many Americans have less to spend for items that cost more to buy. Economic prophecy is at best an uncertain art. As demonstrated by the prediction one year ago from this same podium that 1960 would be, and I quote, “the most prosperous year in our history.” Unquote. Nevertheless, forecast of continued slack and only slightly reduced unemployment through 1961 and 1962 have been made with alarming unanimity. And this administration does not intend to stand helplessly by. We cannot afford to waste idle hours and empty plant while awaiting the re-end of the recession. We must show the world what a free economy can do to reduce unemployment, to put unused capacity to work, to spur new productivity, and to foster high economic growth within a range of sound fiscal policies and relative price stability. I will propose to the Congress within the next 14 days measures to improve unemployment compensation through temporary increases in duration on a self-supporting basis, to provide more food for the families of the unemployed and to aid their needy children, to redevelop our areas of chronic labor surplus, to expand the services of the U.S. Employment Office, to stimulate housing and construction, to secure more purchasing power for our lowest paid workers by raising and expanding the minimum wage, to offer tax incentives for sound plant investment, to increase the development of our natural resources, to encourage price stability, and to take other steps aimed at ensuring a prompt recovery and paving the way for increased long-range growth. This is not a partisan program concentrating on our weaknesses. It is, I hope, a national program to realize our national strength. Efficient expansion at home, stimulating the new plant and technology that can make our goods more competitive is also the key to the international balance of payments problem. Laying aside all alarmist talk and panicky solutions, let us put that knotty problem in its proper perspective. It is true that since 1958, the gap between the dollars we spend or invest abroad and the dollars returned to us has substantially widened. This overall deficit in our balance of payments increased by nearly $11 billion in the last three years, and holders of dollars abroad converted them to gold in such a quantity as to cause a total outflow of nearly $5 billion of gold from our reserves. The 1959 deficit was caused in large part by our failure of our exports to penetrate foreign markets, the result both of restrictions on our goods and our own uncompetitive prices. The 1960 deficit, on the other hand, was more the result of an increase in private capital outflow, seeking new opportunity, higher return, or speculative advantage abroad. Meanwhile, this country has continued to bear more than its share of the West’s military and foreign aid obligations. Under existing policies, another deficit of $2 billion is predicted for 1961, and individuals in those countries whose dollar position once depended on these deficits for improvement now wonder aloud whether our gold reserves will remain sufficient to meet our own obligations. All this is cause for concern, but it is not cause for panic, for our monetary and financial position remains exceedingly strong. Including our drawing rights in the International Monetary Fund and the gold reserve held as backing for our currency and Federal Reserve deposits, we have some $22 billion in total gold stocks and other international monetary reserves, and I now pledge that their full strength stands behind the value of the dollar for use if needed. Moreover, we hold large assets abroad. The total owed this nation far exceeds the claim upon our reserves, and our exports once again substantially exceed our imports. In short, we need not and we shall not take any action to increase the dollar price of gold from $35 an ounce, to impose exchange controls, to reduce our anti-recession efforts, to fall back on restrictive trade policies or to weaken our commitments around the world. This administration will not distort the value of the dollar in any fashion, and this is a commitment. Prudence and good sense do require, however, that new steps be taken to ease the payment deficits and prevent any gold crisis. Our success in world affairs has long depended in part upon foreign confidence in our ability to pay. A series of executive orders, legislative remedies, and cooperative efforts with our allies will get underway immediately. Aimed at attracting foreign investment and travel to this country, promoting American exports at stable prices and with more liberal government guarantees and financing, curbing tax and custom loopholes that encourage undue spending of private dollars abroad, and through OECD, NATO, and otherwise, sharing with our allies all efforts to provide for the common defense of the free world and the hopes for growth of the less developed lands. While the current deficit last, ways will be found to ease our dollar outlays abroad without placing the full burden on the families of men who have asked to serve our flag overseas. In short, whatever is required will be done to back up all our efforts abroad and to make certain that in the future as in the past, the dollar is as sound as a dollar. But more than our exchange of international payments is out of balance. The current federal budget for fiscal 1961 is almost certain to show a net deficit. The budget already submitted for fiscal 1962 will remain in balance only if the Congress enacts all the revenue measures requested, and only if an earlier and sharper upturn in the economy than my economic advisors now think likely produces the tax revenues estimated. Nevertheless, a new administration must of necessity build on the spending and revenue estimates already submitted. Within that framework, barring the development of urgent national defense needs or a worsening of the economy, it is my current intention to advocate a program of expenditures which, including revenue from a stimulation of the economy, will not of and by themselves unbalance the earlier budget. However, we will do what must be done. For our national household is clutted with unfinished and neglected tasks. Our cities are being engulfed in squalor. 12 long years after Congress declared our goal to be a decent home and a suitable environment for every American family, we still have 25 million Americans living in sub-standard homes. A new housing program, under a new Housing and Urban Affairs Department will be needed this year. Our classrooms contain 2 million more children than they can properly have room for, taught by 90,000 teachers not properly qualified to teach. One third of our most promising high school graduates are financially unable to continue the development of their talents. The war babies of the 1940s who overcrowded our schools in the 1950s are now descending in 1960 upon our colleges, with two college students for every one, 10 years from now. And our colleges are ill prepared. We lack the scientists, the engineers, and the teachers our world obligations require. We have neglected oceanography, saline water conversion, and the basic research that lies at the root of all progress. Federal grants for both higher and public school education can no longer be delayed. Medical research has achieved new wonders, but these wonders are too often beyond the reach of too many people, owing to a lack of income, particularly among the aged, a lack of hospital beds, a lack of nursing homes, and a lack of doctors and dentists. Measures to provide health care for the aged under Social Security and to increase the supply of both facilities and personnel must be undertaken this year. Our supply of clean water is dwindling. Organized and juvenile crime cost the taxpayers millions of dollars every year, making it essential that we have improved enforcement and new legislative safeguards. The denial of constitutional rights to some of our fellow Americans on account of race at the ballot box and elsewhere disturbs the national conscience and subjects us to the charge of world opinion that our democracy is not equal to the high promise of our heritage. Morality in private business has not been sufficiently spurred by morality in public business. A host of problems and projects in all 50 states, though not possible to include in this message, deserves and will receive the attention of both the Congress and the Executive Branch. On most of these matters, messages will be sent to the Congress within the next two weeks. But all these problems pale when placed besides those which confront us around the world. No man entering upon this office, regardless of his party, regardless of his previous service in Washington, could fail to be staggered upon learning even in this brief 10-day period, the harsh enormities of the trials through which we must pass in the next four years. Each day the crises multiply. Each day their solution grows more difficult. Each day we draw nearer the hour of maximum danger as weapons spread and hostile forces grow stronger. I feel I must inform the Congress that our analysis over the last 10 days make it clear that in each of the principal areas of crisis, the tide of events has been running out and time has not been our friend. In Asia, the relentless pressures of the Chinese Communists menace the security of the entire area from the borders of India and South Vietnam to the jungles of Laos, struggling to protect its newly won independence. We seek in Laos what we seek in all Asia, and indeed in all of the world, freedom for the people and independence for the government, and this nation shall persevere in our pursuit of these objectives. In Africa, the Congo has been brutally torn by civil strife, political unrest, and public disorder. We shall continue to support the heroic efforts of the United Nations to restore peace and order, efforts which are now endangered by mounting tensions, unsolved problems, and decreasing support from many member states. In Latin America, communist agents seeking to exploit that region’s peaceful revolution of hope have established a base on Cuba, only 90 miles from our shore. Our objection with Cuba is not over the people’s drive for a better life. Our objection is to their domination by foreign and domestic tyranny. Cuban social and economic reforms should be encouraged. Questions of economics and trade policy can always be negotiated, but communist domination in this hemisphere can never be negotiated. We are pledged to work with our sister Republics to free the Americas of all such foreign domination and all tyranny, working towards the goal of a free hemisphere of free governments, extending from Cape Horn to the Arctic Circle. In Europe, our alliances are unfiled and in some disarray. The unity of NATO has been weakened by economic rivalry and partially eroded by national interest. It has not yet fully mobilized its resources nor fully achieved a common outlook. Yet no Atlantic power can meet on its own the mutual problems now facing us in defense, foreign aid, monetary reserves, and a host of other areas, and our close ties with those whose hopes and interests we share are among this nation’s most powerful assets. Our greatest challenge is still the world that lies beyond the Cold War. But the first great obstacle is still our relations with the Soviet Union and communist China. We must never be loud into believing that either power has yielded its ambitions for world domination, ambitions which they forcefully restated only a short time ago. On the contrary, our task is to convince them that aggression and subversion will not be profitable roots to pursue these ends. Open and peaceful competition for prestige, for market, for scientific achievement, even for men’s minds is something else again. For if freedom and communism would compete for man’s allegiance in a world at peace, I would look to the future with ever increasing confidence. To meet this array of challenges, to fulfill the role we cannot avoid on the world scene, we must re-examine and revise our whole arsenal of tools. One must not overshadow the other. On the presidential coat of arms, the American Eagle holds in his right talent the olive branch, while in his left he holds a bundle of arrows. We intend to give equal attention to both. First, we must strengthen our military tools. We are moving into a period of uncertain risk and great commitment in which both the military and diplomatic possibilities require a free world force so powerful as to make any aggression clearly futile. Yet in the past, lack of a consistent, coherent military strategy, the absence of basic assumptions about our national requirements, and the faulty estimate and duplication arising from inner service rivalries have all made it difficult to assess accurately how adequate or inadequate our defenses really are. I have therefore instructed the Secretary of Defense to reappraise our entire defense strategy. Our ability to fulfill our commitments, the effectiveness, vulnerability, and dispersal of our strategic basis, forces, and warning systems. The efficiency and economy of our operation and organization, the elimination of obsolete basis and installations, and the adequacy, modernization, and mobility of our present conventional and nuclear forces and weapon systems in the light of present and future dangers. I have asked for preliminary conclusions by the end of February, and I then shall recommend whatever legislative, budgetary, or executive action is needed in the light of these conclusions. In the meantime, I have asked the Defense Secretary to initiate immediately three steps most clearly needed now. First, I have directed prompt attention to increase our airlift capacity. Obtaining additional air transport mobility and obtaining it now will better assure the ability of our conventional forces to respond without discrimination and speed, with discrimination and speed, to any problem at any spot on the globe at any moment’s notice. In particular, it will enable us to meet any deliberate effort to avoid or divert our forces by starting limited wars in widely scattered parts of the globe. B, I have directed prompt action to step up our Polaris submarine program. Using using unobligated shipbuilding funds now to let contracts originally scheduled for the next fiscal year, will build and place on station at least nine months earlier than planned substantially more units of a crucial deterrent. A fleet that will never attack first, but possess sufficient powers of retaliation concealed beneath the sea to discourage any aggressor from launching an attack upon our security. C, I have directed prompt action to accelerate our entire missile program. Until the Secretary of Defense’s reappraisal is completed, the emphasis here will be largely on improved organization and decision making, on cutting down the wasteful duplications and the time lag that have handicapped our whole family of missiles. If we are to keep the peace, we need an invulnerable missile force, powerful enough to deter any aggressor from even threatening an attack that he would know could not destroy enough of our own forces to prevent his own destruction. For as I said upon taking the oath of office, only when our arms are sufficient beyond doubt, can we be certain beyond doubt that they will never be employed. Secondly, we must improve our economic tools. Our role is essential and unavoidable in the construction of a sound and expanding economy for the entire non-communist world, helping other nations build the strength to meet their own problems, to satisfy their own aspirations, to surmount their own dangers. The problems in achieving this goal are towering and unprecedented. The response must be towering and unprecedented as well. Much as Lend Lease and the Marshall Plan were in earlier years, which brought such fruitful results. I intend to ask the Congress for authority to establish a new and more effective program for assisting the economic, educational, and social development of other countries and continents. That program must stimulate and take more effectively into account the contributions of our allies and provide central policy direction for all our own programs that now so often overlap, conflict, or diffuse our energies and resources. Such a program compared to past programs will require more flexibility for short run emergencies, more commitment to long-term development, new attention to education at all levels, greater emphasis on the recipient nation’s role, their effort, their purpose, with greater social justice for their people, a broader distribution and participation by their people, and more efficient public administration and more efficient tax systems of their own. And orderly planning for national and regional development instead of a piecemeal approach. I hope the Senate will take early action approving the convention establishing the organization for economic cooperation and development. This will be an important instrument in sharing with our allies this development effort, working towards the time when each nation will contribute in proportion to its ability to pay. For while we are prepared to assume our full share of these huge burdens, we cannot and must not be expected to bear them alone. To our sister Republics of the South, we have pledged a new alliance for progress, Alianza para Progreso. Our goal is a free and prosperous Latin America, realizing for all its states and all its citizens a degree of economic and social progress that matches their historic contributions of culture, intellect, and liberty. To start this nation’s role at this time in that alliance of neighbors, I am recommending the following. That the Congress appropriate in full the $500 million fund pledged by the Act of Bogota to be used not as an instrument of the Cold War, but as a first step in the sound development of the Americas. That a new interdepartmental task force be established under the leadership of the Department of State to coordinate at the highest level all policies and programs of concern to the Americas. That our delegates to the OAS, working with those of other members, strengthen that body as an instrument to preserve the peace and to prevent foreign domination anywhere in the hemisphere. That in cooperation with other nations, we launch a new hemispheric attack on illiteracy and inadequate educational opportunities at all levels. And finally, that a food for peace mission be sent immediately to Latin America to explore ways in which our vast food abundance can be used to help hunger and malnutrition in certain areas of suffering in our own hemisphere. This administration is expanding its Food for Peace program in every possible way. The product of our abundance must be used more effectively to relieve hunger and help economic growth in all corners of the globe. And I have asked the director of this program to recommend additional ways in which these surpluses can advance the interests of world peace, including the establishment of world food reserves. An even more valuable national asset is our reservoir of dedicated men and women, not only at our college campuses, but in every age group, who have indicated their desire to contribute their skills, their efforts, and a part of their lives to the fight for world order. We can mobilize this talent through the formation of a national Peace Corps, enlisting the services of all those with desire and capacity to help foreign lands meet their urgent needs for trained personnel. Finally, while our attention is centered on the development of the non-communist world, we must never forget our hopes for the ultimate freedom and welfare of the Eastern European people. In order to be prepared to help reestablish historic ties of friendship, I am asking the Congress for increased discretion to use economic tools in this area, whenever this is found to be clearly in the national interest. This will require amendment of the Mutual Defense Assistance Control Act, along the lines I proposed as a member of the Senate, and upon which the Senate voted last summer. Meanwhile, I hope to explore with the Polish government the possibility of using our frozen Polish funds on projects of peace that will demonstrate our abiding friendship and interest in the people of Poland. Third, we must sharpen our political and diplomatic tools. The means of cooperation and agreement on which an enforceable world order must ultimately rest. I have already taken steps to coordinate and expand our disarmament effort, to increase our programs of research and study, and to make arms control a central goal of our national policy under my direction. The deadly arms race and the huge resources it absorbs have too long overshadowed all else we must do. We must prevent that arms racing from spreading to new nations, to new nuclear powers, powers and to the outer reaches of space. We must make certain that our negotiators are better informed and better prepared to formulate workable proposals of our own and to make sound judgment about the proposals of others. I have asked the other governments concerned to agree to a reasonable delay in the talks on a nuclear test ban, and it is our intention to resume negotiations prepared to reach a final agreement with any nation that is equally willing to agree to an effective and enforceable treaty. We must increase our support of the United Nations as an instrument to end the Cold War instead of an arena in which to fight it. In recognition of its increasing importance and the doubling of its membership, we are enlarging and strengthening our own mission to the UN. We shall help ensure that it is properly financed. We shall work to see that the integrity of the office of the Secretary General is maintained. And I would address a special plea to the smaller nations of the world to join with us in strengthening this organization which is far more essential to their security than it is to ours. The only body in the world where no nation need be powerful to be secure, where every nation has an equal voice, and where any nation can exert influence not according to the strength of its armies, but according to the strength of its ideas. It deserves the support of all. Finally, this administration intends to explore promptly all possible areas of cooperation with the Soviet Union and other nations to invoke the wonders of science instead of its terrors. Specifically, I now invite all nations, including the Soviet Union, to join with us in developing a weather prediction program, in a new communications satellite program, and in preparation for probing the distant planets of Mars and Venus, probes which may someday unlock the deepest secrets of the universe. Today this country is ahead in the science and technology of space, while the Soviet Union is ahead in the capacity while the Soviet Union is ahead in the capacity to lift large vehicles into orbit. Both nations would help themselves as well as other nations by removing these endeavors from the bitter and wasteful competition of the Cold War. The United States would be willing to join with the Soviet Union and the scientists of all nations in a greater effort to make the fruits of this new knowledge available to all. And beyond that, in an effort to extend farm technology to hungry nations, to wipe out disease, to increase the exchanges of scientists and knowledge, and to make our own laboratories available to technicians of other lands who lack the facilities to pursue their own work. Where nature makes natural allies of us all. We can demonstrate that beneficial relations are possible even with those with whom we most deeply disagree. And this must someday be the basis of world peace and world law. I have commented on the state of the domestic economy, our balance of payments, our federal and social budget, and the state of the world. I would like to conclude with a few remarks about the state of the Executive Branch. We have found it full of honest and useful public servants. But their capacity to act decisively at the exact time action is needed has too often been muffled in the morass of committees, timidities, and fictitious theories, which have created a growing gap between decision and execution, between planning and reality. In a time of rapidly deteriorating situations at home and abroad, this is bad for the public service and particularly bad for the country. And we mean to make a change. I have pledged myself and my colleagues in the Cabinet to a continuous encouragement of initiative, responsibility, and energy in serving the public interest. Let every public servant know whether his post is high or low, that a man’s rank and reputation in this administration will be determined by the size of the job he does and not by the size of his staff, his office, or his budget. Let it be clear that this administration recognizes the value of dissent and daring, that we greet healthy controversy as the hallmark of healthy change. Let the public service be a proud and lively career. And let every man and woman who works in any area of our national government, in any branch, at any level, be able to say with pride and with honor in future years, I served the United States Government in that hour of our nation’s need. For only with complete dedication by us all to the national interest, can we bring our country through the troubled years that lie ahead. Our problems are critical, the tide is unfavorable, the news will be worse before it is better. And while hoping and working for the best, we should prepare ourselves now for the worst. We cannot escape our dangers. Neither must we let them drive us into panic or narrow isolation. In many areas of the world, where the balance of power already rests with our adversaries, the forces of freedom are sharply divided. It is one of the ironies of our time that the techniques of a harsh and repressive system should be able to instill discipline and order in its servants, while the blessings of liberty have too often stood for privilege, materialism, and a life of ease. But I have a different view of liberty. Life in 1961 will not be easy. Wishing it, predicting it, even asking for it will not make it so. There will be further setbacks before the tide is turned, but turn it we must. The hopes of all mankind rest upon us. Not simply upon those of us in this chamber, but upon the peasant in Laos, the fisherman in Nigeria, the exile from Cuba, the spirit that moves every man and nation who shares our hopes for freedom and the future. And in the final analysis, they rest most of all upon the pride and perseverance of our fellow citizens of the Great Republic. In the words of a great president whose birthday we honor today, closing his final State of the Union message, 16 years ago, we pray that we may be worthy of the unlimited opportunities that God has given us.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Audio Quickstart"
    ]
  },
  {
    "objectID": "quickstarts/Audio.html#use-a-youtube-video",
    "href": "quickstarts/Audio.html#use-a-youtube-video",
    "title": "Gemini API: Audio Quickstart",
    "section": "Use a Youtube video",
    "text": "Use a Youtube video\n\nconst YOUTUBE_URL = \"https://www.youtube.com/watch?v=RDOMKIw1aF4\";\n\nconst youtube_response = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: [\n    `\n    Analyze the following YouTube video content. Provide a concise summary covering:\n\n    1.  **Main Thesis/Claim:** What is the central point the creator is making?\n    2.  **Key Topics:** List the main subjects discussed, referencing specific examples or technologies mentioned (e.g., AI models, programming languages, projects).\n    3.  **Call to Action:** Identify any explicit requests made to the viewer.\n    4.  **Summary:** Provide a concise summary of the video content.\n\n    Use the provided title, chapter timestamps/descriptions, and description text for your analysis.\n    `,\n    google.createPartFromUri(YOUTUBE_URL, \"video/x-youtube\"),\n  ],\n});\ntslab.display.markdown(youtube_response.text ?? \"\");\n\nHere’s an analysis of the YouTube video content:\n\nMain Thesis/Claim: Google’s newly released Gemini 2.5 Pro is a highly capable AI for coding, demonstrating impressive abilities in code generation and refactoring across various programming languages, often outperforming or competing strongly with other leading AI models. While it excels at functional code, it still faces challenges in accurately translating complex visual mockups into fully functional and aesthetically precise UIs.\nKey Topics:\n\nGemini 2.5 Pro Capabilities: Highlighted for its large 1M token context window and overall performance as a coding AI.\nCode Generation:\n\nSuccessfully generated an “Ultimate Tic-Tac-Toe” game in Java (Swing) in a single attempt (“one-shot”).\nCreated a “Kitten Cannon” style game using P5.js, requiring minor error corrections (fixed with two additional prompts, making it a “three-shot” overall).\nRecreated a static HTML/CSS layout of the X (Twitter) website, demonstrating visual accuracy but no dynamic functionality.\n\nCode Refactoring: Showcased a significant improvement in refactoring complex Rust code, converting traditional loops to more idiomatic iterators and generally cleaning up the codebase, surpassing other models in this task.\nUI Generation Limitations: Demonstrated struggles with building a precise landing page using Vite, React, and Tailwind CSS from a visual mockup, indicating difficulties with complex visual translation and asset integration.\nBenchmarking and Performance: Compared Gemini 2.5 Pro’s performance against other prominent AI models (OpenAI’s GPT-4/o3-mini, Claude 3, DeepSeek, Grok) across reasoning, science, mathematics, code generation, and long context. Gemini showed strong leads in reasoning, science, and mathematics, and competitive performance in code generation and editing.\nInformation Currentness (Grounding): Noted Gemini’s training data extends to March 2025 and its ability to “ground” responses with Google Search for real-time information (e.g., current React.js version).\n\nCall to Action:\n\nEncourages viewers to share their experiences with Gemini 2.5 Pro in the comments.\nAsks Rust developers for their opinion on the refactored Rust code.\nRequests viewers to subscribe, like the video, and enable notifications.\n\nSummary: The video showcases the impressive coding prowess of Google’s new Gemini 2.5 Pro AI model, emphasizing its large context window and strong capabilities in generating and refactoring code. The creator successfully uses Gemini to build a Java Tic-Tac-Toe game in one attempt, a P5.js game with minor fixes, and a static Twitter UI. A key highlight is Gemini’s superior ability to refactor complex Rust code into cleaner, more idiomatic forms. While the model excels in functional coding, it encounters limitations when tasked with generating complex, pixel-perfect user interfaces directly from visual mockups. The creator also presents benchmark comparisons, demonstrating Gemini’s competitive edge over other leading AI models in various domains, including its up-to-date knowledge base. The video concludes by recommending Gemini 2.5 Pro as a powerful and potentially cost-effective tool for developers.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Audio Quickstart"
    ]
  },
  {
    "objectID": "quickstarts/Audio.html#count-audio-tokens",
    "href": "quickstarts/Audio.html#count-audio-tokens",
    "title": "Gemini API: Audio Quickstart",
    "section": "Count audio tokens",
    "text": "Count audio tokens\nYou can count the number of tokens in your audio file using the countTokens method.\nAudio files have a fixed per second token rate (more details in the dedicated count token quickstart.)\n\nconst count_tokens_response = await ai.models.countTokens({\n  model: MODEL_ID,\n  contents: [google.createPartFromUri(audio_file.uri ?? \"\", audio_file.mimeType ?? \"\")],\n});\ntslab.display.markdown(`Token count for the audio file: \\`${count_tokens_response.totalTokens}\\``);\n\nToken count for the audio file: 83528",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Audio Quickstart"
    ]
  },
  {
    "objectID": "quickstarts/Audio.html#next-steps",
    "href": "quickstarts/Audio.html#next-steps",
    "title": "Gemini API: Audio Quickstart",
    "section": "Next Steps",
    "text": "Next Steps\n\nUseful API references:\nMore details about Gemini API’s vision capabilities in the documentation.\nIf you want to know about the File API, check its API reference or the File API quickstart.\n\n\nRelated examples\nCheck this example using the audio files to give you more ideas on what the gemini API can do with them:\n\nShare Voice memos with Gemini API and brainstorm ideas.\n\n\n\nContinue your discovery of the Gemini API\nHave a look at the Audio quickstart to learn about another type of media file, then learn more about prompting with media files in the docs, including the supported formats and maximum length for audio files.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: Audio Quickstart"
    ]
  },
  {
    "objectID": "quickstarts/Counting_Tokens.html",
    "href": "quickstarts/Counting_Tokens.html",
    "title": "Gemini API: All about tokens",
    "section": "",
    "text": "An understanding of tokens is central to using the Gemini API. This guide will provide a interactive introduction to what tokens are and how they are used in the Gemini API.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: All about tokens"
    ]
  },
  {
    "objectID": "quickstarts/Counting_Tokens.html#about-tokens",
    "href": "quickstarts/Counting_Tokens.html#about-tokens",
    "title": "Gemini API: All about tokens",
    "section": "About tokens",
    "text": "About tokens\nLLMs break up their input and produce their output at a granularity that is smaller than a word, but larger than a single character or code-point.\nThese tokens can be single characters, like z, or whole words, like the. Long words may be broken up into several tokens. The set of all tokens used by the model is called the vocabulary, and the process of breaking down text into tokens is called tokenization.\nFor Gemini models, a token is equivalent to about 4 characters. 100 tokens are about 60-80 English words.\nWhen billing is enabled, the price of a paid request is controlled by the number of input and output tokens, so knowing how to count your tokens is important.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: All about tokens"
    ]
  },
  {
    "objectID": "quickstarts/Counting_Tokens.html#setup",
    "href": "quickstarts/Counting_Tokens.html#setup",
    "title": "Gemini API: All about tokens",
    "section": "Setup",
    "text": "Setup\n\nInstall the Google GenAI SDK\nInstall the Google GenAI SDK from npm.\n$ npm install @google/genai\n\n\nSetup your API key\nYou can create your API key using Google AI Studio with a single click.\nRemember to treat your API key like a password. Don’t accidentally save it in a notebook or source file you later commit to GitHub. In this notebook we will be storing the API key in a .env file. You can also set it as an environment variable or use a secret manager.\nHere’s how to set it up in a .env file:\n$ touch .env\n$ echo \"GEMINI_API_KEY=&lt;YOUR_API_KEY&gt;\" &gt;&gt; .env\n\n\n\n\n\n\nTip\n\n\n\nAnother option is to set the API key as an environment variable. You can do this in your terminal with the following command:\n$ export GEMINI_API_KEY=\"&lt;YOUR_API_KEY&gt;\"\n\n\n\n\nLoad the API key\nTo load the API key from the .env file, we will use the dotenv package. This package loads environment variables from a .env file into process.env.\n$ npm install dotenv\nThen, we can load the API key in our code:\n\nconst dotenv = require(\"dotenv\") as typeof import(\"dotenv\");\n\ndotenv.config({\n  path: \"../.env\",\n});\n\nconst GEMINI_API_KEY = process.env.GEMINI_API_KEY ?? \"\";\nif (!GEMINI_API_KEY) {\n  throw new Error(\"GEMINI_API_KEY is not set in the environment variables\");\n}\nconsole.log(\"GEMINI_API_KEY is set in the environment variables\");\n\nGEMINI_API_KEY is set in the environment variables\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn our particular case the .env is is one directory up from the notebook, hence we need to use ../ to go up one directory. If the .env file is in the same directory as the notebook, you can omit it altogether.\n│\n├── .env\n└── quickstarts\n    └── Counting_Tokens.ipynb\n\n\n\n\nInitialize SDK Client\nWith the new SDK, now you only need to initialize a client with you API key (or OAuth if using Vertex AI). The model is now set in each call.\n\nconst google = require(\"@google/genai\") as typeof import(\"@google/genai\");\n\nconst ai = new google.GoogleGenAI({ apiKey: GEMINI_API_KEY });",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: All about tokens"
    ]
  },
  {
    "objectID": "quickstarts/Counting_Tokens.html#tokens-in-the-gemini-api",
    "href": "quickstarts/Counting_Tokens.html#tokens-in-the-gemini-api",
    "title": "Gemini API: All about tokens",
    "section": "Tokens in the Gemini API",
    "text": "Tokens in the Gemini API\n\nContext windows\nThe models available through the Gemini API have context windows that are measured in tokens. These define how much input you can provide, and how much output the model can generate, and combined are referred to as the “context window”. This information is available directly through the API and in the models documentation.\nIn this example you can see the gemini-2.5-flash-preview-05-20 model has an 1M tokens context window. If you need more, Pro models have an even bigger 2M tokens context window.\n\nconst tslab = require(\"tslab\") as typeof import(\"tslab\");\n\nconst model_info_2_5_flash_preview_05_20 = await ai.models.get({\n  model: \"gemini-2.5-flash-preview-05-20\",\n});\nconsole.log(JSON.stringify(model_info_2_5_flash_preview_05_20, null, 2));\n\n{\n  \"name\": \"models/gemini-2.5-flash-preview-05-20\",\n  \"displayName\": \"Gemini 2.5 Flash Preview 05-20\",\n  \"description\": \"Preview release (April 17th, 2025) of Gemini 2.5 Flash\",\n  \"version\": \"2.5-preview-05-20\",\n  \"tunedModelInfo\": {},\n  \"inputTokenLimit\": 1048576,\n  \"outputTokenLimit\": 65536,\n  \"supportedActions\": [\n    \"generateContent\",\n    \"countTokens\",\n    \"createCachedContent\",\n    \"batchGenerateContent\"\n  ]\n}\n\n\n\n\nCounting tokens\nThe API provides an endpoint for counting the number of tokens in a request: client.models.countTokens. You pass the same arguments as you would to client.models.generateContent and the service will return the number of tokens in that request.\n\n\nChoose a model\nNow select the model you want to use in this guide, either by selecting one in the list or writing it down. Keep in mind that some models, like the 2.5 ones are thinking models and thus take slightly more time to respond (cf. thinking notebook for more details and in particular learn how to switch the thiking off).\nThe tokenization should be more or less the same for each of the Gemini models, but you can still switch between the different ones to double-check.\nFor more information about all Gemini models, check the documentation for extended information on each of them.\n\nconst MODEL_ID = \"gemini-2.5-flash-preview-05-20\";\n\n\n\nText tokens\n\nconst text_response = await ai.models.countTokens({\n  model: MODEL_ID,\n  contents: \"What's the highest mountain in Africa?\",\n});\nconsole.log(`Prompt Tokens: ${text_response.totalTokens}`);\n\nPrompt Tokens: 10\n\n\nWhen you call client.models.generateContent (or chat.sendMessage) the response object has a usageMetadata attribute containing both the input and output token counts (promptTokenCount and candidatesTokenCount):\n\nconst text_response_2 = await ai.models.generateContent({\n  model: MODEL_ID,\n  contents: \"The quick brown fox jumps over the lazy dog.\",\n});\ntslab.display.markdown(text_response_2.text ?? \"\");\n\nYes, that’s a classic and very well-known pangram!\nIt’s famous because it contains every letter of the English alphabet, which makes it useful for testing typewriters, keyboards, and font displays.\n\n\n\nconsole.log(`Prompt Tokens: ${text_response_2.usageMetadata?.promptTokenCount}`);\nconsole.log(`Output Tokens: ${text_response_2.usageMetadata?.candidatesTokenCount}`);\nconsole.log(`Total Tokens: ${text_response_2.usageMetadata?.totalTokenCount}`);\n\nPrompt Tokens: 11\nOutput Tokens: 47\nTotal Tokens: 881\n\n\n\n\nMulti-modal tokens\nAll input to the API is tokenized, including images or other non-text modalities.\nImages are considered to be a fixed size, so they consume a fixed number of tokens, regardless of their display or file size.\nVideo and audio files are converted to tokens at a fixed per second rate.\nThe current rates and token sizes can be found on the documentation.\n\nconst fs = require(\"fs\") as typeof import(\"fs\");\nconst path = require(\"path\") as typeof import(\"path\");\n\nconst IMG_URL = \"https://goo.gle/instrument-img\";\n\nconst downloadFile = async (url: string, filePath: string) =&gt; {\n  const response = await fetch(url);\n  if (!response.ok) {\n    throw new Error(`Failed to download image: ${response.statusText}`);\n  }\n  const buffer = await response.blob();\n  const bufferData = Buffer.from(await buffer.arrayBuffer());\n  fs.writeFileSync(filePath, bufferData);\n};\n\nconst filePath = path.join(\"../assets\", \"organ.jpg\");\nawait downloadFile(IMG_URL, filePath);\n\n\ntslab.display.jpeg(fs.readFileSync(filePath));\n\n\n\n\n\n\n\n\n\n\nInline content\nMedia objects can be sent to the API inline with the request:\n\nconst inline_response = await ai.models.countTokens({\n  model: MODEL_ID,\n  contents: [google.createPartFromBase64(fs.readFileSync(filePath).toString(\"base64\"), \"image/jpeg\")],\n});\nconsole.log(`Image Tokens: ${inline_response.totalTokens}`);\n\nImage Tokens: 259\n\n\nYou can try with different images and should always get the same number of tokens, that is independent of their display or file size. Note that an extra token seems to be added, representing the empty prompt.\n\n\nFiles API\nThe model sees identical tokens if you upload parts of the prompt through the files API instead:\n\nconst file_upload_response = await ai.files.upload({\n  file: filePath,\n  config: {\n    displayName: \"organ.jpg\",\n    mimeType: \"image/jpeg\",\n  },\n});\nconst file_response = await ai.models.countTokens({\n  model: MODEL_ID,\n  contents: [google.createPartFromUri(file_upload_response.uri ?? \"\", file_upload_response.mimeType ?? \"\")],\n});\nconsole.log(`File Tokens: ${file_response.totalTokens}`);\n\nFile Tokens: 259\n\n\nAudio and video are each converted to tokens at a fixed rate of tokens per minute.\n\nconst AUDIO_URL =\n  \"https://storage.googleapis.com/generativeai-downloads/data/State_of_the_Union_Address_30_January_1961.mp3\";\nconst audioPath = path.join(\"../assets\", \"audio.mp3\");\n\nawait downloadFile(AUDIO_URL, audioPath);\n\n\nimport { ffprobe, FfprobeData } from \"fluent-ffmpeg\";\n\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\nffprobe(audioPath, (err: any, metadata: FfprobeData) =&gt; {\n  if (err) {\n    console.error(\"Error getting audio metadata:\", err);\n    return;\n  }\n  const { duration } = metadata.format;\n  console.log(`Audio Duration: ${duration} seconds`);\n});\n\nAudio Duration: 2610.128938 seconds\n\n\nAs you can see, this audio file is 2610s long.\n\nconst audio_file_response = await ai.files.upload({\n  file: audioPath,\n  config: {\n    displayName: \"audio.mp3\",\n    mimeType: \"audio/mpeg\",\n  },\n});\n\nconst audio_response = await ai.models.countTokens({\n  model: MODEL_ID,\n  contents: [google.createPartFromUri(audio_file_response.uri ?? \"\", audio_file_response.mimeType ?? \"\")],\n});\n\nconsole.log(`Audio Tokens: ${audio_response.totalTokens}`);\nconsole.log(`Tokens per second: ${audio_response.totalTokens / 2610}`);\n\nAudio Tokens: 83528\nTokens per second: 32.003065134099614\n\n\n\n\nChat, tools and cache\nChat, tools and cache are currently not supported by the unified SDK countTokens method. This notebook will be updated when that will be the case.\nIn the meantime you can still check the token used after the call using the usageMetadata from the response. Check the caching notebook for more details.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: All about tokens"
    ]
  },
  {
    "objectID": "quickstarts/Counting_Tokens.html#further-reading",
    "href": "quickstarts/Counting_Tokens.html#further-reading",
    "title": "Gemini API: All about tokens",
    "section": "Further reading",
    "text": "Further reading\nFor more on token counting, check out the documentation or the API reference:\n\ncountTokens REST API reference\nmodels.countTokens JS SDK reference",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Gemini API: All about tokens"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_LiveAPI.html",
    "href": "quickstarts/Get_started_LiveAPI.html",
    "title": "Multimodal Live API - Quickstart",
    "section": "",
    "text": "Preview: The Live API is in preview.\nThis notebook demonstrates simple usage of the Gemini Multimodal Live API. For an overview of new capabilities refer to the Gemini Live API docs.\nThis notebook implements a simple turn-based chat where you send messages as text, and the model replies with audio. The API is capable of much more than that. The goal here is to demonstrate with simple code.\nIf you aren’t looking for code, and just want to try multimedia streaming use Live API in Google AI Studio.\nThe Next steps section at the end of this tutorial provides links to additional resources.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Multimodal Live API - Quickstart"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_LiveAPI.html#setup",
    "href": "quickstarts/Get_started_LiveAPI.html#setup",
    "title": "Multimodal Live API - Quickstart",
    "section": "Setup",
    "text": "Setup\n\nInstall the Google GenAI SDK\nInstall the Google GenAI SDK from npm.\n$ npm install @google/genai\n\n\nSetup your API key\nYou can create your API key using Google AI Studio with a single click.\nRemember to treat your API key like a password. Don’t accidentally save it in a notebook or source file you later commit to GitHub. In this notebook we will be storing the API key in a .env file. You can also set it as an environment variable or use a secret manager.\nHere’s how to set it up in a .env file:\n$ touch .env\n$ echo \"GEMINI_API_KEY=&lt;YOUR_API_KEY&gt;\" &gt;&gt; .env\n\n\n\n\n\n\nTip\n\n\n\nAnother option is to set the API key as an environment variable. You can do this in your terminal with the following command:\n$ export GEMINI_API_KEY=\"&lt;YOUR_API_KEY&gt;\"\n\n\n\n\nLoad the API key\nTo load the API key from the .env file, we will use the dotenv package. This package loads environment variables from a .env file into process.env.\n$ npm install dotenv\nThen, we can load the API key in our code:\n\nconst dotenv = require(\"dotenv\") as typeof import(\"dotenv\");\n\ndotenv.config({\n  path: \"../.env\",\n});\n\nconst GEMINI_API_KEY = process.env.GEMINI_API_KEY ?? \"\";\nif (!GEMINI_API_KEY) {\n  throw new Error(\"GEMINI_API_KEY is not set in the environment variables\");\n}\nconsole.log(\"GEMINI_API_KEY is set in the environment variables\");\n\nGEMINI_API_KEY is set in the environment variables\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn our particular case the .env is is one directory up from the notebook, hence we need to use ../ to go up one directory. If the .env file is in the same directory as the notebook, you can omit it altogether.\n│\n├── .env\n└── quickstarts\n    └── Get_started_LiveAPI.ipynb",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Multimodal Live API - Quickstart"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_LiveAPI.html#initialize-sdk-client",
    "href": "quickstarts/Get_started_LiveAPI.html#initialize-sdk-client",
    "title": "Multimodal Live API - Quickstart",
    "section": "Initialize SDK Client",
    "text": "Initialize SDK Client\nWith the new SDK, now you only need to initialize a client with you API key (or OAuth if using Vertex AI). The model is now set in each call.\n\nconst google = require(\"@google/genai\") as typeof import(\"@google/genai\");\n\nconst ai = new google.GoogleGenAI({ apiKey: GEMINI_API_KEY });",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Multimodal Live API - Quickstart"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_LiveAPI.html#text-to-text",
    "href": "quickstarts/Get_started_LiveAPI.html#text-to-text",
    "title": "Multimodal Live API - Quickstart",
    "section": "Text to Text",
    "text": "Text to Text\nThe simplest way to use the Live API is as a text-to-text chat interface, but it can do a lot more than this.\n\nconst tslab = require(\"tslab\") as typeof import(\"tslab\");\n\nconst MODEL_ID = \"gemini-2.0-flash-live-001\";\n\nThe Live API uses a streaming model over a WebSocket connection. When you interact with the API, a persistent connection is created. Your input (audio, video, or text) is streamed continuously to the model, and the model’s response (text or audio) is streamed back in real-time over the same connection. Here we use a responseQueue to handle the streaming responses and determine when the server has finished sending the response.\n\nimport { LiveServerMessage, Modality } from \"@google/genai\";\n\nasync function text_to_text() {\n  const responseQueue: LiveServerMessage[] = [];\n  const session = await ai.live.connect({\n    model: MODEL_ID,\n    callbacks: {\n      onopen: function () {\n        console.debug(\"Opened\");\n      },\n      onmessage: function (message) {\n        responseQueue.push(message);\n      },\n      onerror: function (e) {\n        console.debug(\"Error:\", e.message);\n      },\n      onclose: function (e) {\n        console.debug(\"Close:\", e.reason);\n      },\n    },\n    config: { responseModalities: [Modality.TEXT] },\n  });\n  const message = \"Hello? Gemini are you there?\";\n  session.sendClientContent({\n    turns: message,\n    turnComplete: true,\n  });\n  console.debug(\"Sent message:\", message);\n  let done = false;\n  while (!done) {\n    if (responseQueue.length &gt; 0) {\n      const response = responseQueue.shift();\n      if (response?.text) {\n        console.debug(\"Received response:\", response.text);\n      } else if (response?.data) {\n        console.debug(\"Received data:\", response.data);\n      }\n      if (response?.serverContent?.turnComplete) {\n        done = true;\n      }\n    } else {\n      await new Promise((resolve) =&gt; setTimeout(resolve, 100));\n    }\n  }\n  session.close();\n  console.debug(\"Session closed\");\n}\n\nawait text_to_text();\n\nOpened\nSent message: Hello? Gemini are you there?\nReceived response: Yes, I am\nReceived response:  here! How can I help you today?\n\nSession closed\nClose:",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Multimodal Live API - Quickstart"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_LiveAPI.html#text-to-audio",
    "href": "quickstarts/Get_started_LiveAPI.html#text-to-audio",
    "title": "Multimodal Live API - Quickstart",
    "section": "Text to audio",
    "text": "Text to audio\nThe simplest way to playback the audio in Colab, is to write it out to a .wav file. So here is a simple wave file writer:\n\nconst fs = require(\"fs\") as typeof import(\"fs\");\nconst path = require(\"path\") as typeof import(\"path\");\nconst wave = require(\"wavefile\") as typeof import(\"wavefile\");\n\nfunction saveAudioToFile(audioData: Int16Array, filePath: string) {\n  fs.mkdirSync(path.dirname(filePath), { recursive: true });\n  const wav = new wave.WaveFile();\n  wav.fromScratch(1, 24000, \"16\", audioData);\n  fs.writeFileSync(filePath, wav.toBuffer());\n  console.debug(`Audio saved to ${filePath}`);\n}\n\n\nimport { LiveServerMessage, Modality } from \"@google/genai\";\n\nasync function text_to_audio() {\n  const responseQueue: LiveServerMessage[] = [];\n  const session = await ai.live.connect({\n    model: MODEL_ID,\n    callbacks: {\n      onopen: function () {\n        console.debug(\"Opened\");\n      },\n      onmessage: function (message) {\n        responseQueue.push(message);\n      },\n      onerror: function (e) {\n        console.debug(\"Error:\", e.message);\n      },\n      onclose: function (e) {\n        console.debug(\"Close:\", e.reason);\n      },\n    },\n    config: { responseModalities: [Modality.AUDIO] },\n  });\n  const message = \"Hello? Gemini are you there?\";\n  session.sendClientContent({\n    turns: message,\n    turnComplete: true,\n  });\n  console.debug(\"Sent message:\", message);\n  let done = false;\n  const chunks: LiveServerMessage[] = [];\n  while (!done) {\n    if (responseQueue.length &gt; 0) {\n      const response = responseQueue.shift();\n      if (response) {\n        chunks.push(response);\n      }\n      if (response?.serverContent?.turnComplete) {\n        done = true;\n        console.debug(\"Received complete response\");\n      }\n    } else {\n      await new Promise((resolve) =&gt; setTimeout(resolve, 100));\n    }\n  }\n  const audioData = chunks.reduce&lt;number[]&gt;((acc, message) =&gt; {\n    if (message.data) {\n      const audioBuffer = Buffer.from(message.data, \"base64\");\n      const intArray = new Int16Array(\n        audioBuffer.buffer,\n        audioBuffer.byteOffset,\n        audioBuffer.length / Int16Array.BYTES_PER_ELEMENT\n      );\n      return acc.concat(Array.from(intArray));\n    }\n    return acc;\n  }, []);\n  const audioFilePath = path.join(\"../assets/live\", \"text_to_audio_response.wav\");\n  saveAudioToFile(new Int16Array(audioData), audioFilePath);\n  session.close();\n  console.debug(\"Session closed\");\n}\n\nawait text_to_audio();\ntslab.display.html(`\n  &lt;h3&gt;Text to Audio Response&lt;/h3&gt;\n  &lt;audio controls&gt;\n    &lt;source src=\"../assets/live/text_to_audio_response.wav\" type=\"audio/wav\"&gt;\n    Your browser does not support the audio element.\n  &lt;/audio&gt;\n  &lt;/audio&gt;\n`);\n\nOpened\nSent message: Hello? Gemini are you there?\nReceived complete response\nAudio saved to ../assets/live/text_to_audio_response.wav\nSession closed\n\n\n\nText to Audio Response\n\n  \n  Your browser does not support the audio element.\n\n\n\n\nClose:",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Multimodal Live API - Quickstart"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_LiveAPI.html#towards-async-tasks",
    "href": "quickstarts/Get_started_LiveAPI.html#towards-async-tasks",
    "title": "Multimodal Live API - Quickstart",
    "section": "Towards Async Tasks",
    "text": "Towards Async Tasks\nThe real power of the Live API is that it’s real time, and interruptable. You can’t get that full power in a simple sequence of steps. To really use the functionality you will move the send and recieve operations (and others) into their own async tasks.\nBecause of the limitations of Colab this tutorial doesn’t totally implement the interactive async tasks, but it does implement the next step in that direction:\n\nIt separates the send and receive, but still runs them sequentially.\nIn the next tutorial you’ll run these in separate async tasks.\n\n\nimport { GoogleGenAI, LiveServerMessage, Modality, Session } from \"@google/genai\";\n\nclass AudioLooper {\n  private session: Session;\n  private turnIndex = 0;\n  private responseQueue: LiveServerMessage[] = [];\n\n  constructor(\n    private ai: GoogleGenAI,\n    private modelId: string\n  ) {}\n\n  async start() {\n    this.session = await this.ai.live.connect({\n      model: this.modelId,\n      callbacks: {\n        onopen: () =&gt; {\n          console.debug(\"Opened\");\n        },\n        onmessage: (message) =&gt; this.responseQueue.push(message),\n        onerror: (e) =&gt; {\n          console.debug(\"Error:\", e.message);\n        },\n        onclose: (e) =&gt; {\n          console.debug(\"Close:\", e.reason);\n        },\n      },\n      config: { responseModalities: [Modality.AUDIO] },\n    });\n  }\n\n  send(message: string) {\n    this.session.sendClientContent({\n      turns: message,\n      turnComplete: true,\n    });\n    console.debug(\"Sent message:\", message);\n  }\n\n  async receive() {\n    let done = false;\n    const audioChunks: number[] = [];\n    while (!done) {\n      if (this.responseQueue.length &gt; 0) {\n        const response = this.responseQueue.shift();\n        if (response?.data) {\n          const audioBuffer = Buffer.from(response.data, \"base64\");\n          const intArray = new Int16Array(\n            audioBuffer.buffer,\n            audioBuffer.byteOffset,\n            audioBuffer.length / Int16Array.BYTES_PER_ELEMENT\n          );\n          audioChunks.push(...Array.from(intArray));\n        }\n        if (response?.serverContent?.turnComplete) {\n          done = true;\n          console.debug(\"Received complete response\");\n        }\n      } else {\n        await new Promise((resolve) =&gt; setTimeout(resolve, 100));\n      }\n    }\n    const audioFilePath = path.join(\"../assets/live\", `audio_response_${this.turnIndex++}.wav`);\n    saveAudioToFile(new Int16Array(audioChunks), audioFilePath);\n    tslab.display.html(`\n      &lt;h3&gt;Audio Response ${this.turnIndex}&lt;/h3&gt;\n      &lt;audio controls&gt;\n          &lt;source src=\"../assets/live/audio_response_${this.turnIndex - 1}.wav\" type=\"audio/wav\"&gt;\n          Your browser does not support the audio element.\n      &lt;/audio&gt;\n    `);\n  }\n\n  stop() {\n    this.session.close();\n    console.debug(\"Session closed\");\n  }\n}\n\nasync function asyncAudioLooper() {\n  const audioLooper = new AudioLooper(ai, MODEL_ID);\n  await audioLooper.start();\n\n  // Simulate sending messages\n  const messages = [\"Hello? Gemini are you there?\", \"Can you tell me a joke?\", \"What is the weather like today?\"];\n\n  for (const message of messages) {\n    audioLooper.send(message);\n    await audioLooper.receive();\n  }\n\n  audioLooper.stop();\n}\n\nawait asyncAudioLooper();\n\nOpened\nSent message: Hello? Gemini are you there?\nReceived complete response\nAudio saved to ../assets/live/audio_response_0.wav\n\n\n\nAudio Response 1\n\n    \n    Your browser does not support the audio element.\n\n\n\nSent message: Can you tell me a joke?\nReceived complete response\nAudio saved to ../assets/live/audio_response_1.wav\n\n\n\nAudio Response 2\n\n    \n    Your browser does not support the audio element.\n\n\n\nSent message: What is the weather like today?\nReceived complete response\nAudio saved to ../assets/live/audio_response_2.wav\n\n\n\nAudio Response 3\n\n    \n    Your browser does not support the audio element.\n\n\n\nSession closed\nClose: \n\n\nThe above code is divided into several sections:\n\nstart: Initializes the client and sets up the WebSocket connection.\nsend: Sends a message to the model.\nreceive: Receives the model’s response and collects the audio chunks in a loop and writes them to wav file. It breaks when the model indicates it has finished sending the response.\nasyncAudioLooper: This is the main driver function that brings everything together. It initializes the client, starts the WebSocket connection, and then enters a loop where it sends messages and receives responses.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Multimodal Live API - Quickstart"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_LiveAPI.html#working-with-resumable-sessions",
    "href": "quickstarts/Get_started_LiveAPI.html#working-with-resumable-sessions",
    "title": "Multimodal Live API - Quickstart",
    "section": "Working with resumable sessions",
    "text": "Working with resumable sessions\nSession resumption allows you to return to a previous interaction with the Live API by sending the last session handle you got from the previous session.\nWhen you set your session to be resumable, the session information keeps stored on the Live API for up to 24 hours. In this time window, you can resume the conversation and refer to previous information you have shared with the model.\n\nimport { LiveServerMessage, Modality } from \"@google/genai\";\n\nlet HANDLE: string | undefined = undefined;\n\nasync function resumable_session(\n  previousSessionHandle?: string,\n  messages: string[] = [\"Hello\", \"What is the capital of Brazil?\"]\n) {\n  const responseQueue: LiveServerMessage[] = [];\n\n  async function waitMessage(): Promise&lt;LiveServerMessage&gt; {\n    let done = false;\n    let message: LiveServerMessage | undefined = undefined;\n    while (!done) {\n      message = responseQueue.shift();\n      if (message) {\n        done = true;\n      } else {\n        await new Promise((resolve) =&gt; setTimeout(resolve, 100));\n      }\n    }\n    return message!;\n  }\n\n  console.debug(\"Connecting to the service with handle %s...\", previousSessionHandle);\n  const session = await ai.live.connect({\n    model: MODEL_ID,\n    callbacks: {\n      onopen: function () {\n        console.debug(\"Opened\");\n      },\n      onmessage: function (message) {\n        responseQueue.push(message);\n        console.debug(\"Received message:\", JSON.stringify(message));\n        if (message.sessionResumptionUpdate?.resumable && message.sessionResumptionUpdate.newHandle) {\n          HANDLE = message.sessionResumptionUpdate.newHandle;\n        }\n      },\n      onerror: function (e) {\n        console.debug(\"Error:\", e.message);\n      },\n      onclose: function (e) {\n        console.debug(\"Close:\", e.reason);\n      },\n    },\n    config: {\n      responseModalities: [Modality.TEXT],\n      sessionResumption: { handle: previousSessionHandle },\n    },\n  });\n\n  for (const message of messages) {\n    console.debug(\"Sending message:\", message);\n    session.sendClientContent({\n      turns: message,\n      turnComplete: true,\n    });\n    let done = false;\n    while (!done) {\n      const response = await waitMessage();\n      if (response.serverContent?.turnComplete) {\n        done = true;\n      }\n    }\n  }\n\n  // small delay for session resumption update to arrive\n  await new Promise((resolve) =&gt; setTimeout(resolve, 3000));\n\n  session.close();\n}\n\nawait resumable_session();\n\nConnecting to the service with handle undefined...\nOpened\nSending message: Hello\nReceived message: {\"setupComplete\":{}}\nReceived message: {\"sessionResumptionUpdate\":{}}\nReceived message: {\"serverContent\":{\"modelTurn\":{\"parts\":[{\"text\":\"Hello there! How\"}]}}}\nReceived message: {\"serverContent\":{\"modelTurn\":{\"parts\":[{\"text\":\" can I help you today?\\n\"}]}}}\nReceived message: {\"serverContent\":{\"generationComplete\":true}}\nReceived message: {\"serverContent\":{\"turnComplete\":true},\"usageMetadata\":{\"promptTokenCount\":9,\"responseTokenCount\":11,\"totalTokenCount\":20,\"promptTokensDetails\":[{\"modality\":\"TEXT\",\"tokenCount\":9}],\"responseTokensDetails\":[{\"modality\":\"TEXT\",\"tokenCount\":11}]}}\nSending message: What is the capital of Brazil?\nReceived message: {\"sessionResumptionUpdate\":{\"newHandle\":\"CihqdTFxaG1ua2g2aTkweWtiNzB5Ymdzc3V0bW16eDE2ZGkxaXR2d2dt\",\"resumable\":true}}\nReceived message: {\"sessionResumptionUpdate\":{}}\nReceived message: {\"serverContent\":{\"modelTurn\":{\"parts\":[{\"text\":\"The capital of Brazil\"}]}}}\nReceived message: {\"serverContent\":{\"modelTurn\":{\"parts\":[{\"text\":\" is **Brasília**.\\n\"}]}}}\nReceived message: {\"serverContent\":{\"generationComplete\":true}}\nReceived message: {\"serverContent\":{\"turnComplete\":true},\"usageMetadata\":{\"promptTokenCount\":37,\"responseTokenCount\":10,\"totalTokenCount\":47,\"promptTokensDetails\":[{\"modality\":\"TEXT\",\"tokenCount\":37}],\"responseTokensDetails\":[{\"modality\":\"TEXT\",\"tokenCount\":10}]}}\nReceived message: {\"sessionResumptionUpdate\":{\"newHandle\":\"CihrNGZyMjh4dXY3cXFkYzVmMjR5cnlmZ2w5bnBvNTRhcmoxNW1lN2Fi\",\"resumable\":true}}\nClose: \n\n\nWith the session resumption you have the session handle to refer to your previous sessions. In this example, the handle is saved at the handle variable as below:\n\nconsole.debug(\"Session handle:\", HANDLE);\n\nSession handle: CihrNGZyMjh4dXY3cXFkYzVmMjR5cnlmZ2w5bnBvNTRhcmoxNW1lN2Fi\n\n\nNow you can start a new Live API session, but this time pointing to a handle from a previous session. Also, to test you could gather information from the previous session, you will ask the model what was the second question you asked before (in this example, it was “what is the capital of Brazil?”). You can see the Live API recovering that information:\n\nawait resumable_session(HANDLE, [\"what was the last question I asked?\"]);\n\nConnecting to the service with handle CihrNGZyMjh4dXY3cXFkYzVmMjR5cnlmZ2w5bnBvNTRhcmoxNW1lN2Fi...\nOpened\nSending message: what was the last question I asked?\nReceived message: {\"setupComplete\":{}}\nReceived message: {\"sessionResumptionUpdate\":{}}\nReceived message: {\"serverContent\":{\"modelTurn\":{\"parts\":[{\"text\":\"The\"}]}}}\nReceived message: {\"serverContent\":{\"modelTurn\":{\"parts\":[{\"text\":\" last question you asked was: \\\"What is the capital of Brazil?\\\"\\n\"}]}}}\nReceived message: {\"serverContent\":{\"generationComplete\":true}}\nReceived message: {\"serverContent\":{\"turnComplete\":true},\"usageMetadata\":{\"promptTokenCount\":65,\"responseTokenCount\":16,\"totalTokenCount\":81,\"promptTokensDetails\":[{\"modality\":\"TEXT\",\"tokenCount\":65}],\"responseTokensDetails\":[{\"modality\":\"TEXT\",\"tokenCount\":16}]}}\nReceived message: {\"sessionResumptionUpdate\":{\"newHandle\":\"CihmcW04ZzVnZnZwczU2ZnkwN2h1NHpmajFxZmgwcmhieTZ3Zmo3OWt6\",\"resumable\":true}}\nClose:",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Multimodal Live API - Quickstart"
    ]
  },
  {
    "objectID": "quickstarts/Get_started_LiveAPI.html#next-steps",
    "href": "quickstarts/Get_started_LiveAPI.html#next-steps",
    "title": "Multimodal Live API - Quickstart",
    "section": "Next steps",
    "text": "Next steps\n\nThis tutorial just shows basic usage of the Live API, using the Python GenAI SDK.\n\nIf you aren’t looking for code, and just want to try multimedia streaming use Live API in Google AI Studio.\nIf you’re interested in the low level details of using the websockets directly, see the websocket version of this tutorial.\nTry the Tool use in the live API tutorial for an walkthrough of Gemini-2’s new tool use capabilities.\nThere is a Streaming audio in Colab example, but this is more of a demo, it’s not optimized for readability.\nOther nice Gemini 2.0 examples can also be found in the Cookbook’s 2.0 directory, in particular the video understanding and the spatial understanding ones.",
    "crumbs": [
      "Home",
      "Quickstarts",
      "Multimodal Live API - Quickstart"
    ]
  }
]