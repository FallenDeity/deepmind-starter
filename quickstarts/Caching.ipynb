{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gemini API: Context Caching Quickstart\n",
    "\n",
    "This notebook introduces context caching with the Gemini API and provides examples of interacting with the Apollo 11 transcript using the JS     SDK. For a more comprehensive look, check out [the caching guide](https://ai.google.dev/gemini-api/docs/caching?lang=javascript).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Install the Google GenAI SDK\n",
    "\n",
    "Install the Google GenAI SDK from [npm](https://www.npmjs.com/package/@google/genai). \n",
    "\n",
    "```bash\n",
    "$ npm install @google/genai\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup your API key\n",
    "\n",
    "You can [create](https://aistudio.google.com/app/apikey) your API key using Google AI Studio with a single click.\n",
    "\n",
    "Remember to treat your API key like a password. Don't accidentally save it in a notebook or source file you later commit to GitHub. In this notebook we will be storing the API key in a `.env` file. You can also set it as an environment variable or use a secret manager. \n",
    "\n",
    "Here's how to set it up in a `.env` file:\n",
    "\n",
    "```bash\n",
    "$ touch .env\n",
    "$ echo \"GEMINI_API_KEY=<YOUR_API_KEY>\" >> .env\n",
    "```\n",
    "\n",
    ":::{.callout-tip}\n",
    "\n",
    "Another option is to set the API key as an environment variable. You can do this in your terminal with the following command:\n",
    "\n",
    "```bash\n",
    "$ export GEMINI_API_KEY=\"<YOUR_API_KEY>\"\n",
    "```\n",
    ":::\n",
    "\n",
    "### Load the API key\n",
    "\n",
    "To load the API key from the `.env` file, we will use the `dotenv` package. This package loads environment variables from a `.env` file into `process.env`. \n",
    "\n",
    "```bash\n",
    "$ npm install dotenv\n",
    "```\n",
    "\n",
    "Then, we can load the API key in our code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GEMINI_API_KEY is set in the environment variables\n"
     ]
    }
   ],
   "source": [
    "const dotenv = require(\"dotenv\") as typeof import(\"dotenv\");\n",
    "\n",
    "dotenv.config({\n",
    "  path: \"../.env\",\n",
    "});\n",
    "\n",
    "const GEMINI_API_KEY = process.env.GEMINI_API_KEY ?? \"\";\n",
    "if (!GEMINI_API_KEY) {\n",
    "  throw new Error(\"GEMINI_API_KEY is not set in the environment variables\");\n",
    "}\n",
    "console.log(\"GEMINI_API_KEY is set in the environment variables\");\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{.callout-note}\n",
    "In our particular case the `.env` is is one directory up from the notebook, hence we need to use `../` to go up one directory. If the `.env` file is in the same directory as the notebook, you can omit it altogether. \n",
    "\n",
    "```\n",
    "│\n",
    "├── .env\n",
    "└── quickstarts\n",
    "    └── Caching.ipynb\n",
    "```\n",
    ":::\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize SDK Client\n",
    "\n",
    "With the new SDK, now you only need to initialize a client with you API key (or OAuth if using [Vertex AI](https://cloud.google.com/vertex-ai)). The model is now set in each call.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "const google = require(\"@google/genai\") as typeof import(\"@google/genai\");\n",
    "\n",
    "const ai = new google.GoogleGenAI({ apiKey: GEMINI_API_KEY });\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select a model\n",
    "\n",
    "Now select the model you want to use in this guide, either by selecting one in the list or writing it down. Keep in mind that some models, like the 2.5 ones are thinking models and thus take slightly more time to respond (cf. [thinking notebook](quickstarts/Get_started_thinking.ipynb) for more details and in particular learn how to switch the thiking off).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "const tslab = require(\"tslab\") as typeof import(\"tslab\");\n",
    "\n",
    "const MODEL_ID = \"gemini-2.5-flash-preview-05-20\";\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload a file\n",
    "\n",
    "A common pattern with the Gemini API is to ask a number of questions of the same document. Context caching is designed to assist with this case, and can be more efficient by avoiding the need to pass the same tokens through the model for each new request.\n",
    "\n",
    "This example will be based on the transcript from the Apollo 11 mission.\n",
    "\n",
    "Start by downloading that transcript.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "const fs = require(\"fs\") as typeof import(\"fs\");\n",
    "const path = require(\"path\") as typeof import(\"path\");\n",
    "\n",
    "const TEXT_FILE_URL = \"https://storage.googleapis.com/generativeai-downloads/data/a11.txt\";\n",
    "\n",
    "const downloadFile = async (url: string, filePath: string) => {\n",
    "  const response = await fetch(url);\n",
    "  if (!response.ok) {\n",
    "    throw new Error(`Failed to download image: ${response.statusText}`);\n",
    "  }\n",
    "  const buffer = await response.blob();\n",
    "  const bufferData = Buffer.from(await buffer.arrayBuffer());\n",
    "  fs.writeFileSync(filePath, bufferData);\n",
    "};\n",
    "\n",
    "const textFilePath = path.join(\"../assets\", \"a11.txt\");\n",
    "await downloadFile(TEXT_FILE_URL, textFilePath);\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now upload the transcript using the [File API](quickstarts/File_API.ipynb).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "const text_file = await ai.files.upload({\n",
    "  file: textFilePath,\n",
    "  config: {\n",
    "    displayName: \"a11.txt\",\n",
    "    mimeType: \"text/plain\",\n",
    "  },\n",
    "});\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cache the prompt\n",
    "\n",
    "Next create a `CachedContent` object specifying the prompt you want to use, including the file and other fields you wish to cache. In this example the `systemInstruction` has been set, and the document was provided in the prompt.\n",
    "\n",
    "Note that caches are model specific. You cannot use a cache made with a different model as their tokenization might be slightly different.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": \"cachedContents/or2aw5wd2llyqw6dwjebcx3jugv0dl85rir0zfsh\",\n",
      "  \"displayName\": \"\",\n",
      "  \"model\": \"models/gemini-2.5-flash-preview-05-20\",\n",
      "  \"createTime\": \"2025-06-12T16:52:30.246524Z\",\n",
      "  \"updateTime\": \"2025-06-12T16:52:30.246524Z\",\n",
      "  \"expireTime\": \"2025-06-12T17:52:29.753060927Z\",\n",
      "  \"usageMetadata\": {\n",
      "    \"totalTokenCount\": 323384\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "const apollo_cache = await ai.caches.create({\n",
    "  model: \"gemini-2.5-flash-preview-05-20\",\n",
    "  config: {\n",
    "    contents: [google.createPartFromUri(text_file.uri ?? \"\", text_file.mimeType ?? \"text/plain\")],\n",
    "    systemInstruction: \"You are an expert at analyzing transcripts.\",\n",
    "  },\n",
    "});\n",
    "console.log(JSON.stringify(apollo_cache, null, 2));\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "As you can see in the output, you just cached **323384** tokens."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tslab.display.markdown(\n",
    "  `As you can see in the output, you just cached **${apollo_cache.usageMetadata?.totalTokenCount}** tokens.`\n",
    ");\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manage the cache expiry\n",
    "\n",
    "Once you have a `CachedContent` object, you can update the expiry time to keep it alive while you need it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": \"cachedContents/or2aw5wd2llyqw6dwjebcx3jugv0dl85rir0zfsh\",\n",
      "  \"displayName\": \"\",\n",
      "  \"model\": \"models/gemini-2.5-flash-preview-05-20\",\n",
      "  \"createTime\": \"2025-06-12T16:52:30.246524Z\",\n",
      "  \"updateTime\": \"2025-06-12T16:52:30.825743Z\",\n",
      "  \"expireTime\": \"2025-06-12T18:52:30.785032103Z\",\n",
      "  \"usageMetadata\": {\n",
      "    \"totalTokenCount\": 323384\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "const updated_apollo_cache = await ai.caches.update({\n",
    "  name: apollo_cache.name ?? \"\",\n",
    "  config: {\n",
    "    ttl: \"7200s\",\n",
    "  },\n",
    "});\n",
    "console.log(JSON.stringify(updated_apollo_cache, null, 2));\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the cache for generation\n",
    "\n",
    "To use the cache for generation, you can pass the `CachedContent` object name to the `cachedContentName` field in the generation request. This will allow the model to use the cached content for the generation, avoiding the need to pass the same tokens through the model again.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "One lighthearted moment occurs on page 20, around timestamp `00 03 48 45`:\n",
       "\n",
       "> **LMP:** LM looks to be in pretty fine shape from about all we can see from here.\n",
       "\n",
       "> **CC:** Okay. In reference to your question on this step 13 on the decal, I understand that you have used up the contents of the REPRESS O2 package and at that time, instead of being up to 5 psi, you were reading 4.4. Is that correct?\n",
       "\n",
       "> **CMP:** Okay. 4.4. Yes sir.\n",
       "\n",
       "> **CC:** Okay. And you want to know if you can go ahead and use additional oxygen to bring the command module up to 5.0 and continue the equalization? Over.\n",
       "\n",
       "> **CMP:** Yes. We think it's within normal tolerances, Bruce. We just wanted to get your concurrence before we press on with this procedure.\n",
       "\n",
       "> **CC:** Roger, Apollo 11. Go ahead.\n",
       "\n",
       "> **CMP:** Okay. We're pressing on with the procedure.\n",
       "\n",
       "> **CC:** And 11, Houston. We have a request for you. On the service module secondary propellant fuel pressurization valve: As a precautionary measure, we'd like you to momentarily cycle the four switches to the CLOSE position and then release. As you know, we have no TM or talkback on these valve positions, and it's conceivable that one of them might also have been moved into a different position by the shock of separation. Over.\n",
       "\n",
       "> **CMP:** Okay. Good idea. That's being done.\n",
       "\n",
       "> **CC:** Houston. Roger. Out.\n",
       "\n",
       "\n",
       "The humor comes from the casual, almost apologetic tone of the controller (\"Good idea. That's being done.\")  in response to a seemingly trivial request to cycle some switches,  creating a lighthearted contrast to the highly technical nature of the conversation.  The astronauts' matter-of-fact response further enhances this effect.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "const transcript_response = await ai.models.generateContent({\n",
    "  model: MODEL_ID,\n",
    "  contents: [\"Find a lighthearted moment from this transcript\"],\n",
    "  config: {\n",
    "    cachedContent: updated_apollo_cache.name ?? \"\",\n",
    "  },\n",
    "});\n",
    "tslab.display.markdown(transcript_response.text ?? \"\");\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can inspect token usage through `usageMetadata`. Note that the cached prompt tokens are included in `promptTokenCount`, but excluded from the `totalTokenCount`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"promptTokenCount\": 323392,\n",
      "  \"candidatesTokenCount\": 423,\n",
      "  \"totalTokenCount\": 323815,\n",
      "  \"cachedContentTokenCount\": 323384,\n",
      "  \"promptTokensDetails\": [\n",
      "    {\n",
      "      \"modality\": \"TEXT\",\n",
      "      \"tokenCount\": 323392\n",
      "    }\n",
      "  ],\n",
      "  \"cacheTokensDetails\": [\n",
      "    {\n",
      "      \"modality\": \"TEXT\",\n",
      "      \"tokenCount\": 323384\n",
      "    }\n",
      "  ],\n",
      "  \"candidatesTokensDetails\": [\n",
      "    {\n",
      "      \"modality\": \"TEXT\",\n",
      "      \"tokenCount\": 423\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "console.log(JSON.stringify(transcript_response.usageMetadata, null, 2));\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "   As you can see in the `usageMetadata`, the token usage is split between:\n",
       "\n",
       "  *  323384 tokens for the cache,\n",
       "  *  323392 tokens for the input (including the cache, so 8 for the actual prompt),\n",
       "  *  423 tokens for the output,\n",
       "  *  323815 tokens in total.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tslab.display.markdown(`\n",
    "  As you can see in the \\`usageMetadata\\`, the token usage is split between:\n",
    "\n",
    "  *  ${transcript_response.usageMetadata?.cachedContentTokenCount} tokens for the cache,\n",
    "  *  ${transcript_response.usageMetadata?.promptTokenCount} tokens for the input (including the cache, so ${(transcript_response.usageMetadata?.promptTokenCount ?? 0) - (transcript_response.usageMetadata?.cachedContentTokenCount ?? 0)} for the actual prompt),\n",
    "  *  ${transcript_response.usageMetadata?.candidatesTokenCount} tokens for the output,\n",
    "  *  ${transcript_response.usageMetadata?.totalTokenCount} tokens in total.\n",
    "`);\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can ask new questions of the model, and the cache is reused.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The most important part of the transcript is the moment of landing.  The quote is:\n",
       "\n",
       "**\"Houston, Tranquility Base here. The Eagle has landed.\"**\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "const chat = ai.chats.create({\n",
    "  model: MODEL_ID,\n",
    "  config: {\n",
    "    cachedContent: updated_apollo_cache.name ?? \"\",\n",
    "  },\n",
    "});\n",
    "const chat_response_1 = await chat.sendMessage({\n",
    "  message: \"Give me a quote from the most important part of the transcript.\",\n",
    "});\n",
    "tslab.display.markdown(chat_response_1.text ?? \"\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Immediately following the announcement \"Houston, Tranquility Base here. The Eagle has landed.\", the following events and communications are recounted in the transcript:\n",
       "\n",
       "* **Confirmation from Houston:** Mission Control's response was, \"Roger, Tranquility. We copy you on the ground. You got a bunch of guys about to turn blue. We're breathing again. Thanks a lot.\"  This expresses the immense relief and joy at Mission Control.\n",
       "\n",
       "* **Events in the Lunar Module:**  The transcript then details actions taken by the astronauts inside the Lunar Module, including:\n",
       "    * Armstrong and Aldrin confirming the MASTER ARM was ON.\n",
       "    * Aldrin noting a \"very smooth touchdown.\"\n",
       "    * Aldrin reporting the venting of oxidizer.\n",
       "    * Confirmation from Houston that Eagle was to STAY for T1 (the first planned post-landing activity).\n",
       "    * Further actions taken to secure the Lunar Module after landing.\n",
       "\n",
       "* **Communications with Columbia:**  Michael Collins in the Command Module *Columbia* is heard confirming that he heard the landing.  There's a brief exchange with Houston and Collins about *Columbia*'s status.\n",
       "\n",
       "\n",
       "In short, the immediate aftermath of the landing announcement focuses on confirmation of the successful landing, actions to secure the Lunar Module, and initial communications with the Command Module and Mission Control, highlighting the relief and the transition to the next phase of the mission.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "const chat_response_2 = await chat.sendMessage({\n",
    "  message: \"What was recounted after that?\",\n",
    "  config: {\n",
    "    cachedContent: updated_apollo_cache.name ?? \"\",\n",
    "  },\n",
    "});\n",
    "tslab.display.markdown(chat_response_2.text ?? \"\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"promptTokenCount\": 323439,\n",
      "  \"candidatesTokenCount\": 287,\n",
      "  \"totalTokenCount\": 323726,\n",
      "  \"cachedContentTokenCount\": 323384,\n",
      "  \"promptTokensDetails\": [\n",
      "    {\n",
      "      \"modality\": \"TEXT\",\n",
      "      \"tokenCount\": 323439\n",
      "    }\n",
      "  ],\n",
      "  \"cacheTokensDetails\": [\n",
      "    {\n",
      "      \"modality\": \"TEXT\",\n",
      "      \"tokenCount\": 323384\n",
      "    }\n",
      "  ],\n",
      "  \"candidatesTokensDetails\": [\n",
      "    {\n",
      "      \"modality\": \"TEXT\",\n",
      "      \"tokenCount\": 287\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "console.log(JSON.stringify(chat_response_2.usageMetadata, null, 2));\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "  As you can see in the `usageMetadata`, the token usage is split between:\n",
       "\n",
       "  *  323384 tokens for the cache,\n",
       "  *  323439 tokens for the input (including the cache, so 55 for the actual prompt),\n",
       "  *  287 tokens for the output,\n",
       "  *  323726 tokens in total.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tslab.display.markdown(`\n",
    "  As you can see in the \\`usageMetadata\\`, the token usage is split between:\n",
    "\n",
    "  *  ${chat_response_2.usageMetadata?.cachedContentTokenCount} tokens for the cache,\n",
    "  *  ${chat_response_2.usageMetadata?.promptTokenCount} tokens for the input (including the cache, so ${(chat_response_2.usageMetadata?.promptTokenCount ?? 0) - (chat_response_2.usageMetadata?.cachedContentTokenCount ?? 0)} for the actual prompt),\n",
    "  *  ${chat_response_2.usageMetadata?.candidatesTokenCount} tokens for the output,\n",
    "  *  ${chat_response_2.usageMetadata?.totalTokenCount} tokens in total.\n",
    "`);\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the cached tokens are cheaper than the normal ones, it means this prompt was much cheaper that if you had not used caching. Check the [pricing here](https://ai.google.dev/pricing) for the up-to-date discount on cached tokens.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete the cache\n",
    "\n",
    "The cache has a small recurring storage cost (cf. [pricing](https://ai.google.dev/pricing)) so by default it is only saved for an hour. In this case you even set it up for a shorter amont of time (using `\"ttl\"`) of 2h.\n",
    "\n",
    "Still, if you don't need you cache anymore, it is good practice to delete it proactively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cachedContents/or2aw5wd2llyqw6dwjebcx3jugv0dl85rir0zfsh\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Cache deleted successfully."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "console.log(updated_apollo_cache.name ?? \"\");\n",
    "await ai.caches.delete({\n",
    "  name: updated_apollo_cache.name ?? \"\",\n",
    "});\n",
    "tslab.display.markdown(\"Cache deleted successfully.\");\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "### Useful API references:\n",
    "\n",
    "If you want to know more about the caching API, you can check the full [API specifications](https://ai.google.dev/api/caching) and the [caching documentation](https://ai.google.dev/api/caching).\n",
    "\n",
    "### Continue your discovery of the Gemini API\n",
    "\n",
    "Check the File API notebook to know more about that API. The [vision capabilities](quickstarts/Video.ipynb) of the Gemini API are a good reason to use the File API and the caching. The Gemini API also has configurable [safety settings](quickstarts/Safety.ipynb) that you might have to customize when dealing with big files.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TypeScript",
   "language": "typescript",
   "name": "tslab"
  },
  "language_info": {
   "codemirror_mode": {
    "mode": "typescript",
    "name": "javascript",
    "typescript": true
   },
   "file_extension": ".ts",
   "mimetype": "text/typescript",
   "name": "typescript",
   "version": "3.7.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
