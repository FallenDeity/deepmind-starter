{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get started with Music generation using Lyria RealTime\n",
    "\n",
    "[Lyria RealTime](https://deepmind.google/technologies/lyria/), provides access to a state-of-the-art, real-time, streaming music generation model. It allows developers to build applications where users can interactively create, continuously steer, and perform instrumental music using text prompts.\n",
    "\n",
    "Lyria RealTime main characteristics are:\n",
    "\n",
    "- **Highest quality text-to-audio model**: Lyria RealTime generates high-quality instrumental music (no voice) using the latest models produced by DeepMind.\n",
    "- **Non-stopping music**: Using websockets, Lyria RealTime continuously generates music in real time.\n",
    "- **Mix and match influences**: Prompt the model to describe musical idea, genre, instrument, mood, or characteristic. The prompts can be mixed to blend influences and create unique compositions.\n",
    "- **Creative control**: Set the `guidance`, the `bpm`, the `density` of musical notes/sounds, the `brightness` and the `scale` in real time. The model will smoothly transition based on the new input.\n",
    "\n",
    "Check Lyria RealTime's [documentation](https://ai.google.dev/gemini-api/docs/music-generation) for more details.\n",
    "\n",
    ":::{.callout-important}\n",
    "\n",
    "Lyria RealTime is a preview feature. It is free to use for now with quota limitations, but is subject to change.\n",
    "\n",
    ":::\n",
    "\n",
    "Also note that due to Colab limitation, you won't be able to experience the real time capabilities of Lyria RealTime but only limited audio output. Use the AI studio's apps, [Prompt DJ](https://aistudio.google.com/apps/bundled/promptdj) and [MIDI DJ](https://aistudio.google.com/apps/bundled/promptdj-midi) to fully experience Lyria RealTime"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Install the Google GenAI SDK\n",
    "\n",
    "Install the Google GenAI SDK from [npm](https://www.npmjs.com/package/@google/genai). \n",
    "\n",
    "```bash\n",
    "$ npm install @google/genai\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup your API key\n",
    "\n",
    "You can [create](https://aistudio.google.com/app/apikey) your API key using Google AI Studio with a single click.\n",
    "\n",
    "Remember to treat your API key like a password. Don't accidentally save it in a notebook or source file you later commit to GitHub. In this notebook we will be storing the API key in a `.env` file. You can also set it as an environment variable or use a secret manager. \n",
    "\n",
    "Here's how to set it up in a `.env` file:\n",
    "\n",
    "```bash\n",
    "$ touch .env\n",
    "$ echo \"GEMINI_API_KEY=<YOUR_API_KEY>\" >> .env\n",
    "```\n",
    "\n",
    ":::{.callout-tip}\n",
    "\n",
    "Another option is to set the API key as an environment variable. You can do this in your terminal with the following command:\n",
    "\n",
    "```bash\n",
    "$ export GEMINI_API_KEY=\"<YOUR_API_KEY>\"\n",
    "```\n",
    ":::\n",
    "\n",
    "### Load the API key\n",
    "\n",
    "To load the API key from the `.env` file, we will use the `dotenv` package. This package loads environment variables from a `.env` file into `process.env`. \n",
    "\n",
    "```bash\n",
    "$ npm install dotenv\n",
    "```\n",
    "\n",
    "Then, we can load the API key in our code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GEMINI_API_KEY is set in the environment variables\n"
     ]
    }
   ],
   "source": [
    "const dotenv = require(\"dotenv\") as typeof import(\"dotenv\");\n",
    "\n",
    "dotenv.config({\n",
    "  path: \"../.env\",\n",
    "});\n",
    "\n",
    "const GEMINI_API_KEY = process.env.GEMINI_API_KEY ?? \"\";\n",
    "if (!GEMINI_API_KEY) {\n",
    "  throw new Error(\"GEMINI_API_KEY is not set in the environment variables\");\n",
    "}\n",
    "console.log(\"GEMINI_API_KEY is set in the environment variables\");\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{.callout-note}\n",
    "In our particular case the `.env` is is one directory up from the notebook, hence we need to use `../` to go up one directory. If the `.env` file is in the same directory as the notebook, you can omit it altogether. \n",
    "\n",
    "```\n",
    "│\n",
    "├── .env\n",
    "└── quickstarts\n",
    "    └── Get_started_LyriaRealTime.ipynb\n",
    "```\n",
    ":::\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize SDK Client\n",
    "\n",
    "Lyria RealTime API is a new capability introduced with the Lyria RealTime model so only works with the `lyria-realtime-exp` model. As it's an experimental feature, you also need to use the `v1alpha` client version.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "const google = require(\"@google/genai\") as typeof import(\"@google/genai\");\n",
    "\n",
    "const ai = new google.GoogleGenAI({ apiKey: GEMINI_API_KEY, httpOptions: { apiVersion: \"v1alpha\" } });\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select a model\n",
    "\n",
    "Multimodal Live API are a new capability introduced with the [Gemini 2.0](https://ai.google.dev/gemini-api/docs/models/gemini-v2) model. It won't work with previous generation models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "const tslab = require(\"tslab\") as typeof import(\"tslab\");\n",
    "\n",
    "const MODEL_ID = \"models/lyria-realtime-exp\";\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilites\n",
    "\n",
    "You're going to use the Lyria Realtime's audio output, the easiest way hear it in Colab is to write the PCM data out as a WAV file:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "const fs = require(\"fs\") as typeof import(\"fs\");\n",
    "const path = require(\"path\") as typeof import(\"path\");\n",
    "const wave = require(\"wavefile\") as typeof import(\"wavefile\");\n",
    "\n",
    "function saveAudioToFile(audioData: Int16Array, filePath: string) {\n",
    "  fs.mkdirSync(path.dirname(filePath), { recursive: true });\n",
    "  const wav = new wave.WaveFile();\n",
    "  wav.fromScratch(2, 48000, \"16\", audioData);\n",
    "  fs.writeFileSync(filePath, wav.toBuffer());\n",
    "  console.debug(`Audio saved to ${filePath}`);\n",
    "}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate music\n",
    "\n",
    "The Lyria Realtime model utilizes websockets to stream audio data in real time. The model can be prompted with text descriptions of the desired music, and it will generate audio that matches the description and stream it in chunks. It takes 2 different configuration parameters as input:\n",
    "\n",
    "- `WeightedPrompt`: A list of text prompts that describe the desired music. Each prompt can have a `weight` that indicates its influence on the generated music. The prompts can be sent while the session is active, allowing for continuous steering of the music generation.\n",
    "- `LiveMusicGenerationConfig`: A configuration object that specifies the desired characteristics of the generated music, such as `bpm`, `density`, `brightness`, `scale`, and `guidance`. These parameters can be adjusted in real time to influence the music generation.\n",
    "\n",
    ":::{.callout-important}\n",
    "You can't just update a single parameter in the `LiveMusicGenerationConfig` object. You need to send the entire object with all the parameters each time you want to update it, otherwise the other parameters will be reset to their default values.\n",
    "\n",
    "Any updates to `bpm` or `scale` need to be followed by a `resetContext` call to reset the context of the music generation. This is because these parameters affect the musical structure and need to be applied from the beginning of the generation.\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { LiveMusicGenerationConfig, LiveMusicSession, LiveMusicServerMessage, WeightedPrompt } from \"@google/genai\";\n",
    "\n",
    "let n_index = 0;\n",
    "const MAX_CHUNKS = 10; // Maximum number of audio chunks to process\n",
    "const responseQueue: LiveMusicServerMessage[] = [];\n",
    "\n",
    "async function receive() {\n",
    "  console.debug(\"Receiving audio chunks...\");\n",
    "  let done = false;\n",
    "  let chunk_count = 0;\n",
    "  const audioChunks: number[][] = [];\n",
    "  while (!done) {\n",
    "    if (responseQueue.length > 0) {\n",
    "      const response = responseQueue.shift();\n",
    "      if (response?.audioChunk?.data) {\n",
    "        const audioBuffer = Buffer.from(response.audioChunk.data, \"base64\");\n",
    "        const intArray = new Int16Array(\n",
    "          audioBuffer.buffer,\n",
    "          audioBuffer.byteOffset,\n",
    "          audioBuffer.length / Int16Array.BYTES_PER_ELEMENT\n",
    "        );\n",
    "        audioChunks.push(Array.from(intArray));\n",
    "        chunk_count++;\n",
    "      }\n",
    "      if (chunk_count >= MAX_CHUNKS) {\n",
    "        done = true;\n",
    "        console.debug(\"Received complete response\");\n",
    "      }\n",
    "    } else {\n",
    "      await new Promise((resolve) => setTimeout(resolve, 100));\n",
    "    }\n",
    "  }\n",
    "  const audioFilePath = path.join(\"../assets/live\", `lyria_realtime_${n_index}.wav`);\n",
    "  saveAudioToFile(new Int16Array(audioChunks.flat()), audioFilePath);\n",
    "  tslab.display.html(`\n",
    "    <h3>Audio Response Lyria</h3>\n",
    "    <audio controls>\n",
    "        <source src=\"../assets/live/lyria_realtime_${n_index}.wav\" type=\"audio/wav\">\n",
    "        Your browser does not support the audio element.\n",
    "    </audio>\n",
    "  `);\n",
    "  n_index++;\n",
    "}\n",
    "\n",
    "async function generateMusic(prompts: WeightedPrompt[], config: LiveMusicGenerationConfig) {\n",
    "  const session: LiveMusicSession = await ai.live.music.connect({\n",
    "    model: MODEL_ID,\n",
    "    callbacks: {\n",
    "      onmessage: (message) => {\n",
    "        responseQueue.push(message);\n",
    "      },\n",
    "      onerror: (error) => {\n",
    "        console.error(\"music session error:\", error);\n",
    "      },\n",
    "      onclose: () => {\n",
    "        console.log(\"Lyria RealTime stream closed.\");\n",
    "      },\n",
    "    },\n",
    "  });\n",
    "\n",
    "  await session.setWeightedPrompts({\n",
    "    weightedPrompts: prompts,\n",
    "  });\n",
    "  await session.setMusicGenerationConfig({\n",
    "    musicGenerationConfig: config,\n",
    "  });\n",
    "\n",
    "  console.debug(\"Lyria Realtime session started\");\n",
    "  session.play();\n",
    "  await receive();\n",
    "  session.close();\n",
    "  console.debug(\"Lyria Realtime session closed\");\n",
    "}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio Generation Function\n",
    "\n",
    "The above code sample shows how to generate music using the Lyria Realtime model. There are two methods worth noting:\n",
    "\n",
    "### `generateMusic` - Driver method\n",
    "\n",
    "This method is used to start the music generation process. It takes an array of `WeightedPrompt` objects and a `LiveMusicGenerationConfig` object as input. It returns a `LiveMusicGenerationSession` object that can be used to interact with the music generation session.\n",
    "\n",
    "This method:\n",
    "- Opens a websocket connection to the Lyria Realtime model.\n",
    "- Sends the initial prompts to the model using `setWeightedPrompts`, which sets the initial musical influences.\n",
    "- Sends the initial configuration using `setLiveMusicGenerationConfig`, which sets the desired characteristics of the generated music.\n",
    "- Sets up event listeners to handle incoming audio data and errors and start the audio playback.\n",
    "\n",
    "### `receive` - Audio data handler\n",
    "\n",
    "This methods is used to handle incoming audio data from the Lyria Realtime model. It monitors the `responseQueue` for incoming audio data and collects it in a buffer. When the buffer reaches a certain size, it writes the audio data to a WAV file and plays it back using the `saveAudioToFile` utility function.\n",
    "\n",
    ":::{.callout-note}\n",
    "\n",
    "Currently once the `receive` method is called, it blocks further function execution till required number of chunks are met. This means that you won't be able to send new prompts or configuration updates while the `receive` method is running. Ideally, in a real-time application, you would want to run the `receive` method in a separate thread while also having a `send` method to send new prompts and configuration updates.\n",
    "\n",
    ":::\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Lyria Realtime\n",
    "\n",
    "Because of Colab limitation you won't be able to experience the \"real time\" part of Lyria RealTime, so all those examples are going to be one-offs prompt to get an audio file.\n",
    "\n",
    "One thing to note is that the audio will only be played at the end of the session when all would have been written in the wav file. When using the API for real you'll be able to start plyaing as soon as the first chunk arrives. So the longer the duration (using the dedicated parameter) you set, the longer you'll have to wait until you hear something.\n",
    "\n",
    "### Simple Lyria RealTime example\n",
    "\n",
    "Here's first a simple example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Live music generation is experimental and may change in future versions.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lyria Realtime session started\n",
      "Receiving audio chunks...\n",
      "Received complete response\n",
      "Audio saved to ../assets/live/lyria_realtime_0.wav\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <h3>Audio Response Lyria</h3>\n",
       "    <audio controls>\n",
       "        <source src=\"../assets/live/lyria_realtime_0.wav\" type=\"audio/wav\">\n",
       "        Your browser does not support the audio element.\n",
       "    </audio>\n",
       "  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lyria Realtime session closed\n"
     ]
    }
   ],
   "source": [
    "await generateMusic(\n",
    "  [\n",
    "    {\n",
    "      text: \"piano\",\n",
    "      weight: 1.0,\n",
    "    },\n",
    "  ],\n",
    "  { bpm: 120, density: 1.0 }\n",
    ");\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Lyria RealTime by yourself\n",
    "\n",
    "Now you can try mixing multiple prompts, and tinkering with the music configuration.\n",
    "\n",
    "The prompts needs to follow their specific format which is a list of prompts with weights (which can be any values, including negative, except 0) like this:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"text\": \"Text of the prompt\",\n",
    "    \"weight\": 1.0\n",
    "}\n",
    "```\n",
    "\n",
    "You should try to stay simple (unlike when you're using [image-out](quickstarts/Image_out.ipynb)) as the model will better understand things like \"meditation\", \"eerie\", \"harp\" than \"An eerie and relaxing music illustrating the verdoyant forests of Scotland using string instruments\".\n",
    "\n",
    "The music configuration options available to you are:\n",
    "\n",
    "- `bpm`: beats per minute\n",
    "- `guidance`: how strictly the model follows the prompts\n",
    "- `density`: density of musical notes/sounds\n",
    "- `brightness`: tonal quality\n",
    "- `scale`: musical scale (key and mode)\n",
    "  \n",
    "Other options are available (`mute_bass` for ex.). Check the [documentation](https://ai.google.dev/gemini-api/docs/music-generation#controls) for the full list.\n",
    "\n",
    "Select one of the sample prompts (genres, instruments and mood), or write your owns. Check the [documentation](https://ai.google.dev/gemini-api/docs/music-generation#prompt-guide-lyria) for more details and prompt examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Live music generation is experimental and may change in future versions.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lyria RealTime stream closed.\n",
      "Lyria Realtime session started\n",
      "Receiving audio chunks...\n",
      "Received complete response\n",
      "Audio saved to ../assets/live/lyria_realtime_1.wav\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <h3>Audio Response Lyria</h3>\n",
       "    <audio controls>\n",
       "        <source src=\"../assets/live/lyria_realtime_1.wav\" type=\"audio/wav\">\n",
       "        Your browser does not support the audio element.\n",
       "    </audio>\n",
       "  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lyria Realtime session closed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lyria RealTime stream closed.\n"
     ]
    }
   ],
   "source": [
    "await generateMusic(\n",
    "  [\n",
    "    {\n",
    "      text: \"Indie Pop\",\n",
    "      weight: 0.6,\n",
    "    },\n",
    "    {\n",
    "      text: \"Sitar\",\n",
    "      weight: 2,\n",
    "    },\n",
    "    {\n",
    "      text: \"Danceable\",\n",
    "      weight: 1.4,\n",
    "    },\n",
    "  ],\n",
    "  {\n",
    "    bpm: 140,\n",
    "    scale: google.Scale.F_MAJOR_D_MINOR,\n",
    "    density: 0.2,\n",
    "    brightness: 0.7,\n",
    "    guidance: 4.0,\n",
    "  }\n",
    ");\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's next?\n",
    "\n",
    "Now that you know how to generate music, here are other cool things to try:\n",
    "\n",
    "- Instead of music, learn how to generate multi-speakers conversation using the [TTS models](quickstarts/Get_started_TTS.ipynb).\n",
    "- Discover how to generate [images](quickstarts/Get_started_image.ipynb) or [videos](quickstarts/Get_started_Veo.ipynb).\n",
    "- Instead of generation music or audio, find out how to Gemini can [understand Audio files](quickstarts/Audio.ipynb).\n",
    "- Have a real-time conversation with Gemini using the [Live API](quickstarts/Get_started_LiveAPI.ipynb).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TypeScript",
   "language": "typescript",
   "name": "tslab"
  },
  "language_info": {
   "codemirror_mode": {
    "mode": "typescript",
    "name": "javascript",
    "typescript": true
   },
   "file_extension": ".ts",
   "mimetype": "text/typescript",
   "name": "typescript",
   "version": "3.7.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
