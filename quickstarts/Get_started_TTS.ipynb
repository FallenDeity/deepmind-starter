{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gemini API: Gemini Text-to-speech\n",
    "\n",
    "The Gemini API can transform text input into single speaker or multi-speaker audio (podcast-like experience like in [NotebookLM](https://notebooklm.google.com/)). This notebook provides an example of how to control the *Text-to-speech* (TTS) capability of the Gemini model and guide its style, accent, pace, and tone.\n",
    "\n",
    "Before diving in the code, you should try this capability on [AI Studio](https://aistudio.google.com/prompts/new_chat?model=gemini-2.5-flash-preview-05-20-preview-tts).\n",
    "\n",
    "**Note that the TTS model can only do TTS, it does not have the reasoning capabilities of the Gemini models, so you can ask things like \"say this in that style\", but not \"tell me why the sky is blue\".** If that's what you want, you should use the [Live API](quickstarts/Get_started_LiveAPI.ipynb) instead.\n",
    "\n",
    "The [documentation](https://ai.google.dev/gemini-api/docs/speech-generation) is also a good place to start discovering the TTS capability.\n",
    "\n",
    ":::{.callout-important}\n",
    "\n",
    "Audio-out is a preview feature. It is free to use for now with quota limitations, but is subject to change.\n",
    "\n",
    ":::\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Install the Google GenAI SDK\n",
    "\n",
    "Install the Google GenAI SDK from [npm](https://www.npmjs.com/package/@google/genai). \n",
    "\n",
    "```bash\n",
    "$ npm install @google/genai\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup your API key\n",
    "\n",
    "You can [create](https://aistudio.google.com/app/apikey) your API key using Google AI Studio with a single click.\n",
    "\n",
    "Remember to treat your API key like a password. Don't accidentally save it in a notebook or source file you later commit to GitHub. In this notebook we will be storing the API key in a `.env` file. You can also set it as an environment variable or use a secret manager. \n",
    "\n",
    "Here's how to set it up in a `.env` file:\n",
    "\n",
    "```bash\n",
    "$ touch .env\n",
    "$ echo \"GEMINI_API_KEY=<YOUR_API_KEY>\" >> .env\n",
    "```\n",
    "\n",
    ":::{.callout-tip}\n",
    "\n",
    "Another option is to set the API key as an environment variable. You can do this in your terminal with the following command:\n",
    "\n",
    "```bash\n",
    "$ export GEMINI_API_KEY=\"<YOUR_API_KEY>\"\n",
    "```\n",
    ":::\n",
    "\n",
    "### Load the API key\n",
    "\n",
    "To load the API key from the `.env` file, we will use the `dotenv` package. This package loads environment variables from a `.env` file into `process.env`. \n",
    "\n",
    "```bash\n",
    "$ npm install dotenv\n",
    "```\n",
    "\n",
    "Then, we can load the API key in our code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GEMINI_API_KEY is set in the environment variables\n"
     ]
    }
   ],
   "source": [
    "const dotenv = require(\"dotenv\") as typeof import(\"dotenv\");\n",
    "\n",
    "dotenv.config({\n",
    "  path: \"../.env\",\n",
    "});\n",
    "\n",
    "const GEMINI_API_KEY = process.env.GEMINI_API_KEY ?? \"\";\n",
    "if (!GEMINI_API_KEY) {\n",
    "  throw new Error(\"GEMINI_API_KEY is not set in the environment variables\");\n",
    "}\n",
    "console.log(\"GEMINI_API_KEY is set in the environment variables\");\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{.callout-note}\n",
    "In our particular case the `.env` is is one directory up from the notebook, hence we need to use `../` to go up one directory. If the `.env` file is in the same directory as the notebook, you can omit it altogether. \n",
    "\n",
    "```\n",
    "│\n",
    "├── .env\n",
    "└── quickstarts\n",
    "    └── Get_started_TTS.ipynb\n",
    "```\n",
    ":::\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize SDK Client\n",
    "\n",
    "With the new SDK, now you only need to initialize a client with you API key (or OAuth if using [Vertex AI](https://cloud.google.com/vertex-ai)). The model is now set in each call.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "const google = require(\"@google/genai\") as typeof import(\"@google/genai\");\n",
    "\n",
    "const ai = new google.GoogleGenAI({ apiKey: GEMINI_API_KEY });\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select a model\n",
    "\n",
    "Audio-out is only supported by the \"`tts`\" models, `gemini-2.5-flash-preview-tts` and `gemini-2.5-pro-preview-tts`. For more information about all Gemini models, check the [documentation](https://ai.google.dev/gemini-api/docs/models/gemini) for extended information on each of them.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "const tslab = require(\"tslab\") as typeof import(\"tslab\");\n",
    "\n",
    "const MODEL_ID = \"gemini-2.5-flash-preview-tts\";\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilites\n",
    "\n",
    "The simplest way to playback the audio in Colab, is to write it out to a .wav file. So here is a simple wave file writer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "const fs = require(\"fs\") as typeof import(\"fs\");\n",
    "const path = require(\"path\") as typeof import(\"path\");\n",
    "const wave = require(\"wavefile\") as typeof import(\"wavefile\");\n",
    "\n",
    "function saveAudioToFile(audioData: Int16Array, filePath: string) {\n",
    "  fs.mkdirSync(path.dirname(filePath), { recursive: true });\n",
    "  const wav = new wave.WaveFile();\n",
    "  wav.fromScratch(1, 24000, \"16\", audioData);\n",
    "  fs.writeFileSync(filePath, wav.toBuffer());\n",
    "  console.debug(`Audio saved to ${filePath}`);\n",
    "}\n",
    "\n",
    "function base64ToInt16Array(base64String: string): Int16Array {\n",
    "  const buffer = Buffer.from(base64String, \"base64\");\n",
    "  const int16Array = new Int16Array(buffer.buffer, buffer.byteOffset, buffer.length / Int16Array.BYTES_PER_ELEMENT);\n",
    "  return int16Array;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { GenerateContentResponse } from \"@google/genai\";\n",
    "\n",
    "function playAudio(response: GenerateContentResponse, filePath: string) {\n",
    "  if (response.candidates?.[0]?.content) {\n",
    "    const response_content = response.candidates[0].content;\n",
    "    if (response_content.parts) {\n",
    "      const response_blob = response_content.parts[0].inlineData;\n",
    "      if (response_blob?.data) {\n",
    "        const response_filepath = path.join(\"../assets/tts\", filePath);\n",
    "        saveAudioToFile(base64ToInt16Array(response_blob.data), response_filepath);\n",
    "        tslab.display.html(`\n",
    "                  <audio controls>\n",
    "                      <source src=\"${response_filepath}\" type=\"audio/wav\">\n",
    "                      Your browser does not support the audio element.\n",
    "                  </audio>\n",
    "              `);\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a simple audio output\n",
    "\n",
    "Let's start with something simple:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "const simple_response = await ai.models.generateContent({\n",
    "  model: MODEL_ID,\n",
    "  contents: [\"Say 'hello, my name is Gemini!'\"],\n",
    "  config: {\n",
    "    responseModalities: [google.Modality.AUDIO],\n",
    "  },\n",
    "});\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generated ouput is in the response `inlineData` and as you can see it's indeed audio data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio saved to ../assets/tts/simple_response.wav\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                  <audio controls>\n",
       "                      <source src=\"../assets/tts/simple_response.wav\" type=\"audio/wav\">\n",
       "                      Your browser does not support the audio element.\n",
       "                  </audio>\n",
       "              "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "playAudio(simple_response, `simple_response.wav`);\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{.callout-note}\n",
    "\n",
    "Note that the model can only do TTS, so you should always tell it to \"say\", \"read\", \"TTS\" something, otherwise it won't do anything.\n",
    "\n",
    ":::"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Control how the model speaks\n",
    "\n",
    "There are 30 different built-in voices you can use and 24 supported languages which gives you plenty of combinations to try.\n",
    "\n",
    "### Choose a voice\n",
    "\n",
    "Choose a voice among the 30 different ones. You can find their characteristics in the [documentation](https://ai.google.dev/gemini-api/docs/speech-generation#voices).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "const VOICE_ID = \"Leda\";\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio saved to ../assets/tts/custom_voice_response.wav\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                  <audio controls>\n",
       "                      <source src=\"../assets/tts/custom_voice_response.wav\" type=\"audio/wav\">\n",
       "                      Your browser does not support the audio element.\n",
       "                  </audio>\n",
       "              "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "const custom_voice_response = await ai.models.generateContent({\n",
    "  model: MODEL_ID,\n",
    "  contents: [\n",
    "    `Say \"I am a very knowlegeable model, especially when using grounding\", wait 5 seconds then say \"Don't you think?\".`,\n",
    "  ],\n",
    "  config: {\n",
    "    responseModalities: [google.Modality.AUDIO],\n",
    "    speechConfig: {\n",
    "      voiceConfig: {\n",
    "        prebuiltVoiceConfig: {\n",
    "          voiceName: VOICE_ID,\n",
    "        },\n",
    "      },\n",
    "    },\n",
    "  },\n",
    "});\n",
    "playAudio(custom_voice_response, `custom_voice_response.wav`);\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change the language\n",
    "\n",
    "Just tell the model to speak in a certain language and it will. The [documentation](https://ai.google.dev/gemini-api/docs/speech-generation#languages) lists all the supported ones.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio saved to ../assets/tts/custom_language_response.wav\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                  <audio controls>\n",
       "                      <source src=\"../assets/tts/custom_language_response.wav\" type=\"audio/wav\">\n",
       "                      Your browser does not support the audio element.\n",
       "                  </audio>\n",
       "              "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "const custom_language_response = await ai.models.generateContent({\n",
    "  model: MODEL_ID,\n",
    "  contents: [\n",
    "    `\n",
    "    Read this in French:\n",
    "\n",
    "    Les chaussettes de l'archiduchesse sont-elles sèches ? Archi-sèches ?\n",
    "    Un chasseur sachant chasser doit savoir chasser sans son chien.\n",
    "    `,\n",
    "  ],\n",
    "  config: {\n",
    "    responseModalities: [google.Modality.AUDIO],\n",
    "  },\n",
    "});\n",
    "playAudio(custom_language_response, `custom_language_response.wav`);\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt the model to speak in certain ways\n",
    "\n",
    "You can control style, tone, accent, and pace using natural language prompts, for example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio saved to ../assets/tts/custom_style_response.wav\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                  <audio controls>\n",
       "                      <source src=\"../assets/tts/custom_style_response.wav\" type=\"audio/wav\">\n",
       "                      Your browser does not support the audio element.\n",
       "                  </audio>\n",
       "              "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "const custom_style_response = await ai.models.generateContent({\n",
    "  model: MODEL_ID,\n",
    "  contents: [\n",
    "    `\n",
    "    Say in an spooky whisper:\n",
    "    \"By the pricking of my thumbs...\n",
    "    Something wicked this way comes!\"\n",
    "    `,\n",
    "  ],\n",
    "  config: {\n",
    "    responseModalities: [google.Modality.AUDIO],\n",
    "  },\n",
    "});\n",
    "playAudio(custom_style_response, `custom_style_response.wav`);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio saved to ../assets/tts/custom_pace_response.wav\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                  <audio controls>\n",
       "                      <source src=\"../assets/tts/custom_pace_response.wav\" type=\"audio/wav\">\n",
       "                      Your browser does not support the audio element.\n",
       "                  </audio>\n",
       "              "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "const custom_pace_response = await ai.models.generateContent({\n",
    "  model: MODEL_ID,\n",
    "  contents: [\n",
    "    `\n",
    "    Read this disclaimer in as fast a voice as possible while remaining intelligible:\n",
    "\n",
    "    [The author] assumes no responsibility or liability for any errors or omissions in the content of this site.\n",
    "    The information contained in this site is provided on an 'as is' basis with no guarantees of completeness, accuracy, usefulness or timeliness\n",
    "    `,\n",
    "  ],\n",
    "  config: {\n",
    "    responseModalities: [google.Modality.AUDIO],\n",
    "  },\n",
    "});\n",
    "playAudio(custom_pace_response, `custom_pace_response.wav`);\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mutlti-speakers\n",
    "\n",
    "The TTS model can also read discussions between 2 speakers (like [NotebookLM](https://fnotebooklm.google.com/) podcast feature). You just need to tell it that there are two speakers:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio saved to ../assets/tts/multi_response.wav\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                  <audio controls>\n",
       "                      <source src=\"../assets/tts/multi_response.wav\" type=\"audio/wav\">\n",
       "                      Your browser does not support the audio element.\n",
       "                  </audio>\n",
       "              "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "const multi_response = await ai.models.generateContent({\n",
    "  model: MODEL_ID,\n",
    "  contents: [\n",
    "    `\n",
    "    Make Speaker1 sound tired and bored, and Speaker2 sound excited and happy:\n",
    "\n",
    "    Speaker1: So... what's on the agenda today?\n",
    "    Speaker2: You're never going to guess!\n",
    "    `,\n",
    "  ],\n",
    "  config: {\n",
    "    responseModalities: [google.Modality.AUDIO],\n",
    "  },\n",
    "});\n",
    "playAudio(multi_response, `multi_response.wav`);\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also select the voices for each participants and pass their names to the model.\n",
    "\n",
    "But first let's generate a discussion between two scientists:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**(Sound of distant jungle chirps fading, replaced by a slightly crackly podcast mic)**\n",
       "\n",
       "**Dr. Claire:** (Practically bouncing) I'm still buzzing, Aurora! Genuinely buzzing!\n",
       "\n",
       "**Aurora:** (Squealing) Dr. Claire, my hands are still shaking! The *Emerald Mossback*! We actually saw one!\n",
       "\n",
       "**Dr. Claire:** Not just saw it, Aurora! We *documented* its full iridescent display! The way it *melded* with the bromeliads... it was indistinguishable! The camouflage... it's beyond anything in the textbooks!\n",
       "\n",
       "**Aurora:** It was like watching magic! No wonder they were thought extinct for decades! Who could ever spot that? My heart was pounding!\n",
       "\n",
       "**Dr. Claire:** Precisely! This changes our entire understanding of its crypsis. This episode is going to be *legendary* for 'Reptile Revelations'!\n",
       "\n",
       "**Aurora:** Totally! Best day ever, Dr. Claire! Best. Day. Ever!"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "const multi_speaker_transcript = await ai.models.generateContent({\n",
    "  model: \"gemini-2.5-flash-preview-05-20\",\n",
    "  contents: [\n",
    "    `\n",
    "    Hi, please generate a short (like 100 words) transcript that reads like\n",
    "    it was clipped from a podcast by excited herpetologists, Dr. Claire and\n",
    "    her assistant, the young Aurora.\n",
    "    `,\n",
    "  ],\n",
    "});\n",
    "tslab.display.markdown(multi_speaker_transcript.text ?? \"\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio saved to ../assets/tts/transcript_response.wav\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                  <audio controls>\n",
       "                      <source src=\"../assets/tts/transcript_response.wav\" type=\"audio/wav\">\n",
       "                      Your browser does not support the audio element.\n",
       "                  </audio>\n",
       "              "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "const text = multi_speaker_transcript.text ?? \"\";\n",
    "const transcript_response = await ai.models.generateContent({\n",
    "  model: MODEL_ID,\n",
    "  contents: [\n",
    "    `TTS the following conversation between a very excited Dr. Claire and her assistant, the young Aurora: ${text}`,\n",
    "  ],\n",
    "  config: {\n",
    "    responseModalities: [google.Modality.AUDIO],\n",
    "    speechConfig: {\n",
    "      multiSpeakerVoiceConfig: {\n",
    "        speakerVoiceConfigs: [\n",
    "          {\n",
    "            speaker: \"Dr. Claire\",\n",
    "            voiceConfig: {\n",
    "              prebuiltVoiceConfig: {\n",
    "                voiceName: \"Aoede\",\n",
    "              },\n",
    "            },\n",
    "          },\n",
    "          {\n",
    "            speaker: \"Aurora\",\n",
    "            voiceConfig: {\n",
    "              prebuiltVoiceConfig: {\n",
    "                voiceName: \"Leda\",\n",
    "              },\n",
    "            },\n",
    "          },\n",
    "        ],\n",
    "      },\n",
    "    },\n",
    "  },\n",
    "});\n",
    "playAudio(transcript_response, `transcript_response.wav`);\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's next?\n",
    "\n",
    "Now that you know how to generate multi-speaker conversations, here are other cool things to try:\n",
    "\n",
    "- Instead of speech, learn how to generate music conversation using the [Lyria RealTime](quickstarts/Get_started_LyriaRealtime.ipynb).\n",
    "- Discover how to generate [images](quickstarts/Get_started_image.ipynb) or [videos](quickstarts/Get_started_Veo.ipynb).\n",
    "- Instead of generation music or audio, find out how to Gemini can [understand Audio files](quickstarts/Audio.ipynb).\n",
    "- Have a real-time conversation with Gemini using the [Live API](quickstarts/Get_started_LiveAPI.ipynb).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TypeScript",
   "language": "typescript",
   "name": "tslab"
  },
  "language_info": {
   "codemirror_mode": {
    "mode": "typescript",
    "name": "javascript",
    "typescript": true
   },
   "file_extension": ".ts",
   "mimetype": "text/typescript",
   "name": "typescript",
   "version": "3.7.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
