{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multimodal Live API - Quickstart\n",
    "**Preview**: The Live API is in preview.\n",
    "\n",
    "This notebook demonstrates simple usage of the Gemini Multimodal Live API. For an overview of new capabilities refer to the [Gemini Live API docs](https://ai.google.dev/gemini-api/docs/live).\n",
    "\n",
    "This notebook implements a simple turn-based chat where you send messages as text, and the model replies with audio. The API is capable of much more than that. The goal here is to demonstrate with **simple code**.\n",
    "\n",
    "Some features of the API are not working in Colab, to try them it is recommended to have a look at this [Python script](./Get_started_LiveAPI.py) and run it locally.\n",
    "\n",
    "If you aren't looking for code, and just want to try multimedia streaming use [Live API in Google AI Studio](https://aistudio.google.com/app/live).\n",
    "\n",
    "The [Next steps](#next_steps) section at the end of this tutorial provides links to additional resources.\n",
    "\n",
    "#### Native audio output\n",
    "\n",
    "**Info**: Gemini 2.5 introduces [native audio generation](https://ai.google.dev/gemini-api/docs/live#native-audio-output), which directly generates audio output, providing a more natural sounding audio, more expressive voices, more awareness of additional context, e.g., tone, and more proactive responses. You can try a native audio example in this [script](./Get_started_LiveAPI_NativeAudio.py).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Install the Google GenAI SDK\n",
    "\n",
    "Install the Google GenAI SDK from [npm](https://www.npmjs.com/package/@google/genai). \n",
    "\n",
    "```bash\n",
    "$ npm install @google/genai\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup your API key\n",
    "\n",
    "You can [create](https://aistudio.google.com/app/apikey) your API key using Google AI Studio with a single click.\n",
    "\n",
    "Remember to treat your API key like a password. Don't accidentally save it in a notebook or source file you later commit to GitHub. In this notebook we will be storing the API key in a `.env` file. You can also set it as an environment variable or use a secret manager. \n",
    "\n",
    "Here's how to set it up in a `.env` file:\n",
    "\n",
    "```bash\n",
    "$ touch .env\n",
    "$ echo \"GEMINI_API_KEY=<YOUR_API_KEY>\" >> .env\n",
    "```\n",
    "\n",
    ":::{.callout-tip}\n",
    "\n",
    "Another option is to set the API key as an environment variable. You can do this in your terminal with the following command:\n",
    "\n",
    "```bash\n",
    "$ export GEMINI_API_KEY=\"<YOUR_API_KEY>\"\n",
    "```\n",
    ":::\n",
    "\n",
    "### Load the API key\n",
    "\n",
    "To load the API key from the `.env` file, we will use the `dotenv` package. This package loads environment variables from a `.env` file into `process.env`. \n",
    "\n",
    "```bash\n",
    "$ npm install dotenv\n",
    "```\n",
    "\n",
    "Then, we can load the API key in our code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GEMINI_API_KEY is set in the environment variables\n"
     ]
    }
   ],
   "source": [
    "const dotenv = require(\"dotenv\") as typeof import(\"dotenv\");\n",
    "\n",
    "dotenv.config({\n",
    "  path: \"../.env\",\n",
    "});\n",
    "\n",
    "const GEMINI_API_KEY = process.env.GEMINI_API_KEY ?? \"\";\n",
    "if (!GEMINI_API_KEY) {\n",
    "  throw new Error(\"GEMINI_API_KEY is not set in the environment variables\");\n",
    "}\n",
    "console.log(\"GEMINI_API_KEY is set in the environment variables\");\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{.callout-note}\n",
    "In our particular case the `.env` is is one directory up from the notebook, hence we need to use `../` to go up one directory. If the `.env` file is in the same directory as the notebook, you can omit it altogether. \n",
    "\n",
    "```\n",
    "│\n",
    "├── .env\n",
    "└── quickstarts\n",
    "    └── Get_started_LiveAPI.ipynb\n",
    "```\n",
    ":::\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize SDK Client\n",
    "\n",
    "With the new SDK, now you only need to initialize a client with you API key (or OAuth if using [Vertex AI](https://cloud.google.com/vertex-ai)). The model is now set in each call.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "const google = require(\"@google/genai\") as typeof import(\"@google/genai\");\n",
    "\n",
    "const ai = new google.GoogleGenAI({ apiKey: GEMINI_API_KEY });\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text to Text\n",
    "\n",
    "The simplest way to use the Live API is as a text-to-text chat interface, but it can do a lot more than this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "const tslab = require(\"tslab\") as typeof import(\"tslab\");\n",
    "\n",
    "const MODEL_ID = \"gemini-2.0-flash-live-001\";\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Live API uses a streaming model over a WebSocket connection. When you interact with the API, a persistent connection is created. Your input (audio, video, or text) is streamed continuously to the model, and the model's response (text or audio) is streamed back in real-time over the same connection. Here we use a `responseQueue` to handle the streaming responses and determine when the server has finished sending the response.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opened\n",
      "Sent message: Hello? Gemini are you there?\n",
      "Received response: Yes, I am\n",
      "Received response:  here! How can I help you today?\n",
      "\n",
      "Session closed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Close: \n"
     ]
    }
   ],
   "source": [
    "// @ts-expect-error duplicate identifier\n",
    "import { LiveServerMessage, Modality } from \"@google/genai\";\n",
    "\n",
    "async function text_to_text() {\n",
    "  const responseQueue: LiveServerMessage[] = [];\n",
    "  const session = await ai.live.connect({\n",
    "    model: MODEL_ID,\n",
    "    callbacks: {\n",
    "      onopen: function () {\n",
    "        console.debug(\"Opened\");\n",
    "      },\n",
    "      onmessage: function (message) {\n",
    "        responseQueue.push(message);\n",
    "      },\n",
    "      onerror: function (e) {\n",
    "        console.debug(\"Error:\", e.message);\n",
    "      },\n",
    "      onclose: function (e) {\n",
    "        console.debug(\"Close:\", e.reason);\n",
    "      },\n",
    "    },\n",
    "    config: { responseModalities: [Modality.TEXT] },\n",
    "  });\n",
    "  const message = \"Hello? Gemini are you there?\";\n",
    "  session.sendClientContent({\n",
    "    turns: message,\n",
    "    turnComplete: true,\n",
    "  });\n",
    "  console.debug(\"Sent message:\", message);\n",
    "  let done = false;\n",
    "  while (!done) {\n",
    "    if (responseQueue.length > 0) {\n",
    "      const response = responseQueue.shift();\n",
    "      if (response?.text) {\n",
    "        console.debug(\"Received response:\", response.text);\n",
    "      } else if (response?.data) {\n",
    "        console.debug(\"Received data:\", response.data);\n",
    "      }\n",
    "      if (response?.serverContent?.turnComplete) {\n",
    "        done = true;\n",
    "      }\n",
    "    } else {\n",
    "      await new Promise((resolve) => setTimeout(resolve, 100));\n",
    "    }\n",
    "  }\n",
    "  session.close();\n",
    "  console.debug(\"Session closed\");\n",
    "}\n",
    "\n",
    "await text_to_text();\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text to audio\n",
    "\n",
    "The simplest way to playback the audio in Colab, is to write it out to a .wav file. So here is a simple wave file writer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "const fs = require(\"fs\") as typeof import(\"fs\");\n",
    "const path = require(\"path\") as typeof import(\"path\");\n",
    "const wave = require(\"wavefile\") as typeof import(\"wavefile\");\n",
    "\n",
    "function saveAudioToFile(audioData: Int16Array, filePath: string) {\n",
    "  fs.mkdirSync(path.dirname(filePath), { recursive: true });\n",
    "  const wav = new wave.WaveFile();\n",
    "  wav.fromScratch(1, 24000, \"16\", audioData);\n",
    "  fs.writeFileSync(filePath, wav.toBuffer());\n",
    "  console.debug(`Audio saved to ${filePath}`);\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opened\n",
      "Sent message: Hello? Gemini are you there?\n",
      "Received complete response\n",
      "Audio saved to ../assets/live/text_to_audio_response.wav\n",
      "Session closed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<h3>Text to Audio Response</h3>\n",
       "<audio controls>\n",
       "  <source src=\"../assets/live/text_to_audio_response.wav\" type=\"audio/wav\">\n",
       "  Your browser does not support the audio element.\n",
       "</audio>\n",
       "</audio>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Close: \n"
     ]
    }
   ],
   "source": [
    "// @ts-expect-error duplicate identifier\n",
    "import { LiveServerMessage, Modality } from \"@google/genai\";\n",
    "\n",
    "async function text_to_audio() {\n",
    "  const responseQueue: LiveServerMessage[] = [];\n",
    "  const session = await ai.live.connect({\n",
    "    model: MODEL_ID,\n",
    "    callbacks: {\n",
    "      onopen: function () {\n",
    "        console.debug(\"Opened\");\n",
    "      },\n",
    "      onmessage: function (message) {\n",
    "        responseQueue.push(message);\n",
    "      },\n",
    "      onerror: function (e) {\n",
    "        console.debug(\"Error:\", e.message);\n",
    "      },\n",
    "      onclose: function (e) {\n",
    "        console.debug(\"Close:\", e.reason);\n",
    "      },\n",
    "    },\n",
    "    config: { responseModalities: [Modality.AUDIO] },\n",
    "  });\n",
    "  const message = \"Hello? Gemini are you there?\";\n",
    "  session.sendClientContent({\n",
    "    turns: message,\n",
    "    turnComplete: true,\n",
    "  });\n",
    "  console.debug(\"Sent message:\", message);\n",
    "  let done = false;\n",
    "  const chunks: LiveServerMessage[] = [];\n",
    "  while (!done) {\n",
    "    if (responseQueue.length > 0) {\n",
    "      const response = responseQueue.shift();\n",
    "      if (response) {\n",
    "        chunks.push(response);\n",
    "      }\n",
    "      if (response?.serverContent?.turnComplete) {\n",
    "        done = true;\n",
    "        console.debug(\"Received complete response\");\n",
    "      }\n",
    "    } else {\n",
    "      await new Promise((resolve) => setTimeout(resolve, 100));\n",
    "    }\n",
    "  }\n",
    "  const audioData = chunks.reduce<number[]>((acc, message) => {\n",
    "    if (message.data) {\n",
    "      const audioBuffer = Buffer.from(message.data, \"base64\");\n",
    "      const intArray = new Int16Array(\n",
    "        audioBuffer.buffer,\n",
    "        audioBuffer.byteOffset,\n",
    "        audioBuffer.length / Int16Array.BYTES_PER_ELEMENT\n",
    "      );\n",
    "      return acc.concat(Array.from(intArray));\n",
    "    }\n",
    "    return acc;\n",
    "  }, []);\n",
    "  const audioFilePath = path.join(\"../assets/live\", \"text_to_audio_response.wav\");\n",
    "  saveAudioToFile(new Int16Array(audioData), audioFilePath);\n",
    "  session.close();\n",
    "  console.debug(\"Session closed\");\n",
    "}\n",
    "\n",
    "await text_to_audio();\n",
    "tslab.display.html(`\n",
    "  <h3>Text to Audio Response</h3>\n",
    "  <audio controls>\n",
    "    <source src=\"../assets/live/text_to_audio_response.wav\" type=\"audio/wav\">\n",
    "    Your browser does not support the audio element.\n",
    "  </audio>\n",
    "  </audio>\n",
    "`);\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Towards Async Tasks\n",
    "\n",
    "The real power of the Live API is that it's real time, and interruptable. You can't get that full power in a simple sequence of steps. To really use the functionality you will move the `send` and `recieve` operations (and others) into their own async tasks.\n",
    "\n",
    "Because of the limitations of Colab this tutorial doesn't totally implement the interactive async tasks, but it does implement the next step in that direction:\n",
    "\n",
    "- It separates the send and receive, but still runs them sequentially.\n",
    "- In the next tutorial you'll run these in separate async tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opened\n",
      "Sent message: Hello? Gemini are you there?\n",
      "Received complete response\n",
      "Audio saved to ../assets/live/audio_response_0.wav\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<h3>Audio Response 1</h3>\n",
       "<audio controls>\n",
       "    <source src=\"../assets/live/audio_response_0.wav\" type=\"audio/wav\">\n",
       "    Your browser does not support the audio element.\n",
       "</audio>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent message: Can you tell me a joke?\n",
      "Received complete response\n",
      "Audio saved to ../assets/live/audio_response_1.wav\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<h3>Audio Response 2</h3>\n",
       "<audio controls>\n",
       "    <source src=\"../assets/live/audio_response_1.wav\" type=\"audio/wav\">\n",
       "    Your browser does not support the audio element.\n",
       "</audio>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent message: What is the weather like today?\n",
      "Received complete response\n",
      "Audio saved to ../assets/live/audio_response_2.wav\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<h3>Audio Response 3</h3>\n",
       "<audio controls>\n",
       "    <source src=\"../assets/live/audio_response_2.wav\" type=\"audio/wav\">\n",
       "    Your browser does not support the audio element.\n",
       "</audio>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session closed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Close: \n"
     ]
    }
   ],
   "source": [
    "// @ts-expect-error duplicate identifier\n",
    "import { GoogleGenAI, LiveServerMessage, Modality, Session } from \"@google/genai\";\n",
    "\n",
    "class AudioLooper {\n",
    "  private session: Session;\n",
    "  private turnIndex = 0;\n",
    "  private responseQueue: LiveServerMessage[] = [];\n",
    "\n",
    "  constructor(\n",
    "    private ai: GoogleGenAI,\n",
    "    private modelId: string\n",
    "  ) {}\n",
    "\n",
    "  async start() {\n",
    "    this.session = await this.ai.live.connect({\n",
    "      model: this.modelId,\n",
    "      callbacks: {\n",
    "        onopen: () => {\n",
    "          console.debug(\"Opened\");\n",
    "        },\n",
    "        onmessage: (message) => this.responseQueue.push(message),\n",
    "        onerror: (e) => {\n",
    "          console.debug(\"Error:\", e.message);\n",
    "        },\n",
    "        onclose: (e) => {\n",
    "          console.debug(\"Close:\", e.reason);\n",
    "        },\n",
    "      },\n",
    "      config: { responseModalities: [Modality.AUDIO] },\n",
    "    });\n",
    "  }\n",
    "\n",
    "  send(message: string) {\n",
    "    this.session.sendClientContent({\n",
    "      turns: message,\n",
    "      turnComplete: true,\n",
    "    });\n",
    "    console.debug(\"Sent message:\", message);\n",
    "  }\n",
    "\n",
    "  async receive() {\n",
    "    let done = false;\n",
    "    const audioChunks: number[] = [];\n",
    "    while (!done) {\n",
    "      if (this.responseQueue.length > 0) {\n",
    "        const response = this.responseQueue.shift();\n",
    "        if (response?.data) {\n",
    "          const audioBuffer = Buffer.from(response.data, \"base64\");\n",
    "          const intArray = new Int16Array(\n",
    "            audioBuffer.buffer,\n",
    "            audioBuffer.byteOffset,\n",
    "            audioBuffer.length / Int16Array.BYTES_PER_ELEMENT\n",
    "          );\n",
    "          audioChunks.push(...Array.from(intArray));\n",
    "        }\n",
    "        if (response?.serverContent?.turnComplete) {\n",
    "          done = true;\n",
    "          console.debug(\"Received complete response\");\n",
    "        }\n",
    "      } else {\n",
    "        await new Promise((resolve) => setTimeout(resolve, 100));\n",
    "      }\n",
    "    }\n",
    "    const audioFilePath = path.join(\"../assets/live\", `audio_response_${this.turnIndex++}.wav`);\n",
    "    saveAudioToFile(new Int16Array(audioChunks), audioFilePath);\n",
    "    tslab.display.html(`\n",
    "      <h3>Audio Response ${this.turnIndex}</h3>\n",
    "      <audio controls>\n",
    "          <source src=\"../assets/live/audio_response_${this.turnIndex - 1}.wav\" type=\"audio/wav\">\n",
    "          Your browser does not support the audio element.\n",
    "      </audio>\n",
    "    `);\n",
    "  }\n",
    "\n",
    "  stop() {\n",
    "    this.session.close();\n",
    "    console.debug(\"Session closed\");\n",
    "  }\n",
    "}\n",
    "\n",
    "async function asyncAudioLooper() {\n",
    "  const audioLooper = new AudioLooper(ai, MODEL_ID);\n",
    "  await audioLooper.start();\n",
    "\n",
    "  // Simulate sending messages\n",
    "  const messages = [\"Hello? Gemini are you there?\", \"Can you tell me a joke?\", \"What is the weather like today?\"];\n",
    "\n",
    "  for (const message of messages) {\n",
    "    audioLooper.send(message);\n",
    "    await audioLooper.receive();\n",
    "  }\n",
    "\n",
    "  audioLooper.stop();\n",
    "}\n",
    "\n",
    "await asyncAudioLooper();\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code is divided into several sections:\n",
    "\n",
    "- `start`: Initializes the client and sets up the WebSocket connection.\n",
    "- `send`: Sends a message to the model.\n",
    "- `receive`: Receives the model's response and collects the audio chunks in a loop and writes them to `wav` file. It breaks when the model indicates it has finished sending the response.\n",
    "- `asyncAudioLooper`: This is the main driver function that brings everything together. It initializes the client, starts the WebSocket connection, and then enters a loop where it sends messages and receives responses."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with resumable sessions\n",
    "\n",
    "Session resumption allows you to return to a previous interaction with the Live API by sending the last session handle you got from the previous session.\n",
    "\n",
    "When you set your session to be resumable, the session information keeps stored on the Live API for up to 24 hours. In this time window, you can resume the conversation and refer to previous information you have shared with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to the service with handle undefined...\n",
      "Opened\n",
      "Sending message: Hello\n",
      "Received message: {\"setupComplete\":{}}\n",
      "Received message: {\"sessionResumptionUpdate\":{}}\n",
      "Received message: {\"serverContent\":{\"modelTurn\":{\"parts\":[{\"text\":\"Hello there! How\"}]}}}\n",
      "Received message: {\"serverContent\":{\"modelTurn\":{\"parts\":[{\"text\":\" can I help you today?\\n\"}]}}}\n",
      "Received message: {\"serverContent\":{\"generationComplete\":true}}\n",
      "Received message: {\"serverContent\":{\"turnComplete\":true},\"usageMetadata\":{\"promptTokenCount\":9,\"responseTokenCount\":11,\"totalTokenCount\":20,\"promptTokensDetails\":[{\"modality\":\"TEXT\",\"tokenCount\":9}],\"responseTokensDetails\":[{\"modality\":\"TEXT\",\"tokenCount\":11}]}}\n",
      "Sending message: What is the capital of Brazil?\n",
      "Received message: {\"sessionResumptionUpdate\":{\"newHandle\":\"CihqdTFxaG1ua2g2aTkweWtiNzB5Ymdzc3V0bW16eDE2ZGkxaXR2d2dt\",\"resumable\":true}}\n",
      "Received message: {\"sessionResumptionUpdate\":{}}\n",
      "Received message: {\"serverContent\":{\"modelTurn\":{\"parts\":[{\"text\":\"The capital of Brazil\"}]}}}\n",
      "Received message: {\"serverContent\":{\"modelTurn\":{\"parts\":[{\"text\":\" is **Brasília**.\\n\"}]}}}\n",
      "Received message: {\"serverContent\":{\"generationComplete\":true}}\n",
      "Received message: {\"serverContent\":{\"turnComplete\":true},\"usageMetadata\":{\"promptTokenCount\":37,\"responseTokenCount\":10,\"totalTokenCount\":47,\"promptTokensDetails\":[{\"modality\":\"TEXT\",\"tokenCount\":37}],\"responseTokensDetails\":[{\"modality\":\"TEXT\",\"tokenCount\":10}]}}\n",
      "Received message: {\"sessionResumptionUpdate\":{\"newHandle\":\"CihrNGZyMjh4dXY3cXFkYzVmMjR5cnlmZ2w5bnBvNTRhcmoxNW1lN2Fi\",\"resumable\":true}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Close: \n"
     ]
    }
   ],
   "source": [
    "// @ts-expect-error duplicate identifier\n",
    "import { LiveServerMessage, Modality } from \"@google/genai\";\n",
    "\n",
    "let HANDLE: string | undefined = undefined;\n",
    "\n",
    "async function resumable_session(\n",
    "  previousSessionHandle?: string,\n",
    "  messages: string[] = [\"Hello\", \"What is the capital of Brazil?\"]\n",
    ") {\n",
    "  const responseQueue: LiveServerMessage[] = [];\n",
    "\n",
    "  async function waitMessage(): Promise<LiveServerMessage> {\n",
    "    let done = false;\n",
    "    let message: LiveServerMessage | undefined = undefined;\n",
    "    while (!done) {\n",
    "      message = responseQueue.shift();\n",
    "      if (message) {\n",
    "        done = true;\n",
    "      } else {\n",
    "        await new Promise((resolve) => setTimeout(resolve, 100));\n",
    "      }\n",
    "    }\n",
    "    return message!;\n",
    "  }\n",
    "\n",
    "  console.debug(\"Connecting to the service with handle %s...\", previousSessionHandle);\n",
    "  const session = await ai.live.connect({\n",
    "    model: MODEL_ID,\n",
    "    callbacks: {\n",
    "      onopen: function () {\n",
    "        console.debug(\"Opened\");\n",
    "      },\n",
    "      onmessage: function (message) {\n",
    "        responseQueue.push(message);\n",
    "        console.debug(\"Received message:\", JSON.stringify(message));\n",
    "        if (message.sessionResumptionUpdate?.resumable && message.sessionResumptionUpdate.newHandle) {\n",
    "          HANDLE = message.sessionResumptionUpdate.newHandle;\n",
    "        }\n",
    "      },\n",
    "      onerror: function (e) {\n",
    "        console.debug(\"Error:\", e.message);\n",
    "      },\n",
    "      onclose: function (e) {\n",
    "        console.debug(\"Close:\", e.reason);\n",
    "      },\n",
    "    },\n",
    "    config: {\n",
    "      responseModalities: [Modality.TEXT],\n",
    "      sessionResumption: { handle: previousSessionHandle },\n",
    "    },\n",
    "  });\n",
    "\n",
    "  for (const message of messages) {\n",
    "    console.debug(\"Sending message:\", message);\n",
    "    session.sendClientContent({\n",
    "      turns: message,\n",
    "      turnComplete: true,\n",
    "    });\n",
    "    let done = false;\n",
    "    while (!done) {\n",
    "      const response = await waitMessage();\n",
    "      if (response.serverContent?.turnComplete) {\n",
    "        done = true;\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "\n",
    "  // small delay for session resumption update to arrive\n",
    "  await new Promise((resolve) => setTimeout(resolve, 3000));\n",
    "\n",
    "  session.close();\n",
    "}\n",
    "\n",
    "await resumable_session();\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the session resumption you have the session handle to refer to your previous sessions. In this example, the handle is saved at the `handle` variable as below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session handle: CihrNGZyMjh4dXY3cXFkYzVmMjR5cnlmZ2w5bnBvNTRhcmoxNW1lN2Fi\n"
     ]
    }
   ],
   "source": [
    "console.debug(\"Session handle:\", HANDLE);\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can start a new Live API session, but this time pointing to a handle from a previous session. Also, to test you could gather information from the previous session, you will ask the model what was the second question you asked before (in this example, it was \"what is the capital of Brazil?\"). You can see the Live API recovering that information:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to the service with handle CihrNGZyMjh4dXY3cXFkYzVmMjR5cnlmZ2w5bnBvNTRhcmoxNW1lN2Fi...\n",
      "Opened\n",
      "Sending message: what was the last question I asked?\n",
      "Received message: {\"setupComplete\":{}}\n",
      "Received message: {\"sessionResumptionUpdate\":{}}\n",
      "Received message: {\"serverContent\":{\"modelTurn\":{\"parts\":[{\"text\":\"The\"}]}}}\n",
      "Received message: {\"serverContent\":{\"modelTurn\":{\"parts\":[{\"text\":\" last question you asked was: \\\"What is the capital of Brazil?\\\"\\n\"}]}}}\n",
      "Received message: {\"serverContent\":{\"generationComplete\":true}}\n",
      "Received message: {\"serverContent\":{\"turnComplete\":true},\"usageMetadata\":{\"promptTokenCount\":65,\"responseTokenCount\":16,\"totalTokenCount\":81,\"promptTokensDetails\":[{\"modality\":\"TEXT\",\"tokenCount\":65}],\"responseTokensDetails\":[{\"modality\":\"TEXT\",\"tokenCount\":16}]}}\n",
      "Received message: {\"sessionResumptionUpdate\":{\"newHandle\":\"CihmcW04ZzVnZnZwczU2ZnkwN2h1NHpmajFxZmgwcmhieTZ3Zmo3OWt6\",\"resumable\":true}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Close: \n"
     ]
    }
   ],
   "source": [
    "await resumable_session(HANDLE, [\"what was the last question I asked?\"]);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "<a name=\"next_steps\"></a>\n",
    "\n",
    "This tutorial just shows basic usage of the Live API, using the Python GenAI SDK.\n",
    "\n",
    "- If you aren't looking for code, and just want to try multimedia streaming use [Live API in Google AI Studio](https://aistudio.google.com/app/live).\n",
    "- If you want to see how to setup streaming interruptible audio and video using the Live API see the [Audio and Video input Tutorial](../quickstarts/Get_started_LiveAPI.py).\n",
    "- If you're interested in the low level details of using the websockets directly, see the [websocket version of this tutorial](../quickstarts/websockets/Get_started_LiveAPI.ipynb).\n",
    "- Try the [Tool use in the live API tutorial](../quickstarts/Get_started_LiveAPI_tools.ipynb) for an walkthrough of Gemini-2's new tool use capabilities.\n",
    "- There is a [Streaming audio in Colab example](../quickstarts/websockets/LiveAPI_streaming_in_colab.ipynb), but this is more of a **demo**, it's **not optimized for readability**.\n",
    "- Other nice Gemini 2.0 examples can also be found in the [Cookbook's 2.0 directory](https://github.com/google-gemini/cookbook/tree/main/gemini-2/), in particular the [video understanding](../quickstarts/Video_understanding.ipynb) and the [spatial understanding](../quickstarts/Spatial_understanding.ipynb) ones.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TypeScript",
   "language": "typescript",
   "name": "tslab"
  },
  "language_info": {
   "codemirror_mode": {
    "mode": "typescript",
    "name": "javascript",
    "typescript": true
   },
   "file_extension": ".ts",
   "mimetype": "text/typescript",
   "name": "typescript",
   "version": "3.7.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
